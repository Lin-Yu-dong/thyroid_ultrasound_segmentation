{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d71838a-0474-4d6f-a2e2-567afb8b8f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/home/ha24521/DL/thyroid_ultrasound_segmentation'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c678bf-f58b-406d-8da7-c99a0262b775",
   "metadata": {},
   "source": [
    "Please open the table of contents — it will help clarify the overall structure.\n",
    "\n",
    "# 4 base-line segmentation architecture\n",
    "\n",
    "The initial three architectures are located in `DL/6_segmentation_architecture_1.ipynb`. Please run that notebook before proceeding.  \n",
    "## 1/4: MobileViT\n",
    "## 2/4: transUnet\n",
    "## 3/4: Attention U-Net\n",
    "## 4/4: U-Net\n",
    "Here we need `DL/ozan_oktay_Attention_UNet/models/networks/unet_2D.py`\n",
    "\n",
    "### 4.1 train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c759cfa6-2382-4d7f-b0bf-d5bebc2fbac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "name: unet_test\n",
      "epochs: 5\n",
      "batch_size: 4\n",
      "arch: Axial\n",
      "deep_supervision: False\n",
      "input_channels: 3\n",
      "num_classes: 1\n",
      "input_w: 224\n",
      "input_h: 224\n",
      "loss: BCEDiceLoss\n",
      "dataset: ICOS\n",
      "img_ext: .jpg\n",
      "mask_ext: .jpg\n",
      "optimizer: Adam\n",
      "lr: 0.001\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0001\n",
      "nesterov: False\n",
      "scheduler: CosineAnnealingLR\n",
      "min_lr: 1e-05\n",
      "factor: 0.1\n",
      "patience: 2\n",
      "milestones: 1,2\n",
      "gamma: 0.6666666666666666\n",
      "early_stopping: -1\n",
      "num_workers: 4\n",
      "--------------------\n",
      "\n",
      "Trainable parameters 2465329\n",
      "Epoch [0/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/575 [00:00<?, ?it/s]/data/home/ha24521/.conda/envs/Practical_5/lib/python3.11/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "100%|██████████| 575/575 [00:11<00:00, 50.12it/s, loss=0.853, iou=0.0509]  \n",
      "100%|██████████| 144/144 [00:02<00:00, 70.91it/s, loss=0.762, iou=0.223, dice=0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.8526 - iou 0.0509 - val_loss 0.7615 - val_iou 0.2225\n",
      "Current LR: 0.000905463412215599\n",
      "=> saved best model\n",
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:08<00:00, 71.13it/s, loss=0.752, iou=0.263]\n",
      "100%|██████████| 144/144 [00:01<00:00, 123.06it/s, loss=0.754, iou=0.297, dice=0.447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7524 - iou 0.2627 - val_loss 0.7539 - val_iou 0.2966\n",
      "Current LR: 0.000657963412215599\n",
      "=> saved best model\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:07<00:00, 72.29it/s, loss=0.69, iou=0.328] \n",
      "100%|██████████| 144/144 [00:01<00:00, 121.98it/s, loss=0.699, iou=0.362, dice=0.521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.6903 - iou 0.3281 - val_loss 0.6987 - val_iou 0.3624\n",
      "Current LR: 0.0003520365877844011\n",
      "=> saved best model\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:07<00:00, 74.24it/s, loss=0.659, iou=0.37] \n",
      "100%|██████████| 144/144 [00:01<00:00, 121.45it/s, loss=0.609, iou=0.415, dice=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.6585 - iou 0.3704 - val_loss 0.6094 - val_iou 0.4147\n",
      "Current LR: 0.00010453658778440106\n",
      "=> saved best model\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:07<00:00, 72.96it/s, loss=0.604, iou=0.408]\n",
      "100%|██████████| 144/144 [00:01<00:00, 128.80it/s, loss=0.585, iou=0.397, dice=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.6041 - iou 0.4082 - val_loss 0.5847 - val_iou 0.3966\n",
      "Current LR: 1e-05\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yaml\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import Compose, OneOf  # version of albumentaation 1.3.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from albumentations import RandomRotate90,Resize\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import archs\n",
    "import losses\n",
    "from metrics import iou_score\n",
    "from utils import AverageMeter, str2bool\n",
    "from dataset import Dataset\n",
    "\n",
    "from ozan_oktay_Attention_UNet.models.networks.unet_2D import unet_2D \n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--name', default=None,\n",
    "                        help='model name: (default: arch)')\n",
    "    parser.add_argument('--epochs', default=500, type=int, metavar='N',   \n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-b', '--batch_size', default=4, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 16)')\n",
    "    \n",
    "    # model\n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='Axial')   \n",
    "    parser.add_argument('--deep_supervision', default=False, type=str2bool)\n",
    "    parser.add_argument('--input_channels', default=3, type=int,           \n",
    "                        help='input channels')\n",
    "    parser.add_argument('--num_classes', default=1, type=int,              \n",
    "                        help='number of classes')\n",
    "    parser.add_argument('--input_w', default=224, type=int,                \n",
    "                        help='image width')\n",
    "    parser.add_argument('--input_h', default=224, type=int,\n",
    "                        help='image height')\n",
    "    \n",
    "    # loss\n",
    "    parser.add_argument('--loss', default='BCEDiceLoss',\n",
    "                        choices=LOSS_NAMES,\n",
    "                        help='loss: ' +\n",
    "                        ' | '.join(LOSS_NAMES) +\n",
    "                        ' (default: BCEDiceLoss)')\n",
    "    \n",
    "    # dataset\n",
    "    parser.add_argument('--dataset', default='ICOS',\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--img_ext', default='.jpg',\n",
    "                        help='image file extension')\n",
    "    parser.add_argument('--mask_ext', default='.jpg',\n",
    "                        help='mask file extension')\n",
    "\n",
    "    # optimizer\n",
    "    parser.add_argument('--optimizer', default='Adam',\n",
    "                        choices=['Adam', 'SGD'],\n",
    "                        help='loss: ' +\n",
    "                        ' | '.join(['Adam', 'SGD']) +\n",
    "                        ' (default: Adam)')\n",
    "    parser.add_argument('--lr', '--learning_rate', default=1e-3, type=float,\n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float,\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--nesterov', default=False, type=str2bool,\n",
    "                        help='nesterov')\n",
    "\n",
    "    # scheduler\n",
    "    parser.add_argument('--scheduler', default='CosineAnnealingLR',\n",
    "                        choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])\n",
    "    parser.add_argument('--min_lr', default=1e-5, type=float,\n",
    "                        help='minimum learning rate')\n",
    "    parser.add_argument('--factor', default=0.1, type=float)\n",
    "    parser.add_argument('--patience', default=2, type=int)\n",
    "    parser.add_argument('--milestones', default='1,2', type=str)\n",
    "    parser.add_argument('--gamma', default=2/3, type=float)\n",
    "    parser.add_argument('--early_stopping', default=-1, type=int,\n",
    "                        metavar='N', help='early stopping (default: -1)')\n",
    "    parser.add_argument('--cfg', type=str, metavar=\"FILE\", help='path to config file', )\n",
    "\n",
    "    parser.add_argument('--num_workers', default=4, type=int)\n",
    "\n",
    "    config = parser.parse_args()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "def train(config, train_loader, model, criterion, optimizer):\n",
    "    avg_meters = {'loss': AverageMeter(),\n",
    "                  'iou': AverageMeter()}\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=len(train_loader))\n",
    "    for input, target in train_loader:     \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        target = target.unsqueeze(1)\n",
    "\n",
    "        # compute output\n",
    "        if config['deep_supervision']:\n",
    "            outputs = model(input)\n",
    "            loss = 0\n",
    "            for output in outputs:\n",
    "                loss += criterion(output, target)\n",
    "            loss /= len(outputs)\n",
    "            iou,dice = iou_score(outputs[-1], target)\n",
    "        else:\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            iou,dice = iou_score(output, target)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "        avg_meters['iou'].update(iou, input.size(0))\n",
    "\n",
    "        postfix = OrderedDict([\n",
    "            ('loss', avg_meters['loss'].avg),\n",
    "            ('iou', avg_meters['iou'].avg),\n",
    "        ])\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "\n",
    "def validate(config, val_loader, model, criterion):\n",
    "    avg_meters = {'loss': AverageMeter(),\n",
    "                  'iou': AverageMeter(),\n",
    "                  'dice': AverageMeter()}\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(val_loader))\n",
    "        for input, target in val_loader:     \n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            target = target.unsqueeze(1)\n",
    "\n",
    "            # compute output\n",
    "            if config['deep_supervision']:\n",
    "                outputs = model(input)\n",
    "                loss = 0\n",
    "                for output in outputs:\n",
    "                    loss += criterion(output, target)\n",
    "                loss /= len(outputs)\n",
    "                iou,dice = iou_score(outputs[-1], target)\n",
    "            else:\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "                iou,dice = iou_score(output, target)\n",
    "\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['iou'].update(iou, input.size(0))\n",
    "            avg_meters['dice'].update(dice, input.size(0))\n",
    "            postfix = OrderedDict([\n",
    "                ('loss', avg_meters['loss'].avg),\n",
    "                ('iou', avg_meters['iou'].avg),\n",
    "                ('dice', avg_meters['dice'].avg)\n",
    "            ])\n",
    "            pbar.set_postfix(postfix)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg),\n",
    "                        ('dice', avg_meters['dice'].avg)])\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "        weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "        wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "        wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "        pred = torch.sigmoid(pred)\n",
    "        smooth = 1\n",
    "        size = pred.size(0)\n",
    "        pred_flat = pred.view(size, -1)\n",
    "        mask_flat = mask.view(size, -1)\n",
    "        intersection = pred_flat * mask_flat\n",
    "        dice_score = (2 * intersection.sum(1) + smooth) / (pred_flat.sum(1) + mask_flat.sum(1) + smooth)\n",
    "        dice_loss = 1 - dice_score.sum() / size\n",
    "\n",
    "        return (wbce + 0.6*dice_loss).mean()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        \n",
    "def main():\n",
    "   \n",
    "    config = {\n",
    "    'name': 'unet_test', \n",
    "    'epochs': 5, # I recommend 400 epochs for fully demonstrating the potential of transUnet. Here I use 5 epochs to test whether the code runs successfully. \n",
    "    'batch_size': 4,\n",
    "    'arch': 'Axial',\n",
    "    'deep_supervision': False,\n",
    "    'input_channels': 3,\n",
    "    'num_classes': 1,\n",
    "    'input_w': 224,\n",
    "    'input_h': 224,\n",
    "    'loss': 'BCEDiceLoss',\n",
    "    'dataset': 'ICOS',\n",
    "    'img_ext': '.jpg',\n",
    "    'mask_ext': '.jpg',\n",
    "    'optimizer': 'Adam',\n",
    "    \n",
    "    'lr': 1e-3,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-4,\n",
    "    'nesterov': False,\n",
    "    'scheduler': 'CosineAnnealingLR',\n",
    "    'min_lr': 1e-5,\n",
    "    'factor': 0.1,\n",
    "    'patience': 2,\n",
    "    'milestones': '1,2',\n",
    "    'gamma': 2/3,\n",
    "    'early_stopping': -1,   \n",
    "    'num_workers': 4\n",
    "}\n",
    "    set_seed(42)\n",
    "    \n",
    "    if config['name'] is None:\n",
    "        if config['deep_supervision']:\n",
    "            config['name'] = '%s_%s_wDS' % (config['dataset'], config['arch'])\n",
    "        else:\n",
    "            config['name'] = '%s_%s_woDS' % (config['dataset'], config['arch'])\n",
    "    \n",
    "    os.makedirs('models/%s' % config['name'], exist_ok=True)\n",
    "\n",
    "    print('-' * 20)\n",
    "    for key in config:\n",
    "        print('%s: %s' % (key, config[key]))\n",
    "    print('-' * 20)\n",
    "\n",
    "    with open('models/%s/config.yml' % config['name'], 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    # define loss function (criterion)\n",
    "    if config['loss'] == 'BCEDiceLoss':\n",
    "        criterion = BCEDiceLoss().cuda() #nn.BCEWithLogitsLoss().cuda()\n",
    "    else:\n",
    "        criterion = losses.__dict__[config['loss']]().cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    model = unet_2D(n_classes=1, in_channels=3)        \n",
    "    model = model.cuda()\n",
    "\n",
    "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print (\"\\nTrainable parameters\", pytorch_total_params)\n",
    "    \n",
    "    # print(params)\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n",
    "                              nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if config['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=config['epochs'], eta_min=config['min_lr'])\n",
    "    elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config['factor'], patience=config['patience'],\n",
    "                                                   verbose=1, min_lr=config['min_lr'])\n",
    "    elif config['scheduler'] == 'MultiStepLR':\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config['milestones'].split(',')], gamma=config['gamma'])\n",
    "    elif config['scheduler'] == 'ConstantLR':\n",
    "        scheduler = None\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Data loading code\n",
    "    train_img_ids = glob('dataset_TN3K/training_image/*')\n",
    "    train_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in train_img_ids]\n",
    "\n",
    "    val_img_ids = glob('dataset_TN3K/validation_image/*')\n",
    "    val_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in val_img_ids]\n",
    "    \n",
    "\n",
    "    train_transform = Compose([\n",
    "        RandomRotate90(),\n",
    "        A.HorizontalFlip(), \n",
    "        A.VerticalFlip(),   \n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.5, rotate_limit=45, p=0.75),\n",
    "        A.Transpose(),\n",
    "        Resize(config['input_h'], config['input_w']), \n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    val_transform = Compose([\n",
    "        Resize(config['input_h'], config['input_w']),\n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = Dataset(\n",
    "        img_ids=train_img_ids,\n",
    "        img_dir='dataset_TN3K/training_image',\n",
    "        mask_dir='dataset_TN3K/training_mask',\n",
    "        \n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=train_transform)\n",
    "    \n",
    "    val_dataset = Dataset(\n",
    "        img_ids=val_img_ids,\n",
    "        img_dir=os.path.join('dataset_TN3K/validation_image'),\n",
    "        mask_dir=os.path.join('dataset_TN3K/validation_mask'),\n",
    "        \n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=val_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        drop_last=False)\n",
    "    log = OrderedDict([\n",
    "        ('epoch', []),\n",
    "        ('lr', []),\n",
    "        ('loss', []),\n",
    "        ('iou', []),\n",
    "        ('val_loss', []),\n",
    "        ('val_iou', []),\n",
    "        ('val_dice', []),\n",
    "    ])\n",
    "\n",
    "    best_iou = 0\n",
    "    trigger = 0\n",
    "    for epoch in range(config['epochs']):\n",
    "        print('Epoch [%d/%d]' % (epoch, config['epochs']))\n",
    "\n",
    "        # train for one epoch\n",
    "        train_log = train(config, train_loader, model, criterion, optimizer)\n",
    "        # evaluate on validation set\n",
    "        val_log = validate(config, val_loader, model, criterion)\n",
    "\n",
    "        if config['scheduler'] == 'CosineAnnealingLR':\n",
    "            scheduler.step()\n",
    "        elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "            scheduler.step(val_log['loss'])\n",
    "\n",
    "        print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n",
    "              % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n",
    "\n",
    "        \n",
    "        log['epoch'].append(epoch)   \n",
    "        log['lr'].append(optimizer.param_groups[0]['lr'])       \n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']}\") \n",
    "        \n",
    "        log['loss'].append(train_log['loss'])\n",
    "        log['iou'].append(train_log['iou'])\n",
    "        log['val_loss'].append(val_log['loss'])\n",
    "        log['val_iou'].append(val_log['iou'])\n",
    "        log['val_dice'].append(val_log['dice'])\n",
    "\n",
    "        pd.DataFrame(log).to_csv('models/%s/log.csv' %\n",
    "                                 config['name'], index=False)\n",
    "\n",
    "        trigger += 1\n",
    "\n",
    "        if val_log['iou'] > best_iou:\n",
    "            torch.save(model.state_dict(), 'models/%s/model.pth' %\n",
    "                       config['name'])\n",
    "            best_iou = val_log['iou']\n",
    "            print(\"=> saved best model\")\n",
    "            trigger = 0\n",
    "\n",
    "        # early stopping\n",
    "        if config['early_stopping'] >= 0 and trigger >= config['early_stopping']:\n",
    "            print(\"=> early stopping\")\n",
    "            break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db942fda-1579-4a6a-b44c-2188c31cd539",
   "metadata": {},
   "source": [
    "### 4.2 calculate IoU and Dice\n",
    "unet_test  \n",
    "dataset_TN3K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e8cbb9-7886-4522-b035-6139f5a941e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/networks_other.py:42: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/networks_other.py:46: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  init.normal(m.weight.data, 1.0, 0.02)\n",
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/networks_other.py:47: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  init.constant(m.bias.data, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Loaded training configuration:\n",
      "arch: Axial\n",
      "batch_size: 4\n",
      "dataset: ICOS\n",
      "deep_supervision: False\n",
      "early_stopping: -1\n",
      "epochs: 5\n",
      "factor: 0.1\n",
      "gamma: 0.6666666666666666\n",
      "img_ext: .jpg\n",
      "input_channels: 3\n",
      "input_h: 224\n",
      "input_w: 224\n",
      "loss: BCEDiceLoss\n",
      "lr: 0.001\n",
      "mask_ext: .jpg\n",
      "milestones: 1,2\n",
      "min_lr: 1e-05\n",
      "momentum: 0.9\n",
      "name: unet_test\n",
      "nesterov: False\n",
      "num_classes: 1\n",
      "num_workers: 4\n",
      "optimizer: Adam\n",
      "patience: 2\n",
      "scheduler: CosineAnnealingLR\n",
      "weight_decay: 0.0001\n",
      "--------------------\n",
      "Successfully loaded model weights from models/unet_test/model.pth\n",
      "\n",
      "Starting evaluation and prediction saving to: models/unet_test/predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:   0%|          | 0/614 [00:00<?, ?it/s]\u001b[A/data/home/ha24521/.conda/envs/Practical_5/lib/python3.11/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "\n",
      "Validating and Predicting:   0%|          | 0/614 [00:00<?, ?it/s, loss=0.72, iou=0.153, dice=0.266]\u001b[A\n",
      "Validating and Predicting:   0%|          | 1/614 [00:00<02:53,  3.52it/s, loss=0.72, iou=0.153, dice=0.266]\u001b[A\n",
      "Validating and Predicting:   0%|          | 1/614 [00:00<02:53,  3.52it/s, loss=0.724, iou=0.0767, dice=0.133]\u001b[A\n",
      "Validating and Predicting:   0%|          | 2/614 [00:00<02:53,  3.52it/s, loss=1.06, iou=0.0564, dice=0.0989]\u001b[A\n",
      "Validating and Predicting:   0%|          | 3/614 [00:00<02:53,  3.52it/s, loss=0.964, iou=0.101, dice=0.17]  \u001b[A\n",
      "Validating and Predicting:   1%|          | 4/614 [00:00<02:53,  3.52it/s, loss=0.896, iou=0.118, dice=0.198]\u001b[A\n",
      "Validating and Predicting:   1%|          | 5/614 [00:00<02:52,  3.52it/s, loss=0.801, iou=0.221, dice=0.306]\u001b[A\n",
      "Validating and Predicting:   1%|          | 6/614 [00:00<02:52,  3.52it/s, loss=0.792, iou=0.19, dice=0.264] \u001b[A\n",
      "Validating and Predicting:   1%|          | 7/614 [00:00<02:52,  3.52it/s, loss=0.887, iou=0.168, dice=0.234]\u001b[A\n",
      "Validating and Predicting:   1%|▏         | 8/614 [00:00<02:51,  3.52it/s, loss=0.834, iou=0.209, dice=0.286]\u001b[A\n",
      "Validating and Predicting:   1%|▏         | 9/614 [00:00<02:51,  3.52it/s, loss=0.788, iou=0.245, dice=0.329]\u001b[A\n",
      "Validating and Predicting:   2%|▏         | 10/614 [00:00<02:51,  3.52it/s, loss=0.762, iou=0.281, dice=0.371]\u001b[A\n",
      "Validating and Predicting:   2%|▏         | 11/614 [00:00<02:51,  3.52it/s, loss=0.749, iou=0.281, dice=0.377]\u001b[A\n",
      "Validating and Predicting:   2%|▏         | 12/614 [00:00<00:15, 38.79it/s, loss=0.749, iou=0.281, dice=0.377]\u001b[A\n",
      "Validating and Predicting:   2%|▏         | 12/614 [00:00<00:15, 38.79it/s, loss=0.721, iou=0.314, dice=0.411]\u001b[A\n",
      "Validating and Predicting:   2%|▏         | 13/614 [00:00<00:15, 38.79it/s, loss=0.724, iou=0.303, dice=0.402]\u001b[A\n",
      "Validating and Predicting:   2%|▏         | 14/614 [00:00<00:15, 38.79it/s, loss=0.713, iou=0.305, dice=0.408]\u001b[A\n",
      "Validating and Predicting:   2%|▏         | 15/614 [00:00<00:15, 38.79it/s, loss=0.7, iou=0.314, dice=0.421]  \u001b[A\n",
      "Validating and Predicting:   3%|▎         | 16/614 [00:00<00:15, 38.79it/s, loss=0.692, iou=0.308, dice=0.418]\u001b[A\n",
      "Validating and Predicting:   3%|▎         | 17/614 [00:00<00:15, 38.79it/s, loss=0.671, iou=0.327, dice=0.438]\u001b[A\n",
      "Validating and Predicting:   3%|▎         | 18/614 [00:00<00:15, 38.79it/s, loss=0.672, iou=0.318, dice=0.429]\u001b[A\n",
      "Validating and Predicting:   3%|▎         | 19/614 [00:00<00:15, 38.79it/s, loss=0.675, iou=0.309, dice=0.42] \u001b[A\n",
      "Validating and Predicting:   3%|▎         | 20/614 [00:00<00:15, 38.79it/s, loss=0.665, iou=0.324, dice=0.436]\u001b[A\n",
      "Validating and Predicting:   3%|▎         | 21/614 [00:00<00:15, 38.79it/s, loss=0.655, iou=0.335, dice=0.45] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0183\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0586\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0350\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0073\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0476\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0240\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0366\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0130\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0227\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0533\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0020\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0117\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0423\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0007\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0184\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0490\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0587\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0351\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0074\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0477\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0241\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0367\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0131\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:   4%|▎         | 22/614 [00:00<00:15, 38.79it/s, loss=0.64, iou=0.353, dice=0.467]\u001b[A\n",
      "Validating and Predicting:   4%|▎         | 23/614 [00:00<00:15, 38.79it/s, loss=0.625, iou=0.369, dice=0.483]\u001b[A\n",
      "Validating and Predicting:   4%|▍         | 24/614 [00:00<00:09, 64.78it/s, loss=0.625, iou=0.369, dice=0.483]\u001b[A\n",
      "Validating and Predicting:   4%|▍         | 24/614 [00:00<00:09, 64.78it/s, loss=0.624, iou=0.362, dice=0.477]\u001b[A\n",
      "Validating and Predicting:   4%|▍         | 25/614 [00:00<00:09, 64.78it/s, loss=0.609, iou=0.379, dice=0.493]\u001b[A\n",
      "Validating and Predicting:   4%|▍         | 26/614 [00:00<00:09, 64.78it/s, loss=0.61, iou=0.374, dice=0.488] \u001b[A\n",
      "Validating and Predicting:   4%|▍         | 27/614 [00:00<00:09, 64.78it/s, loss=0.611, iou=0.368, dice=0.484]\u001b[A\n",
      "Validating and Predicting:   5%|▍         | 28/614 [00:00<00:09, 64.78it/s, loss=0.603, iou=0.372, dice=0.489]\u001b[A\n",
      "Validating and Predicting:   5%|▍         | 29/614 [00:00<00:09, 64.78it/s, loss=0.602, iou=0.376, dice=0.495]\u001b[A\n",
      "Validating and Predicting:   5%|▍         | 30/614 [00:00<00:09, 64.78it/s, loss=0.604, iou=0.371, dice=0.491]\u001b[A\n",
      "Validating and Predicting:   5%|▌         | 31/614 [00:00<00:08, 64.78it/s, loss=0.594, iou=0.386, dice=0.504]\u001b[A\n",
      "Validating and Predicting:   5%|▌         | 32/614 [00:00<00:08, 64.78it/s, loss=0.594, iou=0.381, dice=0.5]  \u001b[A\n",
      "Validating and Predicting:   5%|▌         | 33/614 [00:00<00:08, 64.78it/s, loss=0.59, iou=0.382, dice=0.502]\u001b[A\n",
      "Validating and Predicting:   6%|▌         | 34/614 [00:00<00:08, 64.78it/s, loss=0.586, iou=0.39, dice=0.511]\u001b[A\n",
      "Validating and Predicting:   6%|▌         | 35/614 [00:00<00:08, 64.78it/s, loss=0.586, iou=0.393, dice=0.515]\u001b[A\n",
      "Validating and Predicting:   6%|▌         | 36/614 [00:00<00:08, 64.78it/s, loss=0.584, iou=0.399, dice=0.522]\u001b[A\n",
      "Validating and Predicting:   6%|▌         | 37/614 [00:00<00:06, 85.27it/s, loss=0.584, iou=0.399, dice=0.522]\u001b[A\n",
      "Validating and Predicting:   6%|▌         | 37/614 [00:00<00:06, 85.27it/s, loss=0.578, iou=0.406, dice=0.529]\u001b[A\n",
      "Validating and Predicting:   6%|▌         | 38/614 [00:00<00:06, 85.27it/s, loss=0.578, iou=0.403, dice=0.527]\u001b[A\n",
      "Validating and Predicting:   6%|▋         | 39/614 [00:00<00:06, 85.27it/s, loss=0.581, iou=0.398, dice=0.522]\u001b[A\n",
      "Validating and Predicting:   7%|▋         | 40/614 [00:00<00:06, 85.27it/s, loss=0.572, iou=0.406, dice=0.53] \u001b[A\n",
      "Validating and Predicting:   7%|▋         | 41/614 [00:00<00:06, 85.27it/s, loss=0.62, iou=0.397, dice=0.519]\u001b[A\n",
      "Validating and Predicting:   7%|▋         | 42/614 [00:00<00:06, 85.27it/s, loss=0.623, iou=0.39, dice=0.51] \u001b[A\n",
      "Validating and Predicting:   7%|▋         | 43/614 [00:00<00:06, 85.27it/s, loss=0.625, iou=0.383, dice=0.503]\u001b[A\n",
      "Validating and Predicting:   7%|▋         | 44/614 [00:00<00:06, 85.27it/s, loss=0.625, iou=0.38, dice=0.5]   \u001b[A\n",
      "Validating and Predicting:   7%|▋         | 45/614 [00:00<00:06, 85.27it/s, loss=0.629, iou=0.375, dice=0.495]\u001b[A\n",
      "Validating and Predicting:   7%|▋         | 46/614 [00:00<00:06, 85.27it/s, loss=0.626, iou=0.375, dice=0.496]\u001b[A\n",
      "Validating and Predicting:   8%|▊         | 47/614 [00:00<00:06, 85.27it/s, loss=0.631, iou=0.368, dice=0.488]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0228\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0534\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0021\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0118\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0424\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0008\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0185\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0491\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0588\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0352\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0075\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0478\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0242\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0368\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0132\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0229\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0535\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0119\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0009\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0186\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0492\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0589\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0353\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0076\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0479\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:   8%|▊         | 48/614 [00:00<00:06, 85.27it/s, loss=0.635, iou=0.368, dice=0.489]\u001b[A\n",
      "Validating and Predicting:   8%|▊         | 49/614 [00:00<00:05, 95.41it/s, loss=0.635, iou=0.368, dice=0.489]\u001b[A\n",
      "Validating and Predicting:   8%|▊         | 49/614 [00:00<00:05, 95.41it/s, loss=0.637, iou=0.363, dice=0.483]\u001b[A\n",
      "Validating and Predicting:   8%|▊         | 50/614 [00:00<00:05, 95.41it/s, loss=0.64, iou=0.358, dice=0.477] \u001b[A\n",
      "Validating and Predicting:   8%|▊         | 51/614 [00:00<00:05, 95.41it/s, loss=0.634, iou=0.365, dice=0.484]\u001b[A\n",
      "Validating and Predicting:   8%|▊         | 52/614 [00:00<00:05, 95.41it/s, loss=0.631, iou=0.367, dice=0.487]\u001b[A\n",
      "Validating and Predicting:   9%|▊         | 53/614 [00:00<00:05, 95.41it/s, loss=0.628, iou=0.371, dice=0.492]\u001b[A\n",
      "Validating and Predicting:   9%|▉         | 54/614 [00:00<00:05, 95.41it/s, loss=0.629, iou=0.367, dice=0.487]\u001b[A\n",
      "Validating and Predicting:   9%|▉         | 55/614 [00:00<00:05, 95.41it/s, loss=0.627, iou=0.367, dice=0.488]\u001b[A\n",
      "Validating and Predicting:   9%|▉         | 56/614 [00:00<00:05, 95.41it/s, loss=0.625, iou=0.367, dice=0.489]\u001b[A\n",
      "Validating and Predicting:   9%|▉         | 57/614 [00:00<00:05, 95.41it/s, loss=0.63, iou=0.36, dice=0.48]   \u001b[A\n",
      "Validating and Predicting:   9%|▉         | 58/614 [00:00<00:05, 95.41it/s, loss=0.628, iou=0.362, dice=0.482]\u001b[A\n",
      "Validating and Predicting:  10%|▉         | 59/614 [00:00<00:05, 95.41it/s, loss=0.63, iou=0.358, dice=0.479] \u001b[A\n",
      "Validating and Predicting:  10%|▉         | 60/614 [00:00<00:05, 99.85it/s, loss=0.63, iou=0.358, dice=0.479]\u001b[A\n",
      "Validating and Predicting:  10%|▉         | 60/614 [00:00<00:05, 99.85it/s, loss=0.63, iou=0.359, dice=0.48] \u001b[A\n",
      "Validating and Predicting:  10%|▉         | 61/614 [00:00<00:05, 99.85it/s, loss=0.63, iou=0.357, dice=0.48]\u001b[A\n",
      "Validating and Predicting:  10%|█         | 62/614 [00:00<00:05, 99.85it/s, loss=0.628, iou=0.359, dice=0.482]\u001b[A\n",
      "Validating and Predicting:  10%|█         | 63/614 [00:00<00:05, 99.85it/s, loss=0.629, iou=0.359, dice=0.483]\u001b[A\n",
      "Validating and Predicting:  10%|█         | 64/614 [00:00<00:05, 99.85it/s, loss=0.63, iou=0.355, dice=0.478] \u001b[A\n",
      "Validating and Predicting:  11%|█         | 65/614 [00:00<00:05, 99.85it/s, loss=0.626, iou=0.357, dice=0.481]\u001b[A\n",
      "Validating and Predicting:  11%|█         | 66/614 [00:00<00:05, 99.85it/s, loss=0.626, iou=0.356, dice=0.48] \u001b[A\n",
      "Validating and Predicting:  11%|█         | 67/614 [00:00<00:05, 99.85it/s, loss=0.625, iou=0.356, dice=0.481]\u001b[A\n",
      "Validating and Predicting:  11%|█         | 68/614 [00:00<00:05, 99.85it/s, loss=0.624, iou=0.357, dice=0.482]\u001b[A\n",
      "Validating and Predicting:  11%|█         | 69/614 [00:00<00:05, 99.85it/s, loss=0.621, iou=0.359, dice=0.484]\u001b[A\n",
      "Validating and Predicting:  11%|█▏        | 70/614 [00:00<00:05, 99.85it/s, loss=0.618, iou=0.362, dice=0.488]\u001b[A\n",
      "Validating and Predicting:  12%|█▏        | 71/614 [00:00<00:05, 99.85it/s, loss=0.618, iou=0.36, dice=0.486] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0243\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0410\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0369\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0133\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0536\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0300\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0090\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0187\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0493\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0354\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0077\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0244\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0411\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0134\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0537\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0301\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0091\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0188\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0494\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0355\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0078\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0245\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0412\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0135\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  12%|█▏        | 72/614 [00:00<00:05, 99.85it/s, loss=0.621, iou=0.357, dice=0.483]\u001b[A\n",
      "Validating and Predicting:  12%|█▏        | 73/614 [00:00<00:04, 108.22it/s, loss=0.621, iou=0.357, dice=0.483]\u001b[A\n",
      "Validating and Predicting:  12%|█▏        | 73/614 [00:00<00:04, 108.22it/s, loss=0.621, iou=0.359, dice=0.486]\u001b[A\n",
      "Validating and Predicting:  12%|█▏        | 74/614 [00:00<00:04, 108.22it/s, loss=0.628, iou=0.358, dice=0.486]\u001b[A\n",
      "Validating and Predicting:  12%|█▏        | 75/614 [00:00<00:04, 108.22it/s, loss=0.624, iou=0.363, dice=0.49] \u001b[A\n",
      "Validating and Predicting:  12%|█▏        | 76/614 [00:00<00:04, 108.22it/s, loss=0.622, iou=0.366, dice=0.494]\u001b[A\n",
      "Validating and Predicting:  13%|█▎        | 77/614 [00:00<00:04, 108.22it/s, loss=0.618, iou=0.371, dice=0.498]\u001b[A\n",
      "Validating and Predicting:  13%|█▎        | 78/614 [00:00<00:04, 108.22it/s, loss=0.618, iou=0.373, dice=0.5]  \u001b[A\n",
      "Validating and Predicting:  13%|█▎        | 79/614 [00:00<00:04, 108.22it/s, loss=0.615, iou=0.376, dice=0.504]\u001b[A\n",
      "Validating and Predicting:  13%|█▎        | 80/614 [00:00<00:04, 108.22it/s, loss=0.611, iou=0.379, dice=0.507]\u001b[A\n",
      "Validating and Predicting:  13%|█▎        | 81/614 [00:00<00:04, 108.22it/s, loss=0.608, iou=0.383, dice=0.511]\u001b[A\n",
      "Validating and Predicting:  13%|█▎        | 82/614 [00:00<00:04, 108.22it/s, loss=0.608, iou=0.383, dice=0.512]\u001b[A\n",
      "Validating and Predicting:  14%|█▎        | 83/614 [00:00<00:04, 108.22it/s, loss=0.609, iou=0.381, dice=0.51] \u001b[A\n",
      "Validating and Predicting:  14%|█▎        | 84/614 [00:01<00:04, 108.22it/s, loss=0.608, iou=0.379, dice=0.507]\u001b[A\n",
      "Validating and Predicting:  14%|█▍        | 85/614 [00:01<00:04, 107.00it/s, loss=0.608, iou=0.379, dice=0.507]\u001b[A\n",
      "Validating and Predicting:  14%|█▍        | 85/614 [00:01<00:04, 107.00it/s, loss=0.607, iou=0.38, dice=0.509] \u001b[A\n",
      "Validating and Predicting:  14%|█▍        | 86/614 [00:01<00:04, 107.00it/s, loss=0.603, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  14%|█▍        | 87/614 [00:01<00:04, 107.00it/s, loss=0.601, iou=0.388, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  14%|█▍        | 88/614 [00:01<00:04, 107.00it/s, loss=0.599, iou=0.39, dice=0.519] \u001b[A\n",
      "Validating and Predicting:  14%|█▍        | 89/614 [00:01<00:04, 107.00it/s, loss=0.597, iou=0.391, dice=0.521]\u001b[A\n",
      "Validating and Predicting:  15%|█▍        | 90/614 [00:01<00:04, 107.00it/s, loss=0.594, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  15%|█▍        | 91/614 [00:01<00:04, 107.00it/s, loss=0.594, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  15%|█▍        | 92/614 [00:01<00:04, 107.00it/s, loss=0.592, iou=0.398, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  15%|█▌        | 93/614 [00:01<00:04, 107.00it/s, loss=0.593, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  15%|█▌        | 94/614 [00:01<00:04, 107.00it/s, loss=0.592, iou=0.396, dice=0.526]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0538\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0302\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0299\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0092\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0189\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0495\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0356\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0079\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0246\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0010\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0413\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0136\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0539\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0303\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0480\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0370\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0093\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0496\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0260\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0357\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0247\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0011\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0414\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  15%|█▌        | 95/614 [00:01<00:04, 107.00it/s, loss=0.593, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  16%|█▌        | 96/614 [00:01<00:04, 107.00it/s, loss=0.613, iou=0.39, dice=0.519] \u001b[A\n",
      "Validating and Predicting:  16%|█▌        | 97/614 [00:01<00:04, 107.00it/s, loss=0.612, iou=0.392, dice=0.521]\u001b[A\n",
      "Validating and Predicting:  16%|█▌        | 98/614 [00:01<00:04, 113.25it/s, loss=0.612, iou=0.392, dice=0.521]\u001b[A\n",
      "Validating and Predicting:  16%|█▌        | 98/614 [00:01<00:04, 113.25it/s, loss=0.611, iou=0.392, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  16%|█▌        | 99/614 [00:01<00:04, 113.25it/s, loss=0.609, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  16%|█▋        | 100/614 [00:01<00:04, 113.25it/s, loss=0.61, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  16%|█▋        | 101/614 [00:01<00:04, 113.25it/s, loss=0.609, iou=0.394, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  17%|█▋        | 102/614 [00:01<00:04, 113.25it/s, loss=0.607, iou=0.395, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  17%|█▋        | 103/614 [00:01<00:04, 113.25it/s, loss=0.608, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  17%|█▋        | 104/614 [00:01<00:04, 113.25it/s, loss=0.607, iou=0.394, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  17%|█▋        | 105/614 [00:01<00:04, 113.25it/s, loss=0.606, iou=0.393, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  17%|█▋        | 106/614 [00:01<00:04, 113.25it/s, loss=0.607, iou=0.393, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  17%|█▋        | 107/614 [00:01<00:04, 113.25it/s, loss=0.608, iou=0.39, dice=0.522] \u001b[A\n",
      "Validating and Predicting:  18%|█▊        | 108/614 [00:01<00:04, 113.25it/s, loss=0.606, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  18%|█▊        | 109/614 [00:01<00:04, 113.25it/s, loss=0.603, iou=0.396, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  18%|█▊        | 110/614 [00:01<00:04, 113.64it/s, loss=0.603, iou=0.396, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  18%|█▊        | 110/614 [00:01<00:04, 113.64it/s, loss=0.603, iou=0.394, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  18%|█▊        | 111/614 [00:01<00:04, 113.64it/s, loss=0.602, iou=0.395, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  18%|█▊        | 112/614 [00:01<00:04, 113.64it/s, loss=0.599, iou=0.398, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  18%|█▊        | 113/614 [00:01<00:04, 113.64it/s, loss=0.599, iou=0.4, dice=0.531]  \u001b[A\n",
      "Validating and Predicting:  19%|█▊        | 114/614 [00:01<00:04, 113.64it/s, loss=0.6, iou=0.398, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  19%|█▊        | 115/614 [00:01<00:04, 113.64it/s, loss=0.599, iou=0.397, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  19%|█▉        | 116/614 [00:01<00:04, 113.64it/s, loss=0.601, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  19%|█▉        | 117/614 [00:01<00:04, 113.64it/s, loss=0.6, iou=0.395, dice=0.526]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0137\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0304\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0481\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0371\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0094\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0497\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0261\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0358\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0248\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0012\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0415\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0138\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0305\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0482\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0372\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0095\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0498\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0262\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0359\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0123\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0526\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0249\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0013\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0416\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  19%|█▉        | 118/614 [00:01<00:04, 113.64it/s, loss=0.598, iou=0.396, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  19%|█▉        | 119/614 [00:01<00:04, 113.64it/s, loss=0.597, iou=0.397, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  20%|█▉        | 120/614 [00:01<00:04, 113.64it/s, loss=0.596, iou=0.397, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  20%|█▉        | 121/614 [00:01<00:04, 113.64it/s, loss=0.594, iou=0.4, dice=0.531]  \u001b[A\n",
      "Validating and Predicting:  20%|█▉        | 122/614 [00:01<00:04, 110.94it/s, loss=0.594, iou=0.4, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  20%|█▉        | 122/614 [00:01<00:04, 110.94it/s, loss=0.594, iou=0.4, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  20%|██        | 123/614 [00:01<00:04, 110.94it/s, loss=0.596, iou=0.397, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  20%|██        | 124/614 [00:01<00:04, 110.94it/s, loss=0.594, iou=0.399, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  20%|██        | 125/614 [00:01<00:04, 110.94it/s, loss=0.599, iou=0.399, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  21%|██        | 126/614 [00:01<00:04, 110.94it/s, loss=0.598, iou=0.4, dice=0.532]  \u001b[A\n",
      "Validating and Predicting:  21%|██        | 127/614 [00:01<00:04, 110.94it/s, loss=0.601, iou=0.397, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  21%|██        | 128/614 [00:01<00:04, 110.94it/s, loss=0.601, iou=0.395, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  21%|██        | 129/614 [00:01<00:04, 110.94it/s, loss=0.6, iou=0.395, dice=0.527]  \u001b[A\n",
      "Validating and Predicting:  21%|██        | 130/614 [00:01<00:04, 110.94it/s, loss=0.601, iou=0.392, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  21%|██▏       | 131/614 [00:01<00:04, 110.94it/s, loss=0.599, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  21%|██▏       | 132/614 [00:01<00:04, 110.94it/s, loss=0.6, iou=0.393, dice=0.523]  \u001b[A\n",
      "Validating and Predicting:  22%|██▏       | 133/614 [00:01<00:04, 110.94it/s, loss=0.599, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  22%|██▏       | 134/614 [00:01<00:04, 113.53it/s, loss=0.599, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  22%|██▏       | 134/614 [00:01<00:04, 113.53it/s, loss=0.597, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  22%|██▏       | 135/614 [00:01<00:04, 113.53it/s, loss=0.598, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  22%|██▏       | 136/614 [00:01<00:04, 113.53it/s, loss=0.596, iou=0.395, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  22%|██▏       | 137/614 [00:01<00:04, 113.53it/s, loss=0.595, iou=0.396, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  22%|██▏       | 138/614 [00:01<00:04, 113.53it/s, loss=0.594, iou=0.397, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  23%|██▎       | 139/614 [00:01<00:04, 113.53it/s, loss=0.595, iou=0.396, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  23%|██▎       | 140/614 [00:01<00:04, 113.53it/s, loss=0.593, iou=0.399, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  23%|██▎       | 141/614 [00:01<00:04, 113.53it/s, loss=0.591, iou=0.401, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  23%|██▎       | 142/614 [00:01<00:04, 113.53it/s, loss=0.59, iou=0.401, dice=0.532] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0139\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0306\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0080\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0483\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0373\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0540\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0096\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0499\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0263\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0124\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0430\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0527\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0014\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0320\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0417\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0307\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0081\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0484\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0374\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0541\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0097\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0264\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0125\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0431\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0528\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  23%|██▎       | 143/614 [00:01<00:04, 113.53it/s, loss=0.59, iou=0.401, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  23%|██▎       | 144/614 [00:01<00:04, 113.53it/s, loss=0.591, iou=0.398, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  24%|██▎       | 145/614 [00:01<00:04, 113.53it/s, loss=0.589, iou=0.401, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  24%|██▍       | 146/614 [00:01<00:04, 114.38it/s, loss=0.589, iou=0.401, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  24%|██▍       | 146/614 [00:01<00:04, 114.38it/s, loss=0.587, iou=0.402, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  24%|██▍       | 147/614 [00:01<00:04, 114.38it/s, loss=0.588, iou=0.403, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  24%|██▍       | 148/614 [00:01<00:04, 114.38it/s, loss=0.588, iou=0.403, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  24%|██▍       | 149/614 [00:01<00:04, 114.38it/s, loss=0.587, iou=0.404, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  24%|██▍       | 150/614 [00:01<00:04, 114.38it/s, loss=0.589, iou=0.405, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  25%|██▍       | 151/614 [00:01<00:04, 114.38it/s, loss=0.589, iou=0.403, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  25%|██▍       | 152/614 [00:01<00:04, 114.38it/s, loss=0.588, iou=0.404, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  25%|██▍       | 153/614 [00:01<00:04, 114.38it/s, loss=0.587, iou=0.405, dice=0.537]\u001b[A\n",
      "Validating and Predicting:  25%|██▌       | 154/614 [00:01<00:04, 114.38it/s, loss=0.586, iou=0.406, dice=0.538]\u001b[A\n",
      "Validating and Predicting:  25%|██▌       | 155/614 [00:01<00:04, 114.38it/s, loss=0.584, iou=0.408, dice=0.54] \u001b[A\n",
      "Validating and Predicting:  25%|██▌       | 156/614 [00:01<00:04, 114.38it/s, loss=0.586, iou=0.406, dice=0.537]\u001b[A\n",
      "Validating and Predicting:  26%|██▌       | 157/614 [00:01<00:03, 114.38it/s, loss=0.586, iou=0.407, dice=0.538]\u001b[A\n",
      "Validating and Predicting:  26%|██▌       | 158/614 [00:01<00:03, 114.38it/s, loss=0.585, iou=0.406, dice=0.538]\u001b[A\n",
      "Validating and Predicting:  26%|██▌       | 159/614 [00:01<00:03, 114.38it/s, loss=0.586, iou=0.407, dice=0.538]\u001b[A\n",
      "Validating and Predicting:  26%|██▌       | 160/614 [00:01<00:03, 120.06it/s, loss=0.586, iou=0.407, dice=0.538]\u001b[A\n",
      "Validating and Predicting:  26%|██▌       | 160/614 [00:01<00:03, 120.06it/s, loss=0.587, iou=0.407, dice=0.539]\u001b[A\n",
      "Validating and Predicting:  26%|██▌       | 161/614 [00:01<00:03, 120.06it/s, loss=0.585, iou=0.409, dice=0.541]\u001b[A\n",
      "Validating and Predicting:  26%|██▋       | 162/614 [00:01<00:03, 120.06it/s, loss=0.585, iou=0.409, dice=0.541]\u001b[A\n",
      "Validating and Predicting:  27%|██▋       | 163/614 [00:01<00:03, 120.06it/s, loss=0.584, iou=0.41, dice=0.542] \u001b[A\n",
      "Validating and Predicting:  27%|██▋       | 164/614 [00:01<00:03, 120.06it/s, loss=0.583, iou=0.412, dice=0.544]\u001b[A\n",
      "Validating and Predicting:  27%|██▋       | 165/614 [00:01<00:03, 120.06it/s, loss=0.583, iou=0.409, dice=0.541]\u001b[A\n",
      "Validating and Predicting:  27%|██▋       | 166/614 [00:01<00:03, 120.06it/s, loss=0.583, iou=0.408, dice=0.54] \u001b[A\n",
      "Validating and Predicting:  27%|██▋       | 167/614 [00:01<00:03, 120.06it/s, loss=0.584, iou=0.407, dice=0.538]\u001b[A\n",
      "Validating and Predicting:  27%|██▋       | 168/614 [00:01<00:03, 120.06it/s, loss=0.584, iou=0.405, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  28%|██▊       | 169/614 [00:01<00:03, 120.06it/s, loss=0.584, iou=0.405, dice=0.537]\u001b[A\n",
      "Validating and Predicting:  28%|██▊       | 170/614 [00:01<00:03, 120.06it/s, loss=0.584, iou=0.403, dice=0.534]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0015\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0321\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0418\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0308\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0082\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0485\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0375\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0542\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0265\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0126\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0432\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0529\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0016\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0322\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0419\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0309\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0083\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0486\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0250\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0376\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0140\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0543\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0266\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0030\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0127\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0433\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0600\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  28%|██▊       | 171/614 [00:01<00:03, 120.06it/s, loss=0.584, iou=0.402, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  28%|██▊       | 172/614 [00:01<00:03, 120.06it/s, loss=0.583, iou=0.403, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  28%|██▊       | 173/614 [00:01<00:03, 118.95it/s, loss=0.583, iou=0.403, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  28%|██▊       | 173/614 [00:01<00:03, 118.95it/s, loss=0.582, iou=0.405, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  28%|██▊       | 174/614 [00:01<00:03, 118.95it/s, loss=0.581, iou=0.406, dice=0.537]\u001b[A\n",
      "Validating and Predicting:  29%|██▊       | 175/614 [00:01<00:03, 118.95it/s, loss=0.58, iou=0.407, dice=0.538] \u001b[A\n",
      "Validating and Predicting:  29%|██▊       | 176/614 [00:01<00:03, 118.95it/s, loss=0.581, iou=0.405, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  29%|██▉       | 177/614 [00:01<00:03, 118.95it/s, loss=0.58, iou=0.407, dice=0.537] \u001b[A\n",
      "Validating and Predicting:  29%|██▉       | 178/614 [00:01<00:03, 118.95it/s, loss=0.583, iou=0.406, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  29%|██▉       | 179/614 [00:01<00:03, 118.95it/s, loss=0.584, iou=0.405, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  29%|██▉       | 180/614 [00:01<00:03, 118.95it/s, loss=0.584, iou=0.404, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  29%|██▉       | 181/614 [00:01<00:03, 118.95it/s, loss=0.585, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  30%|██▉       | 182/614 [00:01<00:03, 118.95it/s, loss=0.585, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  30%|██▉       | 183/614 [00:01<00:03, 118.95it/s, loss=0.584, iou=0.402, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  30%|██▉       | 184/614 [00:01<00:03, 118.95it/s, loss=0.584, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  30%|███       | 185/614 [00:01<00:03, 114.05it/s, loss=0.584, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  30%|███       | 185/614 [00:01<00:03, 114.05it/s, loss=0.586, iou=0.4, dice=0.53]   \u001b[A\n",
      "Validating and Predicting:  30%|███       | 186/614 [00:01<00:03, 114.05it/s, loss=0.588, iou=0.398, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  30%|███       | 187/614 [00:01<00:03, 114.05it/s, loss=0.588, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  31%|███       | 188/614 [00:01<00:03, 114.05it/s, loss=0.587, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  31%|███       | 189/614 [00:01<00:03, 114.05it/s, loss=0.586, iou=0.398, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  31%|███       | 190/614 [00:01<00:03, 114.05it/s, loss=0.587, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  31%|███       | 191/614 [00:01<00:03, 114.05it/s, loss=0.586, iou=0.398, dice=0.528]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0017\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0323\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0084\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0390\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0487\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0251\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0377\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0141\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0544\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0267\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0031\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0128\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0434\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0601\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0018\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0085\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0391\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0488\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0252\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0378\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0142\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  31%|███▏      | 192/614 [00:01<00:03, 114.05it/s, loss=0.587, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  31%|███▏      | 193/614 [00:01<00:03, 114.05it/s, loss=0.589, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  32%|███▏      | 194/614 [00:01<00:03, 114.05it/s, loss=0.59, iou=0.394, dice=0.523] \u001b[A\n",
      "Validating and Predicting:  32%|███▏      | 195/614 [00:01<00:03, 114.05it/s, loss=0.589, iou=0.394, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  32%|███▏      | 196/614 [00:01<00:03, 114.05it/s, loss=0.589, iou=0.394, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  32%|███▏      | 197/614 [00:01<00:03, 114.82it/s, loss=0.589, iou=0.394, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  32%|███▏      | 197/614 [00:01<00:03, 114.82it/s, loss=0.589, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  32%|███▏      | 198/614 [00:01<00:03, 114.82it/s, loss=0.589, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  32%|███▏      | 199/614 [00:01<00:03, 114.82it/s, loss=0.588, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  33%|███▎      | 200/614 [00:01<00:03, 114.82it/s, loss=0.589, iou=0.391, dice=0.521]\u001b[A\n",
      "Validating and Predicting:  33%|███▎      | 201/614 [00:02<00:03, 114.82it/s, loss=0.589, iou=0.39, dice=0.519] \u001b[A\n",
      "Validating and Predicting:  33%|███▎      | 202/614 [00:02<00:03, 114.82it/s, loss=0.59, iou=0.389, dice=0.519]\u001b[A\n",
      "Validating and Predicting:  33%|███▎      | 203/614 [00:02<00:03, 114.82it/s, loss=0.589, iou=0.391, dice=0.52]\u001b[A\n",
      "Validating and Predicting:  33%|███▎      | 204/614 [00:02<00:03, 114.82it/s, loss=0.588, iou=0.391, dice=0.521]\u001b[A\n",
      "Validating and Predicting:  33%|███▎      | 205/614 [00:02<00:03, 114.82it/s, loss=0.588, iou=0.391, dice=0.52] \u001b[A\n",
      "Validating and Predicting:  34%|███▎      | 206/614 [00:02<00:03, 114.82it/s, loss=0.588, iou=0.389, dice=0.519]\u001b[A\n",
      "Validating and Predicting:  34%|███▎      | 207/614 [00:02<00:03, 114.82it/s, loss=0.588, iou=0.39, dice=0.52]  \u001b[A\n",
      "Validating and Predicting:  34%|███▍      | 208/614 [00:02<00:03, 114.82it/s, loss=0.588, iou=0.389, dice=0.519]\u001b[A\n",
      "Validating and Predicting:  34%|███▍      | 209/614 [00:02<00:03, 114.82it/s, loss=0.588, iou=0.39, dice=0.52]  \u001b[A\n",
      "Validating and Predicting:  34%|███▍      | 210/614 [00:02<00:03, 117.15it/s, loss=0.588, iou=0.39, dice=0.52]\u001b[A\n",
      "Validating and Predicting:  34%|███▍      | 210/614 [00:02<00:03, 117.15it/s, loss=0.588, iou=0.388, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  34%|███▍      | 211/614 [00:02<00:03, 117.15it/s, loss=0.588, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  35%|███▍      | 212/614 [00:02<00:03, 117.15it/s, loss=0.59, iou=0.386, dice=0.515] \u001b[A\n",
      "Validating and Predicting:  35%|███▍      | 213/614 [00:02<00:03, 117.15it/s, loss=0.589, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  35%|███▍      | 214/614 [00:02<00:03, 117.15it/s, loss=0.588, iou=0.388, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  35%|███▌      | 215/614 [00:02<00:03, 117.15it/s, loss=0.589, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  35%|███▌      | 216/614 [00:02<00:03, 117.15it/s, loss=0.588, iou=0.388, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  35%|███▌      | 217/614 [00:02<00:03, 117.15it/s, loss=0.587, iou=0.389, dice=0.519]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0545\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0268\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0032\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0129\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0435\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0602\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0019\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0086\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0392\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0489\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0253\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0379\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0143\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0546\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0310\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0269\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0033\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0436\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0200\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0603\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0087\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0393\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0254\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0560\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0144\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0547\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  36%|███▌      | 218/614 [00:02<00:03, 117.15it/s, loss=0.586, iou=0.39, dice=0.52]  \u001b[A\n",
      "Validating and Predicting:  36%|███▌      | 219/614 [00:02<00:03, 117.15it/s, loss=0.585, iou=0.39, dice=0.52]\u001b[A\n",
      "Validating and Predicting:  36%|███▌      | 220/614 [00:02<00:03, 117.15it/s, loss=0.585, iou=0.39, dice=0.52]\u001b[A\n",
      "Validating and Predicting:  36%|███▌      | 221/614 [00:02<00:03, 117.15it/s, loss=0.585, iou=0.389, dice=0.519]\u001b[A\n",
      "Validating and Predicting:  36%|███▌      | 222/614 [00:02<00:03, 117.15it/s, loss=0.586, iou=0.389, dice=0.519]\u001b[A\n",
      "Validating and Predicting:  36%|███▋      | 223/614 [00:02<00:03, 117.15it/s, loss=0.585, iou=0.389, dice=0.52] \u001b[A\n",
      "Validating and Predicting:  36%|███▋      | 224/614 [00:02<00:03, 121.26it/s, loss=0.585, iou=0.389, dice=0.52]\u001b[A\n",
      "Validating and Predicting:  36%|███▋      | 224/614 [00:02<00:03, 121.26it/s, loss=0.586, iou=0.388, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  37%|███▋      | 225/614 [00:02<00:03, 121.26it/s, loss=0.586, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  37%|███▋      | 226/614 [00:02<00:03, 121.26it/s, loss=0.585, iou=0.387, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  37%|███▋      | 227/614 [00:02<00:03, 121.26it/s, loss=0.584, iou=0.387, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  37%|███▋      | 228/614 [00:02<00:03, 121.26it/s, loss=0.583, iou=0.389, dice=0.519]\u001b[A\n",
      "Validating and Predicting:  37%|███▋      | 229/614 [00:02<00:03, 121.26it/s, loss=0.584, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  37%|███▋      | 230/614 [00:02<00:03, 121.26it/s, loss=0.584, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  38%|███▊      | 231/614 [00:02<00:03, 121.26it/s, loss=0.585, iou=0.384, dice=0.514]\u001b[A\n",
      "Validating and Predicting:  38%|███▊      | 232/614 [00:02<00:03, 121.26it/s, loss=0.583, iou=0.386, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  38%|███▊      | 233/614 [00:02<00:03, 121.26it/s, loss=0.583, iou=0.387, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  38%|███▊      | 234/614 [00:02<00:03, 121.26it/s, loss=0.584, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  38%|███▊      | 235/614 [00:02<00:03, 121.26it/s, loss=0.582, iou=0.387, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  38%|███▊      | 236/614 [00:02<00:03, 121.26it/s, loss=0.583, iou=0.386, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  39%|███▊      | 237/614 [00:02<00:03, 121.26it/s, loss=0.584, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  39%|███▉      | 238/614 [00:02<00:03, 121.26it/s, loss=0.583, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  39%|███▉      | 239/614 [00:02<00:02, 126.03it/s, loss=0.583, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  39%|███▉      | 239/614 [00:02<00:02, 126.03it/s, loss=0.583, iou=0.385, dice=0.514]\u001b[A\n",
      "Validating and Predicting:  39%|███▉      | 240/614 [00:02<00:02, 126.03it/s, loss=0.583, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  39%|███▉      | 241/614 [00:02<00:02, 126.03it/s, loss=0.582, iou=0.385, dice=0.514]\u001b[A\n",
      "Validating and Predicting:  39%|███▉      | 242/614 [00:02<00:02, 126.03it/s, loss=0.583, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  40%|███▉      | 243/614 [00:02<00:02, 126.03it/s, loss=0.583, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  40%|███▉      | 244/614 [00:02<00:02, 126.03it/s, loss=0.583, iou=0.383, dice=0.512]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0311\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0034\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0437\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0201\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0604\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0088\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0394\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0255\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0561\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0145\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0548\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0312\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0035\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0438\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0202\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0605\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0199\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0089\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0395\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0256\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0562\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0146\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0549\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0313\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0036\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0439\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0203\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0606\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  40%|███▉      | 245/614 [00:02<00:02, 126.03it/s, loss=0.583, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  40%|████      | 246/614 [00:02<00:02, 126.03it/s, loss=0.583, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  40%|████      | 247/614 [00:02<00:02, 126.03it/s, loss=0.582, iou=0.385, dice=0.514]\u001b[A\n",
      "Validating and Predicting:  40%|████      | 248/614 [00:02<00:02, 126.03it/s, loss=0.582, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  41%|████      | 249/614 [00:02<00:02, 126.03it/s, loss=0.581, iou=0.386, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  41%|████      | 250/614 [00:02<00:02, 126.03it/s, loss=0.581, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  41%|████      | 251/614 [00:02<00:02, 126.03it/s, loss=0.581, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  41%|████      | 252/614 [00:02<00:02, 126.03it/s, loss=0.582, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  41%|████      | 253/614 [00:02<00:02, 126.03it/s, loss=0.58, iou=0.387, dice=0.517] \u001b[A\n",
      "Validating and Predicting:  41%|████▏     | 254/614 [00:02<00:02, 132.19it/s, loss=0.58, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  41%|████▏     | 254/614 [00:02<00:02, 132.19it/s, loss=0.581, iou=0.386, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  42%|████▏     | 255/614 [00:02<00:02, 132.19it/s, loss=0.58, iou=0.387, dice=0.516] \u001b[A\n",
      "Validating and Predicting:  42%|████▏     | 256/614 [00:02<00:02, 132.19it/s, loss=0.58, iou=0.387, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  42%|████▏     | 257/614 [00:02<00:02, 132.19it/s, loss=0.58, iou=0.387, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  42%|████▏     | 258/614 [00:02<00:02, 132.19it/s, loss=0.581, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  42%|████▏     | 259/614 [00:02<00:02, 132.19it/s, loss=0.581, iou=0.386, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  42%|████▏     | 260/614 [00:02<00:02, 132.19it/s, loss=0.581, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  43%|████▎     | 261/614 [00:02<00:02, 132.19it/s, loss=0.581, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  43%|████▎     | 262/614 [00:02<00:02, 132.19it/s, loss=0.58, iou=0.386, dice=0.516] \u001b[A\n",
      "Validating and Predicting:  43%|████▎     | 263/614 [00:02<00:02, 132.19it/s, loss=0.581, iou=0.386, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  43%|████▎     | 264/614 [00:02<00:02, 132.19it/s, loss=0.581, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  43%|████▎     | 265/614 [00:02<00:02, 132.19it/s, loss=0.58, iou=0.386, dice=0.516] \u001b[A\n",
      "Validating and Predicting:  43%|████▎     | 266/614 [00:02<00:02, 132.19it/s, loss=0.579, iou=0.388, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  43%|████▎     | 267/614 [00:02<00:02, 132.19it/s, loss=0.579, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  44%|████▎     | 268/614 [00:02<00:02, 131.91it/s, loss=0.579, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  44%|████▎     | 268/614 [00:02<00:02, 131.91it/s, loss=0.58, iou=0.387, dice=0.517] \u001b[A\n",
      "Validating and Predicting:  44%|████▍     | 269/614 [00:02<00:02, 131.91it/s, loss=0.58, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  44%|████▍     | 270/614 [00:02<00:02, 131.91it/s, loss=0.58, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  44%|████▍     | 271/614 [00:02<00:02, 131.91it/s, loss=0.579, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  44%|████▍     | 272/614 [00:02<00:02, 131.91it/s, loss=0.578, iou=0.388, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  44%|████▍     | 273/614 [00:02<00:02, 131.91it/s, loss=0.578, iou=0.389, dice=0.519]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0380\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0270\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0396\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0160\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0257\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0563\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0147\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0314\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0037\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0204\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0607\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0381\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0271\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0397\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0161\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0258\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0564\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0022\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0425\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0148\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0315\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0038\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0205\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0608\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0382\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0272\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0398\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0162\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "Validating and Predicting:  45%|████▍     | 274/614 [00:02<00:02, 131.91it/s, loss=0.578, iou=0.388, dice=0.519]\u001b[A\n",
      "Validating and Predicting:  45%|████▍     | 275/614 [00:02<00:02, 131.91it/s, loss=0.578, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  45%|████▍     | 276/614 [00:02<00:02, 131.91it/s, loss=0.577, iou=0.388, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  45%|████▌     | 277/614 [00:02<00:02, 131.91it/s, loss=0.578, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  45%|████▌     | 278/614 [00:02<00:02, 131.91it/s, loss=0.578, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  45%|████▌     | 279/614 [00:02<00:02, 131.91it/s, loss=0.577, iou=0.387, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  46%|████▌     | 280/614 [00:02<00:02, 131.91it/s, loss=0.577, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  46%|████▌     | 281/614 [00:02<00:02, 131.91it/s, loss=0.577, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  46%|████▌     | 282/614 [00:02<00:02, 132.76it/s, loss=0.577, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  46%|████▌     | 282/614 [00:02<00:02, 132.76it/s, loss=0.578, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  46%|████▌     | 283/614 [00:02<00:02, 132.76it/s, loss=0.579, iou=0.384, dice=0.514]\u001b[A\n",
      "Validating and Predicting:  46%|████▋     | 284/614 [00:02<00:02, 132.76it/s, loss=0.579, iou=0.383, dice=0.512]\u001b[A\n",
      "Validating and Predicting:  46%|████▋     | 285/614 [00:02<00:02, 132.76it/s, loss=0.578, iou=0.384, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  47%|████▋     | 286/614 [00:02<00:02, 132.76it/s, loss=0.578, iou=0.384, dice=0.514]\u001b[A\n",
      "Validating and Predicting:  47%|████▋     | 287/614 [00:02<00:02, 132.76it/s, loss=0.578, iou=0.383, dice=0.513]\u001b[A\n",
      "Validating and Predicting:  47%|████▋     | 288/614 [00:02<00:02, 132.76it/s, loss=0.577, iou=0.384, dice=0.514]\u001b[A\n",
      "Validating and Predicting:  47%|████▋     | 289/614 [00:02<00:02, 132.76it/s, loss=0.578, iou=0.384, dice=0.514]\u001b[A\n",
      "Validating and Predicting:  47%|████▋     | 290/614 [00:02<00:02, 132.76it/s, loss=0.577, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  47%|████▋     | 291/614 [00:02<00:02, 132.76it/s, loss=0.577, iou=0.386, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  48%|████▊     | 292/614 [00:02<00:02, 132.76it/s, loss=0.576, iou=0.386, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  48%|████▊     | 293/614 [00:02<00:02, 132.76it/s, loss=0.576, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  48%|████▊     | 294/614 [00:02<00:02, 132.76it/s, loss=0.576, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  48%|████▊     | 295/614 [00:02<00:02, 132.76it/s, loss=0.576, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  48%|████▊     | 296/614 [00:02<00:02, 129.98it/s, loss=0.576, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  48%|████▊     | 296/614 [00:02<00:02, 129.98it/s, loss=0.576, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  48%|████▊     | 297/614 [00:02<00:02, 129.98it/s, loss=0.575, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  49%|████▊     | 298/614 [00:02<00:02, 129.98it/s, loss=0.574, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  49%|████▊     | 299/614 [00:02<00:02, 129.98it/s, loss=0.574, iou=0.387, dice=0.517]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0259\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0565\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0023\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0426\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0149\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0316\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0039\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0206\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0609\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0383\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0550\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0273\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0440\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0399\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0163\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0566\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0024\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0330\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0427\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0220\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0317\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0207\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0384\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0551\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0274\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0441\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  49%|████▉     | 300/614 [00:02<00:02, 129.98it/s, loss=0.574, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  49%|████▉     | 301/614 [00:02<00:02, 129.98it/s, loss=0.575, iou=0.385, dice=0.515]\u001b[A\n",
      "Validating and Predicting:  49%|████▉     | 302/614 [00:02<00:02, 129.98it/s, loss=0.574, iou=0.386, dice=0.516]\u001b[A\n",
      "Validating and Predicting:  49%|████▉     | 303/614 [00:02<00:02, 129.98it/s, loss=0.574, iou=0.387, dice=0.517]\u001b[A\n",
      "Validating and Predicting:  50%|████▉     | 304/614 [00:02<00:02, 129.98it/s, loss=0.573, iou=0.388, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  50%|████▉     | 305/614 [00:02<00:02, 129.98it/s, loss=0.573, iou=0.388, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  50%|████▉     | 306/614 [00:02<00:02, 129.98it/s, loss=0.573, iou=0.388, dice=0.518]\u001b[A\n",
      "Validating and Predicting:  50%|█████     | 307/614 [00:02<00:02, 129.98it/s, loss=0.572, iou=0.389, dice=0.519]\u001b[A\n",
      "Validating and Predicting:  50%|█████     | 308/614 [00:02<00:02, 129.98it/s, loss=0.572, iou=0.389, dice=0.519]\u001b[A\n",
      "Validating and Predicting:  50%|█████     | 309/614 [00:02<00:02, 129.98it/s, loss=0.572, iou=0.39, dice=0.52]  \u001b[A\n",
      "Validating and Predicting:  50%|█████     | 310/614 [00:02<00:02, 130.84it/s, loss=0.572, iou=0.39, dice=0.52]\u001b[A\n",
      "Validating and Predicting:  50%|█████     | 310/614 [00:02<00:02, 130.84it/s, loss=0.572, iou=0.389, dice=0.52]\u001b[A\n",
      "Validating and Predicting:  51%|█████     | 311/614 [00:02<00:02, 130.84it/s, loss=0.573, iou=0.39, dice=0.52] \u001b[A\n",
      "Validating and Predicting:  51%|█████     | 312/614 [00:02<00:02, 130.84it/s, loss=0.572, iou=0.391, dice=0.521]\u001b[A\n",
      "Validating and Predicting:  51%|█████     | 313/614 [00:02<00:02, 130.84it/s, loss=0.572, iou=0.391, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  51%|█████     | 314/614 [00:02<00:02, 130.84it/s, loss=0.572, iou=0.392, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  51%|█████▏    | 315/614 [00:02<00:02, 130.84it/s, loss=0.571, iou=0.392, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  51%|█████▏    | 316/614 [00:02<00:02, 130.84it/s, loss=0.571, iou=0.392, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  52%|█████▏    | 317/614 [00:02<00:02, 130.84it/s, loss=0.571, iou=0.392, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  52%|█████▏    | 318/614 [00:02<00:02, 130.84it/s, loss=0.571, iou=0.392, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  52%|█████▏    | 319/614 [00:02<00:02, 130.84it/s, loss=0.57, iou=0.393, dice=0.523] \u001b[A\n",
      "Validating and Predicting:  52%|█████▏    | 320/614 [00:02<00:02, 130.84it/s, loss=0.569, iou=0.394, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  52%|█████▏    | 321/614 [00:02<00:02, 130.84it/s, loss=0.569, iou=0.394, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  52%|█████▏    | 322/614 [00:02<00:02, 130.84it/s, loss=0.569, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  53%|█████▎    | 323/614 [00:02<00:02, 130.84it/s, loss=0.569, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  53%|█████▎    | 324/614 [00:02<00:02, 126.47it/s, loss=0.569, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  53%|█████▎    | 324/614 [00:02<00:02, 126.47it/s, loss=0.568, iou=0.394, dice=0.524]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0164\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0567\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0025\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0331\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0428\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0221\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0318\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0208\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0385\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0552\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0275\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0442\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0165\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0568\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0026\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0332\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0429\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0222\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0319\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0209\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0386\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0150\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0553\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0276\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0040\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  53%|█████▎    | 325/614 [00:02<00:02, 126.47it/s, loss=0.57, iou=0.394, dice=0.524] \u001b[A\n",
      "Validating and Predicting:  53%|█████▎    | 326/614 [00:02<00:02, 126.47it/s, loss=0.571, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  53%|█████▎    | 327/614 [00:02<00:02, 126.47it/s, loss=0.57, iou=0.394, dice=0.524] \u001b[A\n",
      "Validating and Predicting:  53%|█████▎    | 328/614 [00:02<00:02, 126.47it/s, loss=0.57, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  54%|█████▎    | 329/614 [00:02<00:02, 126.47it/s, loss=0.57, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  54%|█████▎    | 330/614 [00:02<00:02, 126.47it/s, loss=0.57, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  54%|█████▍    | 331/614 [00:03<00:02, 126.47it/s, loss=0.57, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  54%|█████▍    | 332/614 [00:03<00:02, 126.47it/s, loss=0.57, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  54%|█████▍    | 333/614 [00:03<00:02, 126.47it/s, loss=0.571, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  54%|█████▍    | 334/614 [00:03<00:02, 126.47it/s, loss=0.571, iou=0.392, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  55%|█████▍    | 335/614 [00:03<00:02, 126.47it/s, loss=0.57, iou=0.393, dice=0.523] \u001b[A\n",
      "Validating and Predicting:  55%|█████▍    | 336/614 [00:03<00:02, 126.47it/s, loss=0.571, iou=0.392, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  55%|█████▍    | 337/614 [00:03<00:02, 126.54it/s, loss=0.571, iou=0.392, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  55%|█████▍    | 337/614 [00:03<00:02, 126.54it/s, loss=0.57, iou=0.392, dice=0.522] \u001b[A\n",
      "Validating and Predicting:  55%|█████▌    | 338/614 [00:03<00:02, 126.54it/s, loss=0.57, iou=0.392, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  55%|█████▌    | 339/614 [00:03<00:02, 126.54it/s, loss=0.57, iou=0.391, dice=0.521]\u001b[A\n",
      "Validating and Predicting:  55%|█████▌    | 340/614 [00:03<00:02, 126.54it/s, loss=0.57, iou=0.392, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  56%|█████▌    | 341/614 [00:03<00:02, 126.54it/s, loss=0.57, iou=0.392, dice=0.522]\u001b[A\n",
      "Validating and Predicting:  56%|█████▌    | 342/614 [00:03<00:02, 126.54it/s, loss=0.57, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  56%|█████▌    | 343/614 [00:03<00:02, 126.54it/s, loss=0.569, iou=0.393, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  56%|█████▌    | 344/614 [00:03<00:02, 126.54it/s, loss=0.57, iou=0.393, dice=0.524] \u001b[A\n",
      "Validating and Predicting:  56%|█████▌    | 345/614 [00:03<00:02, 126.54it/s, loss=0.569, iou=0.394, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  56%|█████▋    | 346/614 [00:03<00:02, 126.54it/s, loss=0.569, iou=0.394, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  57%|█████▋    | 347/614 [00:03<00:02, 126.54it/s, loss=0.568, iou=0.395, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  57%|█████▋    | 348/614 [00:03<00:02, 126.54it/s, loss=0.568, iou=0.394, dice=0.525]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0443\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0610\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0166\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0569\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0027\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0333\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0500\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0223\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0290\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0387\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0151\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0554\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0277\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0041\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0444\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0611\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0167\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0028\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0334\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0501\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0291\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0388\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0152\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0555\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0278\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  57%|█████▋    | 349/614 [00:03<00:02, 126.54it/s, loss=0.567, iou=0.395, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  57%|█████▋    | 350/614 [00:03<00:02, 123.33it/s, loss=0.567, iou=0.395, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  57%|█████▋    | 350/614 [00:03<00:02, 123.33it/s, loss=0.568, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  57%|█████▋    | 351/614 [00:03<00:02, 123.33it/s, loss=0.568, iou=0.395, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  57%|█████▋    | 352/614 [00:03<00:02, 123.33it/s, loss=0.568, iou=0.394, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  57%|█████▋    | 353/614 [00:03<00:02, 123.33it/s, loss=0.568, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  58%|█████▊    | 354/614 [00:03<00:02, 123.33it/s, loss=0.567, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  58%|█████▊    | 355/614 [00:03<00:02, 123.33it/s, loss=0.567, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  58%|█████▊    | 356/614 [00:03<00:02, 123.33it/s, loss=0.567, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  58%|█████▊    | 357/614 [00:03<00:02, 123.33it/s, loss=0.566, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  58%|█████▊    | 358/614 [00:03<00:02, 123.33it/s, loss=0.566, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  58%|█████▊    | 359/614 [00:03<00:02, 123.33it/s, loss=0.565, iou=0.398, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  59%|█████▊    | 360/614 [00:03<00:02, 123.33it/s, loss=0.565, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  59%|█████▉    | 361/614 [00:03<00:02, 123.33it/s, loss=0.566, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  59%|█████▉    | 362/614 [00:03<00:02, 123.33it/s, loss=0.565, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  59%|█████▉    | 363/614 [00:03<00:02, 123.33it/s, loss=0.565, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  59%|█████▉    | 364/614 [00:03<00:01, 127.74it/s, loss=0.565, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  59%|█████▉    | 364/614 [00:03<00:01, 127.74it/s, loss=0.564, iou=0.399, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  59%|█████▉    | 365/614 [00:03<00:01, 127.74it/s, loss=0.564, iou=0.398, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  60%|█████▉    | 366/614 [00:03<00:01, 127.74it/s, loss=0.564, iou=0.399, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  60%|█████▉    | 367/614 [00:03<00:01, 127.74it/s, loss=0.563, iou=0.401, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  60%|█████▉    | 368/614 [00:03<00:01, 127.74it/s, loss=0.562, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  60%|██████    | 369/614 [00:03<00:01, 127.74it/s, loss=0.562, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  60%|██████    | 370/614 [00:03<00:01, 127.74it/s, loss=0.562, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  60%|██████    | 371/614 [00:03<00:01, 127.74it/s, loss=0.563, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  61%|██████    | 372/614 [00:03<00:01, 127.74it/s, loss=0.562, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  61%|██████    | 373/614 [00:03<00:01, 127.74it/s, loss=0.563, iou=0.4, dice=0.53]   \u001b[A\n",
      "Validating and Predicting:  61%|██████    | 374/614 [00:03<00:01, 127.74it/s, loss=0.563, iou=0.399, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  61%|██████    | 375/614 [00:03<00:01, 127.74it/s, loss=0.563, iou=0.399, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  61%|██████    | 376/614 [00:03<00:01, 127.74it/s, loss=0.563, iou=0.399, dice=0.528]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0042\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0445\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0612\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0168\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0029\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0335\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0502\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0292\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0389\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0153\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0556\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0279\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0043\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0446\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0210\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0613\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0169\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0336\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0100\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0503\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0570\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0293\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0154\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0460\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0557\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0044\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0447\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0211\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  61%|██████▏   | 377/614 [00:03<00:01, 127.74it/s, loss=0.564, iou=0.398, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  62%|██████▏   | 378/614 [00:03<00:01, 128.87it/s, loss=0.564, iou=0.398, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  62%|██████▏   | 378/614 [00:03<00:01, 128.87it/s, loss=0.565, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  62%|██████▏   | 379/614 [00:03<00:01, 128.87it/s, loss=0.565, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  62%|██████▏   | 380/614 [00:03<00:01, 128.87it/s, loss=0.566, iou=0.396, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  62%|██████▏   | 381/614 [00:03<00:01, 128.87it/s, loss=0.565, iou=0.396, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  62%|██████▏   | 382/614 [00:03<00:01, 128.87it/s, loss=0.564, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  62%|██████▏   | 383/614 [00:03<00:01, 128.87it/s, loss=0.564, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  63%|██████▎   | 384/614 [00:03<00:01, 128.87it/s, loss=0.563, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  63%|██████▎   | 385/614 [00:03<00:01, 128.87it/s, loss=0.564, iou=0.398, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  63%|██████▎   | 386/614 [00:03<00:01, 128.87it/s, loss=0.563, iou=0.399, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  63%|██████▎   | 387/614 [00:03<00:01, 128.87it/s, loss=0.563, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  63%|██████▎   | 388/614 [00:03<00:01, 128.87it/s, loss=0.563, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  63%|██████▎   | 389/614 [00:03<00:01, 128.87it/s, loss=0.563, iou=0.399, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  64%|██████▎   | 390/614 [00:03<00:01, 128.87it/s, loss=0.563, iou=0.399, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  64%|██████▎   | 391/614 [00:03<00:01, 126.09it/s, loss=0.563, iou=0.399, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  64%|██████▎   | 391/614 [00:03<00:01, 126.09it/s, loss=0.563, iou=0.399, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  64%|██████▍   | 392/614 [00:03<00:01, 126.09it/s, loss=0.563, iou=0.399, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  64%|██████▍   | 393/614 [00:03<00:01, 126.09it/s, loss=0.562, iou=0.4, dice=0.529]  \u001b[A\n",
      "Validating and Predicting:  64%|██████▍   | 394/614 [00:03<00:01, 126.09it/s, loss=0.562, iou=0.401, dice=0.53]\u001b[A\n",
      "Validating and Predicting:  64%|██████▍   | 395/614 [00:03<00:01, 126.09it/s, loss=0.562, iou=0.401, dice=0.53]\u001b[A\n",
      "Validating and Predicting:  64%|██████▍   | 396/614 [00:03<00:01, 126.09it/s, loss=0.561, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  65%|██████▍   | 397/614 [00:03<00:01, 126.09it/s, loss=0.561, iou=0.401, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  65%|██████▍   | 398/614 [00:03<00:01, 126.09it/s, loss=0.562, iou=0.4, dice=0.528]  \u001b[A\n",
      "Validating and Predicting:  65%|██████▍   | 399/614 [00:03<00:01, 126.09it/s, loss=0.562, iou=0.399, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  65%|██████▌   | 400/614 [00:03<00:01, 126.09it/s, loss=0.562, iou=0.4, dice=0.528]  \u001b[A\n",
      "Validating and Predicting:  65%|██████▌   | 401/614 [00:03<00:01, 126.09it/s, loss=0.561, iou=0.401, dice=0.529]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0337\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0101\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0504\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0098\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0571\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0294\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0155\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0461\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0558\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0045\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0448\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0212\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0338\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0102\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0505\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0099\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0572\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0295\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0156\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0462\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0559\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0046\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0449\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0213\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0339\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  65%|██████▌   | 402/614 [00:03<00:01, 126.09it/s, loss=0.561, iou=0.402, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  66%|██████▌   | 403/614 [00:03<00:01, 126.09it/s, loss=0.56, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  66%|██████▌   | 404/614 [00:03<00:01, 125.66it/s, loss=0.56, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  66%|██████▌   | 404/614 [00:03<00:01, 125.66it/s, loss=0.559, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  66%|██████▌   | 405/614 [00:03<00:01, 125.66it/s, loss=0.559, iou=0.404, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  66%|██████▌   | 406/614 [00:03<00:01, 125.66it/s, loss=0.559, iou=0.404, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  66%|██████▋   | 407/614 [00:03<00:01, 125.66it/s, loss=0.56, iou=0.404, dice=0.532] \u001b[A\n",
      "Validating and Predicting:  66%|██████▋   | 408/614 [00:03<00:01, 125.66it/s, loss=0.559, iou=0.405, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  67%|██████▋   | 409/614 [00:03<00:01, 125.66it/s, loss=0.559, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  67%|██████▋   | 410/614 [00:03<00:01, 125.66it/s, loss=0.558, iou=0.406, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  67%|██████▋   | 411/614 [00:03<00:01, 125.66it/s, loss=0.558, iou=0.405, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  67%|██████▋   | 412/614 [00:03<00:01, 125.66it/s, loss=0.558, iou=0.405, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  67%|██████▋   | 413/614 [00:03<00:01, 125.66it/s, loss=0.558, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  67%|██████▋   | 414/614 [00:03<00:01, 125.66it/s, loss=0.558, iou=0.406, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  68%|██████▊   | 415/614 [00:03<00:01, 125.66it/s, loss=0.558, iou=0.405, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  68%|██████▊   | 416/614 [00:03<00:01, 125.66it/s, loss=0.557, iou=0.406, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  68%|██████▊   | 417/614 [00:03<00:01, 125.66it/s, loss=0.557, iou=0.406, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  68%|██████▊   | 418/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.406, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  68%|██████▊   | 418/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.406, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  68%|██████▊   | 419/614 [00:03<00:01, 128.32it/s, loss=0.556, iou=0.407, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  68%|██████▊   | 420/614 [00:03<00:01, 128.32it/s, loss=0.556, iou=0.407, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  69%|██████▊   | 421/614 [00:03<00:01, 128.32it/s, loss=0.556, iou=0.407, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  69%|██████▊   | 422/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.406, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  69%|██████▉   | 423/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.406, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  69%|██████▉   | 424/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  69%|██████▉   | 425/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.405, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  69%|██████▉   | 426/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.405, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  70%|██████▉   | 427/614 [00:03<00:01, 128.32it/s, loss=0.556, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  70%|██████▉   | 428/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.405, dice=0.533]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0103\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0506\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0280\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0170\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0573\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0296\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0060\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0157\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0463\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0324\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0047\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0214\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0520\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0104\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0507\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0281\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0171\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0574\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0297\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0061\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0158\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0464\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0325\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0048\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0215\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0521\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  70%|██████▉   | 429/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.405, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  70%|███████   | 430/614 [00:03<00:01, 128.32it/s, loss=0.557, iou=0.405, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  70%|███████   | 431/614 [00:03<00:01, 126.17it/s, loss=0.557, iou=0.405, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  70%|███████   | 431/614 [00:03<00:01, 126.17it/s, loss=0.556, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  70%|███████   | 432/614 [00:03<00:01, 126.17it/s, loss=0.556, iou=0.406, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  71%|███████   | 433/614 [00:03<00:01, 126.17it/s, loss=0.556, iou=0.406, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  71%|███████   | 434/614 [00:03<00:01, 126.17it/s, loss=0.555, iou=0.407, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  71%|███████   | 435/614 [00:03<00:01, 126.17it/s, loss=0.555, iou=0.407, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  71%|███████   | 436/614 [00:03<00:01, 126.17it/s, loss=0.555, iou=0.407, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  71%|███████   | 437/614 [00:03<00:01, 126.17it/s, loss=0.555, iou=0.407, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  71%|███████▏  | 438/614 [00:03<00:01, 126.17it/s, loss=0.555, iou=0.406, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  71%|███████▏  | 439/614 [00:03<00:01, 126.17it/s, loss=0.555, iou=0.406, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  72%|███████▏  | 440/614 [00:03<00:01, 126.17it/s, loss=0.555, iou=0.407, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  72%|███████▏  | 441/614 [00:03<00:01, 126.17it/s, loss=0.556, iou=0.406, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  72%|███████▏  | 442/614 [00:03<00:01, 126.17it/s, loss=0.555, iou=0.406, dice=0.536]\u001b[A\n",
      "Validating and Predicting:  72%|███████▏  | 443/614 [00:03<00:01, 126.17it/s, loss=0.556, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  72%|███████▏  | 444/614 [00:03<00:01, 126.17it/s, loss=0.556, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  72%|███████▏  | 445/614 [00:03<00:01, 129.15it/s, loss=0.556, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  72%|███████▏  | 445/614 [00:03<00:01, 129.15it/s, loss=0.556, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  73%|███████▎  | 446/614 [00:03<00:01, 129.15it/s, loss=0.555, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  73%|███████▎  | 447/614 [00:03<00:01, 129.15it/s, loss=0.556, iou=0.404, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  73%|███████▎  | 448/614 [00:03<00:01, 129.15it/s, loss=0.555, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  73%|███████▎  | 449/614 [00:03<00:01, 129.15it/s, loss=0.554, iou=0.406, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  73%|███████▎  | 450/614 [00:03<00:01, 129.15it/s, loss=0.554, iou=0.406, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  73%|███████▎  | 451/614 [00:03<00:01, 129.15it/s, loss=0.554, iou=0.406, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  74%|███████▎  | 452/614 [00:03<00:01, 129.15it/s, loss=0.554, iou=0.406, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  74%|███████▍  | 453/614 [00:03<00:01, 129.15it/s, loss=0.553, iou=0.407, dice=0.535]\u001b[A\n",
      "Validating and Predicting:  74%|███████▍  | 454/614 [00:03<00:01, 129.15it/s, loss=0.554, iou=0.406, dice=0.534]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0105\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0508\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0282\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0172\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0575\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0298\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0062\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0159\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0465\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0326\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0049\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0216\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0522\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0106\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0509\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0283\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0450\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0173\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0576\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0340\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0063\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0466\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0230\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0327\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0120\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0217\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0523\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  74%|███████▍  | 455/614 [00:03<00:01, 129.15it/s, loss=0.555, iou=0.405, dice=0.534]\u001b[A\n",
      "Validating and Predicting:  74%|███████▍  | 456/614 [00:03<00:01, 129.15it/s, loss=0.555, iou=0.404, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  74%|███████▍  | 457/614 [00:03<00:01, 129.15it/s, loss=0.555, iou=0.404, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  75%|███████▍  | 458/614 [00:03<00:01, 125.87it/s, loss=0.555, iou=0.404, dice=0.533]\u001b[A\n",
      "Validating and Predicting:  75%|███████▍  | 458/614 [00:04<00:01, 125.87it/s, loss=0.555, iou=0.404, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  75%|███████▍  | 459/614 [00:04<00:01, 125.87it/s, loss=0.555, iou=0.404, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  75%|███████▍  | 460/614 [00:04<00:01, 125.87it/s, loss=0.555, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  75%|███████▌  | 461/614 [00:04<00:01, 125.87it/s, loss=0.555, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  75%|███████▌  | 462/614 [00:04<00:01, 125.87it/s, loss=0.555, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  75%|███████▌  | 463/614 [00:04<00:01, 125.87it/s, loss=0.555, iou=0.404, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  76%|███████▌  | 464/614 [00:04<00:01, 125.87it/s, loss=0.556, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  76%|███████▌  | 465/614 [00:04<00:01, 125.87it/s, loss=0.556, iou=0.402, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  76%|███████▌  | 466/614 [00:04<00:01, 125.87it/s, loss=0.556, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  76%|███████▌  | 467/614 [00:04<00:01, 125.87it/s, loss=0.556, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  76%|███████▌  | 468/614 [00:04<00:01, 125.87it/s, loss=0.556, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  76%|███████▋  | 469/614 [00:04<00:01, 125.87it/s, loss=0.556, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  77%|███████▋  | 470/614 [00:04<00:01, 125.87it/s, loss=0.556, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  77%|███████▋  | 471/614 [00:04<00:01, 125.87it/s, loss=0.557, iou=0.402, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  77%|███████▋  | 472/614 [00:04<00:01, 125.87it/s, loss=0.556, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  77%|███████▋  | 473/614 [00:04<00:01, 130.79it/s, loss=0.556, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  77%|███████▋  | 473/614 [00:04<00:01, 130.79it/s, loss=0.556, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  77%|███████▋  | 474/614 [00:04<00:01, 130.79it/s, loss=0.556, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  77%|███████▋  | 475/614 [00:04<00:01, 130.79it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  78%|███████▊  | 476/614 [00:04<00:01, 130.79it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  78%|███████▊  | 477/614 [00:04<00:01, 130.79it/s, loss=0.556, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  78%|███████▊  | 478/614 [00:04<00:01, 130.79it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  78%|███████▊  | 479/614 [00:04<00:01, 130.79it/s, loss=0.557, iou=0.401, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  78%|███████▊  | 480/614 [00:04<00:01, 130.79it/s, loss=0.556, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  78%|███████▊  | 481/614 [00:04<00:01, 130.79it/s, loss=0.556, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  79%|███████▊  | 482/614 [00:04<00:01, 130.79it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0107\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0284\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0590\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0451\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0174\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0577\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0341\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0064\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0467\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0231\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0328\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0121\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0218\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0524\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0108\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0285\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0591\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0452\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0175\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0578\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0342\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0065\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0468\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0232\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0329\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0122\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0219\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0525\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  79%|███████▊  | 483/614 [00:04<00:01, 130.79it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  79%|███████▉  | 484/614 [00:04<00:00, 130.79it/s, loss=0.556, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  79%|███████▉  | 485/614 [00:04<00:00, 130.79it/s, loss=0.556, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  79%|███████▉  | 486/614 [00:04<00:00, 130.79it/s, loss=0.556, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  79%|███████▉  | 487/614 [00:04<00:00, 132.95it/s, loss=0.556, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  79%|███████▉  | 487/614 [00:04<00:00, 132.95it/s, loss=0.556, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  79%|███████▉  | 488/614 [00:04<00:00, 132.95it/s, loss=0.556, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  80%|███████▉  | 489/614 [00:04<00:00, 132.95it/s, loss=0.556, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  80%|███████▉  | 490/614 [00:04<00:00, 132.95it/s, loss=0.556, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  80%|███████▉  | 491/614 [00:04<00:00, 132.95it/s, loss=0.556, iou=0.403, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  80%|████████  | 492/614 [00:04<00:00, 132.95it/s, loss=0.556, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  80%|████████  | 493/614 [00:04<00:00, 132.95it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  80%|████████  | 494/614 [00:04<00:00, 132.95it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  81%|████████  | 495/614 [00:04<00:00, 132.95it/s, loss=0.557, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  81%|████████  | 496/614 [00:04<00:00, 132.95it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  81%|████████  | 497/614 [00:04<00:00, 132.95it/s, loss=0.557, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  81%|████████  | 498/614 [00:04<00:00, 132.95it/s, loss=0.557, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  81%|████████▏ | 499/614 [00:04<00:00, 132.95it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  81%|████████▏ | 500/614 [00:04<00:00, 132.95it/s, loss=0.557, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  82%|████████▏ | 501/614 [00:04<00:00, 125.88it/s, loss=0.557, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  82%|████████▏ | 501/614 [00:04<00:00, 125.88it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  82%|████████▏ | 502/614 [00:04<00:00, 125.88it/s, loss=0.557, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  82%|████████▏ | 503/614 [00:04<00:00, 125.88it/s, loss=0.557, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  82%|████████▏ | 504/614 [00:04<00:00, 125.88it/s, loss=0.557, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  82%|████████▏ | 505/614 [00:04<00:00, 125.88it/s, loss=0.557, iou=0.403, dice=0.532]\u001b[A\n",
      "Validating and Predicting:  82%|████████▏ | 506/614 [00:04<00:00, 125.88it/s, loss=0.56, iou=0.402, dice=0.532] \u001b[A\n",
      "Validating and Predicting:  83%|████████▎ | 507/614 [00:04<00:00, 125.88it/s, loss=0.56, iou=0.402, dice=0.531]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0109\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0286\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0592\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0050\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0453\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0176\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0579\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0343\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0510\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0066\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0469\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0233\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0400\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0190\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0287\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0593\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0051\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0454\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0177\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0344\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0511\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0067\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0234\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0401\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  83%|████████▎ | 508/614 [00:04<00:00, 125.88it/s, loss=0.561, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  83%|████████▎ | 509/614 [00:04<00:00, 125.88it/s, loss=0.561, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  83%|████████▎ | 510/614 [00:04<00:00, 125.88it/s, loss=0.561, iou=0.402, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  83%|████████▎ | 511/614 [00:04<00:00, 125.88it/s, loss=0.561, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  83%|████████▎ | 512/614 [00:04<00:00, 125.88it/s, loss=0.561, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  84%|████████▎ | 513/614 [00:04<00:00, 125.88it/s, loss=0.561, iou=0.401, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  84%|████████▎ | 514/614 [00:04<00:00, 125.88it/s, loss=0.561, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  84%|████████▍ | 515/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  84%|████████▍ | 515/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.401, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  84%|████████▍ | 516/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.402, dice=0.531]\u001b[A\n",
      "Validating and Predicting:  84%|████████▍ | 517/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.401, dice=0.53] \u001b[A\n",
      "Validating and Predicting:  84%|████████▍ | 518/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.401, dice=0.53]\u001b[A\n",
      "Validating and Predicting:  85%|████████▍ | 519/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.401, dice=0.53]\u001b[A\n",
      "Validating and Predicting:  85%|████████▍ | 520/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.4, dice=0.53]  \u001b[A\n",
      "Validating and Predicting:  85%|████████▍ | 521/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.401, dice=0.53]\u001b[A\n",
      "Validating and Predicting:  85%|████████▌ | 522/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.4, dice=0.53]  \u001b[A\n",
      "Validating and Predicting:  85%|████████▌ | 523/614 [00:04<00:00, 128.79it/s, loss=0.562, iou=0.4, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  85%|████████▌ | 524/614 [00:04<00:00, 128.79it/s, loss=0.561, iou=0.4, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  86%|████████▌ | 525/614 [00:04<00:00, 128.79it/s, loss=0.562, iou=0.4, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  86%|████████▌ | 526/614 [00:04<00:00, 128.79it/s, loss=0.562, iou=0.4, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  86%|████████▌ | 527/614 [00:04<00:00, 128.79it/s, loss=0.562, iou=0.399, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  86%|████████▌ | 528/614 [00:04<00:00, 127.67it/s, loss=0.562, iou=0.399, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  86%|████████▌ | 528/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.399, dice=0.529]\u001b[A\n",
      "Validating and Predicting:  86%|████████▌ | 529/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.399, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  86%|████████▋ | 530/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.398, dice=0.528]\u001b[A\n",
      "Validating and Predicting:  86%|████████▋ | 531/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  87%|████████▋ | 532/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  87%|████████▋ | 533/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  87%|████████▋ | 534/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.397, dice=0.526]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0191\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0288\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0594\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0052\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0455\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0178\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0345\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0512\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0068\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0235\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0402\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0192\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0289\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0595\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0053\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0456\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0179\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0346\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0110\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0513\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0069\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0236\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0000\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0403\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0580\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0470\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0193\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  87%|████████▋ | 535/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  87%|████████▋ | 536/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  87%|████████▋ | 537/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  88%|████████▊ | 538/614 [00:04<00:00, 127.67it/s, loss=0.562, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  88%|████████▊ | 539/614 [00:04<00:00, 127.67it/s, loss=0.562, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  88%|████████▊ | 540/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  88%|████████▊ | 541/614 [00:04<00:00, 127.67it/s, loss=0.563, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  88%|████████▊ | 542/614 [00:04<00:00, 129.43it/s, loss=0.563, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  88%|████████▊ | 542/614 [00:04<00:00, 129.43it/s, loss=0.562, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  88%|████████▊ | 543/614 [00:04<00:00, 129.43it/s, loss=0.562, iou=0.398, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  89%|████████▊ | 544/614 [00:04<00:00, 129.43it/s, loss=0.562, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  89%|████████▉ | 545/614 [00:04<00:00, 129.43it/s, loss=0.563, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  89%|████████▉ | 546/614 [00:04<00:00, 129.43it/s, loss=0.563, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  89%|████████▉ | 547/614 [00:04<00:00, 129.43it/s, loss=0.562, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  89%|████████▉ | 548/614 [00:04<00:00, 129.43it/s, loss=0.562, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  89%|████████▉ | 549/614 [00:04<00:00, 129.43it/s, loss=0.562, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  90%|████████▉ | 550/614 [00:04<00:00, 129.43it/s, loss=0.563, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  90%|████████▉ | 551/614 [00:04<00:00, 129.43it/s, loss=0.563, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  90%|████████▉ | 552/614 [00:04<00:00, 129.43it/s, loss=0.563, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  90%|█████████ | 553/614 [00:04<00:00, 129.43it/s, loss=0.563, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  90%|█████████ | 554/614 [00:04<00:00, 129.43it/s, loss=0.563, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  90%|█████████ | 555/614 [00:04<00:00, 129.43it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  91%|█████████ | 556/614 [00:04<00:00, 129.43it/s, loss=0.563, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  91%|█████████ | 557/614 [00:04<00:00, 134.10it/s, loss=0.563, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  91%|█████████ | 557/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  91%|█████████ | 558/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.396, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  91%|█████████ | 559/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.396, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  91%|█████████ | 560/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  91%|█████████▏| 561/614 [00:04<00:00, 134.10it/s, loss=0.565, iou=0.395, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  92%|█████████▏| 562/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.395, dice=0.525]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0596\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0054\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0360\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0457\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0347\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0111\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0514\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0237\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0001\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0404\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0581\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0471\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0194\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0597\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0055\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0361\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0458\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0348\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0112\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0515\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0238\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0002\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0405\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0582\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0472\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0195\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0598\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0056\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating and Predicting:  92%|█████████▏| 563/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  92%|█████████▏| 564/614 [00:04<00:00, 134.10it/s, loss=0.565, iou=0.395, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  92%|█████████▏| 565/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  92%|█████████▏| 566/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.396, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  92%|█████████▏| 567/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  93%|█████████▎| 568/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  93%|█████████▎| 569/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  93%|█████████▎| 570/614 [00:04<00:00, 134.10it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  93%|█████████▎| 571/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  93%|█████████▎| 571/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  93%|█████████▎| 572/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.396, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  93%|█████████▎| 573/614 [00:04<00:00, 133.58it/s, loss=0.565, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  93%|█████████▎| 574/614 [00:04<00:00, 133.58it/s, loss=0.565, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  94%|█████████▎| 575/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  94%|█████████▍| 576/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  94%|█████████▍| 577/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  94%|█████████▍| 578/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  94%|█████████▍| 579/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  94%|█████████▍| 580/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  95%|█████████▍| 581/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.397, dice=0.527]\u001b[A\n",
      "Validating and Predicting:  95%|█████████▍| 582/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.397, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  95%|█████████▍| 583/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  95%|█████████▌| 584/614 [00:04<00:00, 133.58it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  95%|█████████▌| 585/614 [00:04<00:00, 129.88it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  95%|█████████▌| 585/614 [00:04<00:00, 129.88it/s, loss=0.564, iou=0.396, dice=0.526]\u001b[A\n",
      "Validating and Predicting:  95%|█████████▌| 586/614 [00:04<00:00, 129.88it/s, loss=0.564, iou=0.395, dice=0.525]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0362\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0459\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0349\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0113\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0516\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0239\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0003\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0406\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0180\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0583\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0070\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0473\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0196\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0599\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0057\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0363\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0224\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0530\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0114\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0420\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0517\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0004\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0407\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0181\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0584\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  96%|█████████▌| 587/614 [00:04<00:00, 129.88it/s, loss=0.564, iou=0.395, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  96%|█████████▌| 588/614 [00:04<00:00, 129.88it/s, loss=0.565, iou=0.395, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  96%|█████████▌| 589/614 [00:05<00:00, 129.88it/s, loss=0.565, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  96%|█████████▌| 590/614 [00:05<00:00, 129.88it/s, loss=0.565, iou=0.395, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  96%|█████████▋| 591/614 [00:05<00:00, 129.88it/s, loss=0.565, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  96%|█████████▋| 592/614 [00:05<00:00, 129.88it/s, loss=0.566, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  97%|█████████▋| 593/614 [00:05<00:00, 129.88it/s, loss=0.566, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  97%|█████████▋| 594/614 [00:05<00:00, 129.88it/s, loss=0.565, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  97%|█████████▋| 595/614 [00:05<00:00, 129.88it/s, loss=0.565, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  97%|█████████▋| 596/614 [00:05<00:00, 129.88it/s, loss=0.565, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  97%|█████████▋| 597/614 [00:05<00:00, 129.88it/s, loss=0.565, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  97%|█████████▋| 598/614 [00:05<00:00, 129.88it/s, loss=0.566, iou=0.395, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  98%|█████████▊| 599/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.395, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  98%|█████████▊| 599/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  98%|█████████▊| 600/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.395, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  98%|█████████▊| 601/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  98%|█████████▊| 602/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.394, dice=0.525]\u001b[A\n",
      "Validating and Predicting:  98%|█████████▊| 603/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting:  98%|█████████▊| 604/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  99%|█████████▊| 605/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  99%|█████████▊| 606/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  99%|█████████▉| 607/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  99%|█████████▉| 608/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  99%|█████████▉| 609/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting:  99%|█████████▉| 610/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.394, dice=0.524]\u001b[A\n",
      "Validating and Predicting: 100%|█████████▉| 611/614 [00:05<00:00, 129.22it/s, loss=0.566, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting: 100%|█████████▉| 612/614 [00:05<00:00, 128.96it/s, loss=0.566, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting: 100%|█████████▉| 612/614 [00:05<00:00, 128.96it/s, loss=0.566, iou=0.393, dice=0.523]\u001b[A\n",
      "Validating and Predicting: 100%|█████████▉| 613/614 [00:05<00:00, 128.96it/s, loss=0.566, iou=0.393, dice=0.524]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0071\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0474\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0197\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0058\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0364\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0225\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0531\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0115\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0421\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0518\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0005\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0408\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0182\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0585\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0072\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0475\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0198\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0059\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0365\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0226\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0532\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0116\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0422\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0519\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0006\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0409\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting: 100%|██████████| 614/614 [00:05<00:00, 117.15it/s, loss=0.566, iou=0.393, dice=0.524]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Test Loss: 0.5659\n",
      "Test IoU: 0.3935\n",
      "Test Dice: 0.5237\n",
      "Predicted masks saved to models/unet_test/predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset_alt import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ozan_oktay_Attention_UNet.models.networks.unet_2D import unet_2D \n",
    "\n",
    "\n",
    "def validate_and_predict(config, test_loader, model, criterion, output_dir):\n",
    "    avg_meters = {'loss': AverageMeter(), \n",
    "                  'iou': AverageMeter(),\n",
    "                  'dice': AverageMeter()}\n",
    "\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(test_loader), desc=\"Validating and Predicting\")\n",
    "        for i, (input, target, metadata) in enumerate(test_loader):\n",
    "            img_id = metadata['img_id'][0] \n",
    "\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            target_original_shape = target.shape[1:] # Get H, W of original mask\n",
    "            target = target.unsqueeze(1)             # Add channel dimension\n",
    "\n",
    "            # Print original image and mask sizes\n",
    "            print(f\"\\nProcessing image ID: {img_id}\") \n",
    "            print(f\"Input ultrasound image size (H, W): {input.shape[2:]}\")\n",
    "            print(f\"Ground truth mask size (H, W): {target_original_shape}\")\n",
    "\n",
    "            # Compute output\n",
    "            if config['deep_supervision']:\n",
    "                outputs = model(input)\n",
    "                loss = 0\n",
    "                for output in outputs:\n",
    "                    loss += criterion(output, target)\n",
    "                loss /= len(outputs)\n",
    "                output = outputs[-1] # Use the last output for final prediction\n",
    "            else:\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            output = F.interpolate(output, size=target_original_shape, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            iou, dice = iou_score(output, target) \n",
    "\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['iou'].update(iou, input.size(0))\n",
    "            avg_meters['dice'].update(dice, input.size(0))\n",
    "\n",
    "            postfix = OrderedDict([\n",
    "                ('loss', avg_meters['loss'].avg),\n",
    "                ('iou', avg_meters['iou'].avg),\n",
    "                ('dice', avg_meters['dice'].avg)\n",
    "            ])\n",
    "            pbar.set_postfix(postfix)\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Save prediction mask\n",
    "            pred_mask = torch.sigmoid(output).cpu().numpy()[0, 0] # Take first image in batch, first channel\n",
    "            # pred_mask = pred_mask>0.5   \n",
    "            pred_mask[pred_mask>=0.5]=1   \n",
    "            pred_mask[pred_mask<0.5]=0\n",
    "            \n",
    "            pred_mask = (pred_mask * 255).astype(np.uint8)\n",
    "            predicted_image = Image.fromarray(pred_mask)\n",
    "            predicted_image.save(os.path.join(output_dir, f\"{img_id}_pred.jpg\"))\n",
    "\n",
    "        pbar.close()\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg),\n",
    "                        ('dice', avg_meters['dice'].avg)])\n",
    "\n",
    "    \n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "        # mask = mask.unsqueeze(dim=1)\n",
    "        weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "        wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "        wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "        pred = torch.sigmoid(pred)\n",
    "        smooth = 1\n",
    "        size = pred.size(0)\n",
    "        pred_flat = pred.view(size, -1)\n",
    "        mask_flat = mask.view(size, -1)\n",
    "        intersection = pred_flat * mask_flat\n",
    "        dice_score = (2 * intersection.sum(1) + smooth) / (pred_flat.sum(1) + mask_flat.sum(1) + smooth)\n",
    "        dice_loss = 1 - dice_score.sum() / size\n",
    "\n",
    "        return (wbce + 0.6*dice_loss).mean()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Load the configuration used for training\n",
    "    config_path = 'models/unet_test/config.yml' \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    print('-' * 20)\n",
    "    print(\"Loaded training configuration:\")\n",
    "    for key in config:\n",
    "        print(f'{key}: {config[key]}')\n",
    "    print('-' * 20)\n",
    "\n",
    "    # Define loss function (criterion) - must match training\n",
    "    if config['loss'] == 'BCEDiceLoss':\n",
    "        criterion = BCEDiceLoss().cuda()\n",
    "    else:\n",
    "        # Assuming other losses are defined in losses.py\n",
    "        criterion = losses.__dict__[config['loss']]().cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Create model and load trained weights\n",
    "    model = unet_2D(n_classes=1, in_channels=3) \n",
    "    model = model.cuda()\n",
    "\n",
    "    model_path = 'models/unet_test/model.pth' \n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Successfully loaded model weights from {model_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Model weights not found at {model_path}. Please ensure the training was successful.\")\n",
    "        return\n",
    "\n",
    "    # Data loading code for test set\n",
    "    test_img_dir = 'dataset_TN3K/test-image'\n",
    "    test_mask_dir = 'dataset_TN3K/test-mask'\n",
    "\n",
    "    test_img_ids = glob(os.path.join(test_img_dir, f'*{config[\"img_ext\"]}'))\n",
    "    test_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in test_img_ids]\n",
    "\n",
    "    if not test_img_ids:\n",
    "        print(f\"No test images found in {test_img_dir}. Please check the path and extension.\")\n",
    "        return\n",
    "\n",
    "    test_transform = Compose([\n",
    "        A.Resize(config['input_h'], config['input_w']), # Resize for model input\n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    test_dataset = Dataset(\n",
    "        img_ids=test_img_ids,\n",
    "        img_dir=test_img_dir,\n",
    "        mask_dir=test_mask_dir,\n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1, # Use batch size 1 for validation to easily handle individual image saving and logging\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    output_prediction_dir = 'models/unet_test/predictions'  \n",
    "    print(f\"\\nStarting evaluation and prediction saving to: {output_prediction_dir}\")\n",
    "    val_log = validate_and_predict(config, test_loader, model, criterion, output_prediction_dir)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Test Loss: {val_log['loss']:.4f}\")\n",
    "    print(f\"Test IoU: {val_log['iou']:.4f}\")\n",
    "    print(f\"Test Dice: {val_log['dice']:.4f}\")\n",
    "    print(f\"Predicted masks saved to {output_prediction_dir}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ac7f4-b8cc-41e4-af13-cfa23ba9fd20",
   "metadata": {},
   "source": [
    "Evaluation Results\n",
    "Test Loss: 0.5659  \n",
    "Test IoU: 0.3935  \n",
    "Test Dice: 0.5237  \n",
    "Predicted masks saved to models/unet_test/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce29e3c-b5ee-47ad-a90e-3a3ea8ef7e70",
   "metadata": {},
   "source": [
    "### 4.3 find the best model\n",
    "unet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b40e8a-b10c-4805-90d6-9800d8d652d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model by Dice coefficient of unet_test:\n",
      "epoch       3.000000\n",
      "lr          0.000105\n",
      "loss        0.658520\n",
      "iou         0.370370\n",
      "val_loss    0.609387\n",
      "val_iou     0.414657\n",
      "val_dice    0.573918\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "Best model by IoU:\n",
      "epoch       3.000000\n",
      "lr          0.000105\n",
      "loss        0.658520\n",
      "iou         0.370370\n",
      "val_loss    0.609387\n",
      "val_iou     0.414657\n",
      "val_dice    0.573918\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log_path = 'models/unet_test/log.csv'\n",
    "df = pd.read_csv(log_path)\n",
    "\n",
    "best_model_by_dice = df.loc[df['val_dice'].idxmax()]\n",
    "best_model_by_iou = df.loc[df['val_iou'].idxmax()]\n",
    "\n",
    "print(\"Best model by Dice coefficient of unet_test:\")\n",
    "print(best_model_by_dice)\n",
    "\n",
    "print(\"\\nBest model by IoU:\")\n",
    "print(best_model_by_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c98e5-a0cb-4e42-b1d9-9f5d9477bfab",
   "metadata": {},
   "source": [
    "### 4.4 Plot the Loss and IoU curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ab5c9d-1915-4fd6-a6c8-528b0fda29fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_path = 'models/unet_test/log.csv'\n",
    "df = pd.read_csv(log_path)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))  \n",
    "\n",
    "\n",
    "axes[0].plot(df['epoch'], df['loss'], label='Train Loss')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training vs Validation Loss of attention_unet of unet_224_50epoch')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(df['epoch'], df['iou'], label='Train IoU')\n",
    "axes[1].plot(df['epoch'], df['val_iou'], label='Validation IoU')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('IoU')\n",
    "axes[1].set_title('Training vs Validation IoU')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b403c-22d3-414f-a810-ac81f02e4a5b",
   "metadata": {},
   "source": [
    "## ------------------------------------------\n",
    "# 2 hybrid segmentation architecture\n",
    "I independently designed and implemented these two segmentation architectures.\n",
    "\n",
    "## 1/2 hybrid_Attention_transUnet\n",
    "### 1.1 Edit ResNetV2\n",
    "Part of the code in the following chunk was adapted from `from Beckschen_TransUNet.networks.vit_seg_modeling_resnet_skip import ResNetV2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb425c67-cfe8-498a-b2af-1da1366e4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from os.path import join as pjoin\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "\n",
    "class StdConv2d(nn.Conv2d):\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "        w = (w - m) / torch.sqrt(v + 1e-5)\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n",
    "                        self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=bias, groups=groups)\n",
    "\n",
    "\n",
    "def conv1x1(cin, cout, stride=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n",
    "                     padding=0, bias=bias)\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation (v2) bottleneck block.\"\"\"\n",
    "\n",
    "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
    "        super().__init__()\n",
    "        cout = cout or cin\n",
    "        cmid = cmid or cout//4\n",
    "\n",
    "        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv1 = conv1x1(cin, cmid, bias=False)\n",
    "        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n",
    "        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n",
    "        self.conv3 = conv1x1(cmid, cout, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if (stride != 1 or cin != cout):\n",
    "            # Projection also with pre-activation according to paper.\n",
    "            self.downsample = conv1x1(cin, cout, stride, bias=False)\n",
    "            self.gn_proj = nn.GroupNorm(cout, cout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Residual branch\n",
    "        residual = x\n",
    "        if hasattr(self, 'downsample'):\n",
    "            residual = self.downsample(x)\n",
    "            residual = self.gn_proj(residual)\n",
    "\n",
    "        # Unit's branch\n",
    "        y = self.relu(self.gn1(self.conv1(x)))\n",
    "        y = self.relu(self.gn2(self.conv2(y)))\n",
    "        y = self.gn3(self.conv3(y))\n",
    "\n",
    "        y = self.relu(residual + y)\n",
    "        return y\n",
    "\n",
    "    def load_from(self, weights, n_block, n_unit):\n",
    "        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n",
    "        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n",
    "        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n",
    "\n",
    "        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n",
    "        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n",
    "\n",
    "        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n",
    "        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n",
    "\n",
    "        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n",
    "        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n",
    "\n",
    "        self.conv1.weight.copy_(conv1_weight)\n",
    "        self.conv2.weight.copy_(conv2_weight)\n",
    "        self.conv3.weight.copy_(conv3_weight)\n",
    "\n",
    "        self.gn1.weight.copy_(gn1_weight.view(-1))\n",
    "        self.gn1.bias.copy_(gn1_bias.view(-1))\n",
    "\n",
    "        self.gn2.weight.copy_(gn2_weight.view(-1))\n",
    "        self.gn2.bias.copy_(gn2_bias.view(-1))\n",
    "\n",
    "        self.gn3.weight.copy_(gn3_weight.view(-1))\n",
    "        self.gn3.bias.copy_(gn3_bias.view(-1))\n",
    "\n",
    "        if hasattr(self, 'downsample'):\n",
    "            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n",
    "            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n",
    "            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n",
    "\n",
    "            self.downsample.weight.copy_(proj_conv_weight)\n",
    "            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n",
    "            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d7422fb-382f-4f8e-ae76-b8c737ce08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetV2(nn.Module):                              \n",
    "    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n",
    "\n",
    "    def __init__(self, block_units, width_factor):\n",
    "        super().__init__()\n",
    "        width = int(64 * width_factor)\n",
    "        self.width = width\n",
    "\n",
    "        self.root = nn.Sequential(OrderedDict([\n",
    "            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n",
    "            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n",
    "            ('relu', nn.ReLU(inplace=True)),\n",
    "            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "        ]))\n",
    "\n",
    "        self.body = nn.Sequential(OrderedDict([\n",
    "            ('block1', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n",
    "                ))),\n",
    "            ('block2', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n",
    "                ))),\n",
    "            ('block3', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n",
    "                ))),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        b, c, in_size, _ = x.size()\n",
    "        \n",
    "        x = self.root(x)\n",
    "        features.append(x)  # feature 0 (64x112x112)\n",
    "    \n",
    "        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)  # -> 56x56  # 本来 padding=1\n",
    "    \n",
    "        for i in range(len(self.body)-1):  # block1, block2\n",
    "            x = self.body[i](x)\n",
    "            right_size = int(in_size / 4 / (i+1))\n",
    "            if x.size()[2] != right_size:\n",
    "                pad = right_size - x.size()[2]\n",
    "                assert pad < 3 and pad > 0, f\"x {x.size()} should {right_size}\"\n",
    "                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n",
    "                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n",
    "            else:\n",
    "                feat = x\n",
    "            features.append(feat)\n",
    "    \n",
    "        # 🔧 block3 最后特征也加入 features（最深层）\n",
    "        x = self.body[-1](x)\n",
    "        features.append(x)  # ✅ 添加这一行！\n",
    "    \n",
    "        return x, features[::-1]  # 从深到浅排列（[512, 28, 28] -> [64, 112, 112]）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939596aa-ca61-4845-be4f-3a3f20156112",
   "metadata": {},
   "source": [
    "### 1.2 Define unetConv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf485c9e-70d9-454d-93d8-2adc3b1c9999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ozan_oktay_Attention_UNet.models.networks_other import init_weights \n",
    "\n",
    "class unetConv2(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n",
    "        super(unetConv2, self).__init__()\n",
    "        self.n = n\n",
    "        self.ks = ks\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        s = stride\n",
    "        p = padding\n",
    "        if is_batchnorm:\n",
    "            for i in range(1, n+1):\n",
    "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "                                     nn.BatchNorm2d(out_size),\n",
    "                                     nn.ReLU(inplace=True),)\n",
    "                setattr(self, 'conv%d'%i, conv)\n",
    "                in_size = out_size\n",
    "\n",
    "        else:\n",
    "            for i in range(1, n+1):\n",
    "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "                                     nn.ReLU(inplace=True),)\n",
    "                setattr(self, 'conv%d'%i, conv)\n",
    "                in_size = out_size\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for i in range(1, self.n+1):\n",
    "            conv = getattr(self, 'conv%d'%i)\n",
    "            x = conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7eabf-621d-4599-8b68-e7c33b1b86b9",
   "metadata": {},
   "source": [
    "### 1.3 Modify unetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c38be7-c746-4708-8060-42bfde63180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class unetUp(nn.Module):\n",
    "    def __init__(self, up_in_channels, cat_in_channels, out_channels, is_deconv):\n",
    "        super(unetUp, self).__init__()\n",
    "\n",
    "        if is_deconv:\n",
    "            self.up = nn.ConvTranspose2d(up_in_channels, up_in_channels, kernel_size=4, stride=2, padding=1)\n",
    "        else:\n",
    "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "        self.conv = unetConv2(cat_in_channels, out_channels, False)\n",
    "\n",
    "        # initialise\n",
    "        for m in self.children():\n",
    "            if isinstance(m, unetConv2): continue\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        outputs2 = self.up(inputs2)\n",
    "        \n",
    "        diffY = outputs2.size()[2] - inputs1.size()[2]     \n",
    "        diffX = outputs2.size()[3] - inputs1.size()[3]\n",
    "        \n",
    "        outputs1 = F.pad(inputs1, [diffX // 2, diffX - diffX // 2,\n",
    "                                   diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        return self.conv(torch.cat([outputs1, outputs2], dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74baa2a0-8de4-41a8-9c59-224121558984",
   "metadata": {},
   "source": [
    "### 1.4 class HybridAttentionUNet 🔥\n",
    "🔥 this is Critical components ！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6fea9a8-d077-4b87-8aef-9a3594efc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from ozan_oktay_Attention_UNet.models.layers.nonlocal_layer import NONLocalBlock2D\n",
    "\n",
    "\n",
    "class HybridAttentionUNet(nn.Module):\n",
    "    def __init__(self, n_classes=1, is_deconv=True, feature_scale=4, nonlocal_mode='embedded_gaussian', nonlocal_sf=4):\n",
    "        super(HybridAttentionUNet, self).__init__()\n",
    "\n",
    "        self.is_deconv = is_deconv\n",
    "        self.feature_scale = feature_scale\n",
    "\n",
    "        # Use ResNetV2 as the encoder (replacing the original conv1~center)\n",
    "        self.resnet_encoder = ResNetV2(block_units=[3, 4, 9], width_factor=1)\n",
    "\n",
    "        # Get the number of feature channels from the encoder output, as known from TransUNet, they are: \n",
    "        self.enc_channels = [64, 256, 512, 1024]  # order of features returned from ResNetV2 body\n",
    "\n",
    "        # Build non-local attention\n",
    "        self.nonlocal1 = NONLocalBlock2D(in_channels=self.enc_channels[0], inter_channels=self.enc_channels[0] // 4,\n",
    "                                         sub_sample_factor=nonlocal_sf, mode=nonlocal_mode)\n",
    "        self.nonlocal2 = NONLocalBlock2D(in_channels=self.enc_channels[1], inter_channels=self.enc_channels[1] // 4,\n",
    "                                         sub_sample_factor=nonlocal_sf, mode=nonlocal_mode)\n",
    "\n",
    "\n",
    "        self.up_concat4 = unetUp(up_in_channels=1024, cat_in_channels=2048, out_channels=512, is_deconv=is_deconv) \n",
    "        self.up_concat3 = unetUp(512, 1024, 256, is_deconv)\n",
    "        self.up_concat2 = unetUp(256, 512, 128, is_deconv)\n",
    "        self.up_concat1 = unetUp(128, 192, 64, is_deconv)\n",
    "\n",
    "        \n",
    "        # After reducing output channels, connect to the classification layer\n",
    "        self.final = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "        \n",
    "        # Initialize the weights of the final conv\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, features = self.resnet_encoder(inputs)\n",
    "    \n",
    "        # Now features[0] is the deepest layer (512x28x28), \n",
    "        # while features[3] is the shallowest layer (64x112x112)\n",
    "        conv4 = features[0]\n",
    "        conv3 = features[1]\n",
    "        conv2 = features[2]\n",
    "        conv1 = features[3]\n",
    "    \n",
    "        # Non-local attention\n",
    "        conv1 = self.nonlocal1(conv1)\n",
    "        conv2 = self.nonlocal2(conv2)\n",
    "    \n",
    "        center = x  # alternatively, use conv4\n",
    "\n",
    "        up4 = self.up_concat4(conv4, center)\n",
    "        up3 = self.up_concat3(conv3, up4)\n",
    "        up2 = self.up_concat2(conv2, up3)\n",
    "        up1 = self.up_concat1(conv1, up2)\n",
    "    \n",
    "        final = self.final(up1)\n",
    "        \n",
    "        return final\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_argmax_softmax(pred):\n",
    "        return F.softmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353b0cd-4f60-4aad-ae00-f647fa8c95f2",
   "metadata": {},
   "source": [
    "### 1.5 train model\n",
    "now we begin our pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468e6867-09d4-416a-8b4e-95e9b591d8eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   0%|          | 0/614 [13:28:55<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "name: atten_transUnet_test\n",
      "epochs: 5\n",
      "batch_size: 16\n",
      "arch: Axial\n",
      "deep_supervision: False\n",
      "input_channels: 3\n",
      "num_classes: 1\n",
      "input_w: 384\n",
      "input_h: 384\n",
      "loss: BCEDiceLoss\n",
      "dataset: ICOS\n",
      "img_ext: .jpg\n",
      "mask_ext: .jpg\n",
      "optimizer: Adam\n",
      "lr: 0.0001\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0001\n",
      "nesterov: False\n",
      "scheduler: CosineAnnealingLR\n",
      "min_lr: 1e-06\n",
      "factor: 0.1\n",
      "patience: 2\n",
      "milestones: 1,2\n",
      "gamma: 0.6666666666666666\n",
      "early_stopping: -1\n",
      "num_workers: 12\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/layers/nonlocal_layer.py:51: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  nn.init.constant(self.W[1].weight, 0)\n",
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/layers/nonlocal_layer.py:52: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  nn.init.constant(self.W[1].bias, 0)\n",
      "/data/home/ha24521/.conda/envs/Practical_5/lib/python3.11/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable parameters 49882161\n",
      "Epoch [0/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [01:12<00:00,  1.97it/s, loss=1.36, iou=0.151]\n",
      "100%|██████████| 36/36 [00:06<00:00,  5.21it/s, loss=0.887, iou=0.208, dice=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.3557 - iou 0.1508 - val_loss 0.8868 - val_iou 0.2083\n",
      "Current LR: 9.05463412215599e-05\n",
      "=> saved best model\n",
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [01:04<00:00,  2.21it/s, loss=0.819, iou=0.201]\n",
      "100%|██████████| 36/36 [00:06<00:00,  5.49it/s, loss=0.767, iou=0.245, dice=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.8187 - iou 0.2008 - val_loss 0.7668 - val_iou 0.2453\n",
      "Current LR: 6.57963412215599e-05\n",
      "=> saved best model\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [01:04<00:00,  2.21it/s, loss=0.739, iou=0.273]\n",
      "100%|██████████| 36/36 [00:06<00:00,  5.46it/s, loss=0.707, iou=0.324, dice=0.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7389 - iou 0.2728 - val_loss 0.7068 - val_iou 0.3237\n",
      "Current LR: 3.52036587784401e-05\n",
      "=> saved best model\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [01:04<00:00,  2.21it/s, loss=0.698, iou=0.329]\n",
      "100%|██████████| 36/36 [00:06<00:00,  5.50it/s, loss=0.645, iou=0.387, dice=0.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.6979 - iou 0.3293 - val_loss 0.6452 - val_iou 0.3871\n",
      "Current LR: 1.0453658778440105e-05\n",
      "=> saved best model\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [01:04<00:00,  2.20it/s, loss=0.652, iou=0.379]\n",
      "100%|██████████| 36/36 [00:06<00:00,  5.47it/s, loss=0.628, iou=0.417, dice=0.585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.6517 - iou 0.3791 - val_loss 0.6280 - val_iou 0.4171\n",
      "Current LR: 1e-06\n",
      "=> saved best model\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yaml\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import Compose, OneOf  # version of albumentaation 1.3.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from albumentations import RandomRotate90,Resize\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import archs\n",
    "import losses\n",
    "from dataset import Dataset\n",
    "from metrics import iou_score\n",
    "from utils import AverageMeter, str2bool\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--name', default=None,\n",
    "                        help='model name: (default: arch)')\n",
    "    parser.add_argument('--epochs', default=50, type=int, metavar='N',   \n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-b', '--batch_size', default=4, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 16)')\n",
    "    \n",
    "    # model\n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='Axial')   \n",
    "    parser.add_argument('--deep_supervision', default=False, type=str2bool)\n",
    "    parser.add_argument('--input_channels', default=3, type=int,           \n",
    "                        help='input channels')\n",
    "    parser.add_argument('--num_classes', default=1, type=int,              \n",
    "                        help='number of classes')\n",
    "    parser.add_argument('--input_w', default=384, type=int,                \n",
    "                        help='image width')\n",
    "    parser.add_argument('--input_h', default=384, type=int,\n",
    "                        help='image height')\n",
    "    \n",
    "    # loss\n",
    "    parser.add_argument('--loss', default='BCEDiceLoss',\n",
    "                        choices=LOSS_NAMES,\n",
    "                        help='loss: ' +\n",
    "                        ' | '.join(LOSS_NAMES) +\n",
    "                        ' (default: BCEDiceLoss)')\n",
    "    \n",
    "    # dataset\n",
    "    parser.add_argument('--dataset', default='ICOS',\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--img_ext', default='.jpg',\n",
    "                        help='image file extension')\n",
    "    parser.add_argument('--mask_ext', default='.jpg',\n",
    "                        help='mask file extension')\n",
    "\n",
    "    # optimizer\n",
    "    parser.add_argument('--optimizer', default='Adam',\n",
    "                        choices=['Adam', 'SGD'],\n",
    "                        help='loss: ' +\n",
    "                        ' | '.join(['Adam', 'SGD']) +\n",
    "                        ' (default: Adam)')\n",
    "    parser.add_argument('--lr', '--learning_rate', default=1e-4, type=float,  \n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float,\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--nesterov', default=False, type=str2bool,\n",
    "                        help='nesterov')\n",
    "\n",
    "    # scheduler\n",
    "    parser.add_argument('--scheduler', default='CosineAnnealingLR',\n",
    "                        choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])\n",
    "    parser.add_argument('--min_lr', default=1e-5, type=float,\n",
    "                        help='minimum learning rate')\n",
    "    parser.add_argument('--factor', default=0.1, type=float)\n",
    "    parser.add_argument('--patience', default=2, type=int)\n",
    "    parser.add_argument('--milestones', default='1,2', type=str)\n",
    "    parser.add_argument('--gamma', default=2/3, type=float)\n",
    "    parser.add_argument('--early_stopping', default=-1, type=int,\n",
    "                        metavar='N', help='early stopping (default: -1)')\n",
    "    parser.add_argument('--cfg', type=str, metavar=\"FILE\", help='path to config file', )\n",
    "\n",
    "    parser.add_argument('--num_workers', default=4, type=int)\n",
    "\n",
    "    config = parser.parse_args()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "def train(config, train_loader, model, criterion, optimizer):\n",
    "    avg_meters = {'loss': AverageMeter(),\n",
    "                  'iou': AverageMeter()}\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=len(train_loader))\n",
    "    for input, target in train_loader:     \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        target = target.unsqueeze(1)\n",
    "\n",
    "        # compute output\n",
    "        if config['deep_supervision']:\n",
    "            outputs = model(input)\n",
    "            loss = 0\n",
    "            for output in outputs:\n",
    "                loss += criterion(output, target)\n",
    "            loss /= len(outputs)\n",
    "            iou,dice = iou_score(outputs[-1], target)\n",
    "        else:\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            iou,dice = iou_score(output, target)\n",
    "\n",
    "        # compute gradient and do optimizing step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "        avg_meters['iou'].update(iou, input.size(0))\n",
    "\n",
    "        postfix = OrderedDict([\n",
    "            ('loss', avg_meters['loss'].avg),\n",
    "            ('iou', avg_meters['iou'].avg),\n",
    "        ])\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "\n",
    "def validate(config, val_loader, model, criterion):\n",
    "    avg_meters = {'loss': AverageMeter(),\n",
    "                  'iou': AverageMeter(),\n",
    "                  'dice': AverageMeter()}\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(val_loader))\n",
    "        for input, target in val_loader:     \n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            target = target.unsqueeze(1)\n",
    "\n",
    "            # compute output\n",
    "            if config['deep_supervision']:\n",
    "                outputs = model(input)\n",
    "                loss = 0\n",
    "                for output in outputs:\n",
    "                    loss += criterion(output, target)\n",
    "                loss /= len(outputs)\n",
    "                iou,dice = iou_score(outputs[-1], target)\n",
    "            else:\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "                iou,dice = iou_score(output, target)\n",
    "\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['iou'].update(iou, input.size(0))\n",
    "            avg_meters['dice'].update(dice, input.size(0))\n",
    "            postfix = OrderedDict([\n",
    "                ('loss', avg_meters['loss'].avg),\n",
    "                ('iou', avg_meters['iou'].avg),\n",
    "                ('dice', avg_meters['dice'].avg)\n",
    "            ])\n",
    "            pbar.set_postfix(postfix)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg),\n",
    "                        ('dice', avg_meters['dice'].avg)])\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "        # mask = mask.unsqueeze(dim=1)\n",
    "        weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "        wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "        wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "        pred = torch.sigmoid(pred)\n",
    "        smooth = 1\n",
    "        size = pred.size(0)\n",
    "        pred_flat = pred.view(size, -1)\n",
    "        mask_flat = mask.view(size, -1)\n",
    "        intersection = pred_flat * mask_flat\n",
    "        dice_score = (2 * intersection.sum(1) + smooth) / (pred_flat.sum(1) + mask_flat.sum(1) + smooth)\n",
    "        dice_loss = 1 - dice_score.sum() / size\n",
    "\n",
    "        return (wbce + 0.6*dice_loss).mean()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def main():\n",
    "    \n",
    "    config = {\n",
    "    'name': 'atten_transUnet_test',\n",
    "    'epochs': 5, # I recommend 100 epochs for fully demonstrating the potential of transUnet. Here I use 5 epochs to test whether the code runs successfully. \n",
    "    'batch_size': 16,                \n",
    "    'arch': 'Axial',\n",
    "    'deep_supervision': False,\n",
    "    'input_channels': 3,\n",
    "    'num_classes': 1,\n",
    "    'input_w': 384,           \n",
    "    'input_h': 384,           \n",
    "    'loss': 'BCEDiceLoss',\n",
    "    'dataset': 'ICOS',\n",
    "    'img_ext': '.jpg',\n",
    "    'mask_ext': '.jpg',\n",
    "    'optimizer': 'Adam',\n",
    "    \n",
    "    'lr': 1e-4,                    # I recommend 1e-4 rather than 1e-3\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-4,\n",
    "    'nesterov': False,\n",
    "    'scheduler': 'CosineAnnealingLR',  \n",
    "    'min_lr': 1e-6,\n",
    "    'factor': 0.1,\n",
    "    'patience': 2,\n",
    "    'milestones': '1,2',\n",
    "    'gamma': 2/3,\n",
    "    'early_stopping': -1, \n",
    "    'num_workers': 12     \n",
    "}\n",
    "    set_seed(42)\n",
    "    \n",
    "    if config['name'] is None:\n",
    "        if config['deep_supervision']:\n",
    "            config['name'] = '%s_%s_wDS' % (config['dataset'], config['arch'])\n",
    "        else:\n",
    "            config['name'] = '%s_%s_woDS' % (config['dataset'], config['arch'])\n",
    "    \n",
    "    os.makedirs('models/%s' % config['name'], exist_ok=True)\n",
    "\n",
    "    print('-' * 20)\n",
    "    for key in config:\n",
    "        print('%s: %s' % (key, config[key]))\n",
    "    print('-' * 20)\n",
    "\n",
    "    with open('models/%s/config.yml' % config['name'], 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    # define loss function (criterion)\n",
    "    if config['loss'] == 'BCEDiceLoss':\n",
    "        criterion = BCEDiceLoss().cuda() #nn.BCEWithLogitsLoss().cuda()\n",
    "    else:\n",
    "        criterion = losses.__dict__[config['loss']]().cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    model = HybridAttentionUNet(n_classes=config['num_classes'])    \n",
    "    model = model.cuda()\n",
    "\n",
    "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print (\"\\nTrainable parameters\", pytorch_total_params)\n",
    "    \n",
    "    # print(params)\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n",
    "                              nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if config['scheduler'] == 'CosineAnnealingLR':                     \n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=config['epochs'], eta_min=config['min_lr'])\n",
    "    \n",
    "    elif config['scheduler'] == 'CosineAnnealingWarmRestarts':         \n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(          \n",
    "            optimizer, T_0=50, T_mult=1, eta_min=config['min_lr'])\n",
    "    \n",
    "    elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, factor=config['factor'], patience=config['patience'],\n",
    "            verbose=1, min_lr=config['min_lr'])\n",
    "        \n",
    "    elif config['scheduler'] == 'MultiStepLR':\n",
    "        scheduler = lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[int(e) for e in config['milestones'].split(',')],\n",
    "            gamma=config['gamma'])\n",
    "    elif config['scheduler'] == 'ConstantLR':\n",
    "        scheduler = None\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Data loading code\n",
    "    train_img_ids = glob('dataset_TN3K/training_image/*')\n",
    "    train_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in train_img_ids]\n",
    "\n",
    "    val_img_ids = glob('dataset_TN3K/validation_image/*')\n",
    "    val_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in val_img_ids]\n",
    "    \n",
    "\n",
    "    train_transform = Compose([\n",
    "        RandomRotate90(),\n",
    "        A.HorizontalFlip(),         \n",
    "        A.VerticalFlip(),           \n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.5, rotate_limit=45, p=0.75),\n",
    "        A.Transpose(),\n",
    "        Resize(config['input_h'], config['input_w']), # we did here\n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    val_transform = Compose([\n",
    "        Resize(config['input_h'], config['input_w']),\n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = Dataset(\n",
    "        img_ids=train_img_ids,\n",
    "        img_dir='dataset_TN3K/training_image',\n",
    "        mask_dir='dataset_TN3K/training_mask',\n",
    "        \n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=train_transform)\n",
    "    \n",
    "    val_dataset = Dataset(\n",
    "        img_ids=val_img_ids,\n",
    "        img_dir=os.path.join('dataset_TN3K/validation_image'),\n",
    "        mask_dir=os.path.join('dataset_TN3K/validation_mask'),\n",
    "        \n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=val_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        drop_last=False)\n",
    "    log = OrderedDict([\n",
    "        ('epoch', []),\n",
    "        ('lr', []),\n",
    "        ('loss', []),\n",
    "        ('iou', []),\n",
    "        ('val_loss', []),\n",
    "        ('val_iou', []),\n",
    "        ('val_dice', []),\n",
    "    ])\n",
    "\n",
    "    best_iou = 0\n",
    "    trigger = 0\n",
    "    for epoch in range(config['epochs']):\n",
    "        print('Epoch [%d/%d]' % (epoch, config['epochs']))\n",
    "\n",
    "        # train for one epoch\n",
    "        train_log = train(config, train_loader, model, criterion, optimizer)\n",
    "       \n",
    "        # evaluate on validation set\n",
    "        val_log = validate(config, val_loader, model, criterion)\n",
    "\n",
    "        if config['scheduler'] == 'CosineAnnealingLR':            \n",
    "            scheduler.step()\n",
    "\n",
    "        elif config['scheduler'] == 'CosineAnnealingWarmRestarts': \n",
    "            scheduler.step(epoch)\n",
    "        \n",
    "        elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "            scheduler.step(val_log['loss'])\n",
    "\n",
    "\n",
    "        print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n",
    "              % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n",
    "\n",
    "        log['epoch'].append(epoch)\n",
    "        log['lr'].append(optimizer.param_groups[0]['lr'])       \n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']}\") \n",
    "\n",
    "        log['loss'].append(train_log['loss'])\n",
    "        log['iou'].append(train_log['iou'])\n",
    "        log['val_loss'].append(val_log['loss'])\n",
    "        log['val_iou'].append(val_log['iou'])\n",
    "        log['val_dice'].append(val_log['dice'])\n",
    "\n",
    "        \n",
    "        pd.DataFrame(log).to_csv('models/%s/log.csv' %\n",
    "                                 config['name'], index=False)\n",
    "\n",
    "        trigger += 1\n",
    "\n",
    "        if val_log['iou'] > best_iou:\n",
    "            torch.save(model.state_dict(), 'models/%s/model.pth' %\n",
    "                       config['name'])\n",
    "            best_iou = val_log['iou']\n",
    "            print(\"=> saved best model\")\n",
    "            trigger = 0\n",
    "\n",
    "        # early stopping\n",
    "        if config['early_stopping'] >= 0 and trigger >= config['early_stopping']:\n",
    "            print(\"=> early stopping\")\n",
    "            break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4319c7-3c79-48a1-b830-5f4386615363",
   "metadata": {},
   "source": [
    "### 1.6 calculate IoU and Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3e0071c-c450-4047-bfca-c8fbd6857cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Loaded training configuration:\n",
      "arch: Axial\n",
      "batch_size: 16\n",
      "dataset: ICOS\n",
      "deep_supervision: False\n",
      "early_stopping: -1\n",
      "epochs: 5\n",
      "factor: 0.1\n",
      "gamma: 0.6666666666666666\n",
      "img_ext: .jpg\n",
      "input_channels: 3\n",
      "input_h: 384\n",
      "input_w: 384\n",
      "loss: BCEDiceLoss\n",
      "lr: 0.0001\n",
      "mask_ext: .jpg\n",
      "milestones: 1,2\n",
      "min_lr: 1e-06\n",
      "momentum: 0.9\n",
      "name: atten_transUnet_test\n",
      "nesterov: False\n",
      "num_classes: 1\n",
      "num_workers: 12\n",
      "optimizer: Adam\n",
      "patience: 2\n",
      "scheduler: CosineAnnealingLR\n",
      "weight_decay: 0.0001\n",
      "--------------------\n",
      "Successfully loaded model weights from models/atten_transUnet_test/model.pth\n",
      "\n",
      "Starting evaluation and prediction saving to: models/atten_transUnet_test/predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   0%|          | 0/614 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0183\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   1%|▏         | 8/614 [00:03<02:25,  4.15it/s, loss=0.846, iou=0.217, dice=0.316] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0586\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0350\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0073\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0476\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0240\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0366\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0130\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   2%|▏         | 15/614 [00:03<01:16,  7.80it/s, loss=0.772, iou=0.231, dice=0.343]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0227\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0533\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0020\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0117\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0423\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0007\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0184\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0490\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0587\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   4%|▍         | 24/614 [00:03<00:35, 16.47it/s, loss=0.709, iou=0.289, dice=0.409]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0351\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0074\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0477\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0241\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0367\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0131\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0228\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0534\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0021\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   6%|▌         | 34/614 [00:03<00:22, 25.59it/s, loss=0.66, iou=0.307, dice=0.424] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0118\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0424\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0008\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0185\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0491\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0588\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0352\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0075\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0478\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   7%|▋         | 43/614 [00:03<00:17, 33.17it/s, loss=0.693, iou=0.3, dice=0.415]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0242\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0368\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0132\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0229\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0535\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0119\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0009\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0186\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0492\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   9%|▊         | 53/614 [00:04<00:14, 38.30it/s, loss=0.688, iou=0.291, dice=0.408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0589\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0353\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0076\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0479\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0243\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0410\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0369\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0133\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0536\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  10%|▉         | 61/614 [00:04<00:13, 39.77it/s, loss=0.7, iou=0.275, dice=0.388]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0300\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0090\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0187\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0493\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0354\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0077\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0244\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0411\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0134\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  11%|█▏        | 70/614 [00:04<00:12, 42.02it/s, loss=0.687, iou=0.275, dice=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0537\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0301\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0091\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0188\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0494\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0355\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0078\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0245\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0412\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  12%|█▏        | 76/614 [00:04<00:12, 42.50it/s, loss=0.678, iou=0.291, dice=0.41] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0135\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0538\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0302\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0299\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0092\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0189\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  13%|█▎        | 79/614 [00:05<00:17, 30.56it/s, loss=0.677, iou=0.294, dice=0.414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0495\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0356\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0079\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0246\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  14%|█▍        | 88/614 [00:05<00:17, 29.73it/s, loss=0.664, iou=0.302, dice=0.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0010\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0413\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0136\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0539\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0303\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0480\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0370\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0093\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0496\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  16%|█▌        | 97/614 [00:05<00:14, 35.59it/s, loss=0.658, iou=0.312, dice=0.435]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0260\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0357\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0247\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0011\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0414\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0137\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0304\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  16%|█▌        | 98/614 [00:05<00:14, 35.59it/s, loss=0.654, iou=0.316, dice=0.44] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0481\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0371\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0094\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  16%|█▋        | 101/614 [00:05<00:23, 21.88it/s, loss=0.65, iou=0.319, dice=0.444]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0497\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0261\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  17%|█▋        | 103/614 [00:06<00:23, 21.88it/s, loss=0.648, iou=0.32, dice=0.445] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0358\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0248\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  17%|█▋        | 106/614 [00:06<00:35, 14.27it/s, loss=0.648, iou=0.32, dice=0.447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0012\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0415\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0138\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  18%|█▊        | 109/614 [00:06<00:35, 14.20it/s, loss=0.644, iou=0.322, dice=0.449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0305\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0482\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0372\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  19%|█▉        | 117/614 [00:07<00:29, 16.58it/s, loss=0.64, iou=0.321, dice=0.447] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0095\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0498\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0262\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0359\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0123\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0526\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0249\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0013\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0416\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  21%|██        | 126/614 [00:07<00:19, 25.54it/s, loss=0.635, iou=0.326, dice=0.453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0139\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0306\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0080\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0483\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0373\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0540\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0096\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0499\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0263\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  22%|██▏       | 136/614 [00:07<00:14, 32.67it/s, loss=0.637, iou=0.321, dice=0.447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0124\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0430\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0527\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0014\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0320\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0417\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0307\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0081\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0484\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  24%|██▎       | 145/614 [00:07<00:16, 28.97it/s, loss=0.634, iou=0.321, dice=0.447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0374\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0541\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0097\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0264\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0125\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0431\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0528\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0015\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0321\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  25%|██▌       | 154/614 [00:08<00:14, 32.43it/s, loss=0.629, iou=0.332, dice=0.46] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0418\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0308\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0082\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0485\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0375\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0542\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0265\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0126\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0432\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  26%|██▋       | 162/614 [00:08<00:12, 37.64it/s, loss=0.629, iou=0.333, dice=0.461]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0529\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0016\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0322\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0419\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0309\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0083\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0486\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0250\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0376\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  28%|██▊       | 171/614 [00:08<00:10, 40.54it/s, loss=0.629, iou=0.329, dice=0.455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0140\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0543\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0266\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0030\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0127\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0433\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0600\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0017\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0323\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  29%|██▉       | 181/614 [00:08<00:10, 42.13it/s, loss=0.624, iou=0.335, dice=0.461]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0084\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0390\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0487\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0251\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0377\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0141\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0544\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0267\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0031\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  31%|███       | 190/614 [00:08<00:09, 43.31it/s, loss=0.625, iou=0.332, dice=0.457]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0128\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0434\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0601\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0018\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0085\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0391\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0488\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0252\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0378\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  32%|███▏      | 199/614 [00:09<00:09, 43.57it/s, loss=0.62, iou=0.334, dice=0.46]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0142\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0545\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0268\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0032\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0129\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0435\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0602\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0019\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0086\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  34%|███▍      | 208/614 [00:09<00:09, 43.71it/s, loss=0.619, iou=0.332, dice=0.458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0392\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0489\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0253\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0379\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0143\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0546\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0310\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0269\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0033\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  35%|███▌      | 217/614 [00:09<00:08, 44.12it/s, loss=0.614, iou=0.336, dice=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0436\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0200\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0603\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0087\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0393\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0254\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0560\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0144\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0547\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  37%|███▋      | 226/614 [00:09<00:08, 44.36it/s, loss=0.612, iou=0.336, dice=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0311\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0034\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0437\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0201\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0604\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0088\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0394\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0255\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0561\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  38%|███▊      | 235/614 [00:09<00:08, 44.65it/s, loss=0.608, iou=0.337, dice=0.465]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0145\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0548\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0312\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0035\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0438\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0202\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0605\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0199\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0089\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0395\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  40%|███▉      | 245/614 [00:10<00:08, 43.43it/s, loss=0.607, iou=0.335, dice=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0256\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0562\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0146\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0549\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0313\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0036\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0439\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0203\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0606\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  41%|████      | 253/614 [00:10<00:08, 43.73it/s, loss=0.609, iou=0.335, dice=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0380\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0270\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0396\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0160\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0257\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0563\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0147\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0314\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0037\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  43%|████▎     | 263/614 [00:10<00:08, 42.31it/s, loss=0.611, iou=0.329, dice=0.455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0204\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0607\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0381\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0271\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0397\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0161\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0258\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0564\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0022\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  44%|████▍     | 272/614 [00:10<00:07, 43.42it/s, loss=0.612, iou=0.327, dice=0.453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0425\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0148\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0315\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0038\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0205\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0608\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0382\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0272\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0398\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  46%|████▌     | 281/614 [00:10<00:07, 43.77it/s, loss=0.613, iou=0.323, dice=0.448]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0162\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0259\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0565\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0023\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0426\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0149\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0316\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0039\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0206\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  47%|████▋     | 290/614 [00:11<00:07, 43.85it/s, loss=0.613, iou=0.323, dice=0.448]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0609\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0383\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0550\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0273\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0440\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0399\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0163\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0566\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0024\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  49%|████▉     | 300/614 [00:11<00:07, 44.17it/s, loss=0.609, iou=0.326, dice=0.452]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0330\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0427\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0220\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0317\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0207\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0384\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0551\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0274\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0441\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  50%|█████     | 307/614 [00:11<00:07, 43.67it/s, loss=0.608, iou=0.327, dice=0.453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0164\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0567\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0025\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0331\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0428\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0221\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0318\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0208\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0385\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  51%|█████▏    | 316/614 [00:11<00:06, 42.94it/s, loss=0.607, iou=0.328, dice=0.454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0552\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0275\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0442\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0165\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0568\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0026\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0332\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0429\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0222\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  53%|█████▎    | 326/614 [00:12<00:06, 42.94it/s, loss=0.608, iou=0.329, dice=0.454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0319\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0209\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0386\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0150\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0553\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0276\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0040\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0443\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0610\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  55%|█████▍    | 335/614 [00:12<00:06, 43.48it/s, loss=0.608, iou=0.328, dice=0.453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0166\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0569\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0027\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0333\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0500\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0223\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0290\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0387\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0151\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  56%|█████▌    | 345/614 [00:12<00:06, 43.93it/s, loss=0.609, iou=0.328, dice=0.454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0554\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0277\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0041\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0444\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0611\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0167\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0028\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0334\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0501\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  57%|█████▋    | 352/614 [00:12<00:06, 43.06it/s, loss=0.606, iou=0.331, dice=0.456]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0291\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0388\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0152\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0555\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0278\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0042\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0445\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0612\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0168\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  59%|█████▉    | 362/614 [00:12<00:05, 43.74it/s, loss=0.605, iou=0.333, dice=0.457]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0029\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0335\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0502\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0292\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0389\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0153\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0556\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0279\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0043\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  60%|██████    | 371/614 [00:13<00:05, 44.14it/s, loss=0.605, iou=0.336, dice=0.46] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0446\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0210\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0613\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0169\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0336\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0100\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0503\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0570\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0293\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  62%|██████▏   | 380/614 [00:13<00:05, 44.36it/s, loss=0.606, iou=0.332, dice=0.456]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0154\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0460\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0557\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0044\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0447\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0211\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0337\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0101\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0504\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  64%|██████▎   | 390/614 [00:13<00:05, 44.35it/s, loss=0.604, iou=0.335, dice=0.459]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0098\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0571\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0294\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0155\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0461\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0558\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0045\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0448\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0212\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  65%|██████▍   | 398/614 [00:13<00:04, 44.12it/s, loss=0.603, iou=0.337, dice=0.461]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0338\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0102\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0505\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0099\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0572\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0295\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0156\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0462\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0559\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  66%|██████▌   | 406/614 [00:13<00:04, 43.80it/s, loss=0.599, iou=0.342, dice=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0046\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0449\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0213\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0339\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0103\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0506\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0280\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0170\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0573\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  68%|██████▊   | 416/614 [00:14<00:04, 43.90it/s, loss=0.597, iou=0.343, dice=0.467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0296\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0060\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0157\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0463\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0324\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0047\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0214\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0520\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0104\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  69%|██████▉   | 425/614 [00:14<00:04, 43.22it/s, loss=0.598, iou=0.341, dice=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0507\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0281\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0171\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0574\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0297\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0061\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0158\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0464\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0325\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  71%|███████   | 435/614 [00:14<00:04, 43.78it/s, loss=0.598, iou=0.342, dice=0.467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0048\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0215\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0521\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0105\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0508\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0282\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0172\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0575\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0298\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  72%|███████▏  | 443/614 [00:14<00:03, 44.06it/s, loss=0.598, iou=0.343, dice=0.468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0062\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0159\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0465\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0326\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0049\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0216\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0522\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0106\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0509\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  73%|███████▎  | 451/614 [00:14<00:03, 41.90it/s, loss=0.596, iou=0.342, dice=0.467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0283\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0450\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0173\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0576\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0340\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0063\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0466\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0230\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0327\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  75%|███████▍  | 460/614 [00:15<00:03, 41.08it/s, loss=0.597, iou=0.34, dice=0.465] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0120\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0217\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0523\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0107\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0284\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0590\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0451\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0174\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0577\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  77%|███████▋  | 470/614 [00:15<00:03, 41.50it/s, loss=0.597, iou=0.34, dice=0.465] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0341\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0064\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0467\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0231\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0328\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0121\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0218\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0524\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0108\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  78%|███████▊  | 478/614 [00:15<00:03, 42.07it/s, loss=0.597, iou=0.34, dice=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0285\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0591\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0452\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0175\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0578\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0342\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0065\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0468\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0232\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  79%|███████▉  | 487/614 [00:15<00:02, 42.36it/s, loss=0.596, iou=0.341, dice=0.467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0329\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0122\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0219\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0525\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0109\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0286\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0592\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0050\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0453\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  81%|████████  | 496/614 [00:15<00:02, 42.69it/s, loss=0.596, iou=0.341, dice=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0176\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0579\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0343\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0510\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0066\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0469\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0233\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0400\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0190\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  82%|████████▏ | 505/614 [00:16<00:02, 42.92it/s, loss=0.597, iou=0.341, dice=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0287\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0593\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0051\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0454\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0177\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0344\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0511\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0067\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0234\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  84%|████████▍ | 515/614 [00:16<00:02, 43.51it/s, loss=0.601, iou=0.34, dice=0.466] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0401\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0191\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0288\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0594\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0052\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0455\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0178\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0345\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0512\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  85%|████████▌ | 523/614 [00:16<00:02, 43.61it/s, loss=0.601, iou=0.339, dice=0.464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0068\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0235\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0402\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0192\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0289\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0595\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0053\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0456\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0179\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  86%|████████▌ | 525/614 [00:16<00:02, 43.11it/s, loss=0.6, iou=0.339, dice=0.465]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0346\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0110\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  86%|████████▋ | 530/614 [00:17<00:04, 19.52it/s, loss=0.602, iou=0.339, dice=0.464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0513\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0069\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0236\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0000\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  87%|████████▋ | 534/614 [00:17<00:04, 18.64it/s, loss=0.602, iou=0.338, dice=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0403\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0580\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0470\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0193\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  88%|████████▊ | 538/614 [00:17<00:04, 17.32it/s, loss=0.601, iou=0.338, dice=0.464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0596\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0054\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0360\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0457\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  88%|████████▊ | 541/614 [00:18<00:04, 16.31it/s, loss=0.601, iou=0.337, dice=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0347\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0111\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0514\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  89%|████████▊ | 544/614 [00:18<00:05, 13.43it/s, loss=0.601, iou=0.337, dice=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0237\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0001\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0404\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  89%|████████▉ | 546/614 [00:18<00:05, 13.02it/s, loss=0.601, iou=0.337, dice=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0581\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0471\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0194\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  90%|████████▉ | 550/614 [00:18<00:05, 12.21it/s, loss=0.6, iou=0.337, dice=0.463]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0597\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0055\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0361\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  90%|████████▉ | 552/614 [00:19<00:05, 11.97it/s, loss=0.601, iou=0.337, dice=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0458\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0348\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0112\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  91%|█████████ | 560/614 [00:19<00:03, 16.08it/s, loss=0.601, iou=0.337, dice=0.464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0515\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0238\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0002\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0405\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0582\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0472\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0195\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0598\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  92%|█████████▏| 566/614 [00:19<00:02, 21.81it/s, loss=0.602, iou=0.337, dice=0.464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0056\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0362\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0459\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0349\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0113\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  93%|█████████▎| 574/614 [00:19<00:01, 25.00it/s, loss=0.602, iou=0.338, dice=0.465]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0516\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0239\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0003\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0406\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0180\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0583\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0070\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0473\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0196\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  95%|█████████▍| 583/614 [00:20<00:00, 32.36it/s, loss=0.601, iou=0.341, dice=0.468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0599\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0057\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0363\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0224\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0530\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0114\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0420\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0517\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0004\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  97%|█████████▋| 593/614 [00:20<00:00, 36.26it/s, loss=0.601, iou=0.341, dice=0.468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0407\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0181\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0584\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0071\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0474\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0197\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0058\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0364\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0225\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  98%|█████████▊| 602/614 [00:20<00:00, 40.34it/s, loss=0.602, iou=0.34, dice=0.467] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0531\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0115\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0421\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0518\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0005\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0408\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0182\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0585\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0072\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0475\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting: 100%|█████████▉| 612/614 [00:20<00:00, 42.76it/s, loss=0.602, iou=0.339, dice=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0198\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0059\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0365\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0226\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0532\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0116\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0422\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0519\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0006\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n",
      "\n",
      "Processing image ID: 0409\n",
      "Input ultrasound image size (H, W): torch.Size([384, 384])\n",
      "Ground truth mask size (H, W): torch.Size([384, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting: 100%|██████████| 614/614 [00:20<00:00, 29.45it/s, loss=0.602, iou=0.34, dice=0.467] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Test Loss: 0.6016\n",
      "Test IoU: 0.3397\n",
      "Test Dice: 0.4666\n",
      "Predicted masks saved to models/atten_transUnet_test/predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import Compose\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import archs \n",
    "import losses\n",
    "from metrics import iou_score\n",
    "from dataset_alt import Dataset \n",
    "\n",
    "\n",
    "def validate_and_predict(config, test_loader, model, criterion, output_dir):\n",
    "    avg_meters = {'loss': AverageMeter(), # \n",
    "                  'iou': AverageMeter(),\n",
    "                  'dice': AverageMeter()}\n",
    "\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(test_loader), desc=\"Validating and Predicting\")\n",
    "        for i, (input, target, metadata) in enumerate(test_loader):\n",
    "            img_id = metadata['img_id'][0] \n",
    "\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            target_original_shape = target.shape[1:] # Get H, W of original mask\n",
    "            target = target.unsqueeze(1) # Add channel dimension\n",
    "\n",
    "            # Print original image and mask sizes\n",
    "            print(f\"\\nProcessing image ID: {img_id}\") \n",
    "            print(f\"Input ultrasound image size (H, W): {input.shape[2:]}\")\n",
    "            print(f\"Ground truth mask size (H, W): {target_original_shape}\")\n",
    "\n",
    "            if config['deep_supervision']:\n",
    "                outputs = model(input)\n",
    "                loss = 0\n",
    "                for output in outputs:\n",
    "                    loss += criterion(output, target)\n",
    "                loss /= len(outputs)\n",
    "                output = outputs[-1] # Use the last output for final prediction\n",
    "            else:\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            output = F.interpolate(output, size=target_original_shape, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            iou, dice = iou_score(output, target) \n",
    "\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['iou'].update(iou, input.size(0))\n",
    "            avg_meters['dice'].update(dice, input.size(0))\n",
    "\n",
    "            postfix = OrderedDict([\n",
    "                ('loss', avg_meters['loss'].avg),\n",
    "                ('iou', avg_meters['iou'].avg),\n",
    "                ('dice', avg_meters['dice'].avg)\n",
    "            ])\n",
    "            pbar.set_postfix(postfix)\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Save prediction mask\n",
    "            pred_mask = torch.sigmoid(output).cpu().numpy()[0, 0] # Take first image in batch, first channel\n",
    "            pred_mask[pred_mask>=0.5]=1   \n",
    "            pred_mask[pred_mask<0.5]=0\n",
    "            \n",
    "            pred_mask = (pred_mask * 255).astype(np.uint8)\n",
    "            predicted_image = Image.fromarray(pred_mask)\n",
    "            predicted_image.save(os.path.join(output_dir, f\"{img_id}_pred.jpg\"))\n",
    "\n",
    "        pbar.close()\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg),\n",
    "                        ('dice', avg_meters['dice'].avg)])\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Load the configuration used for training\n",
    "    config_path = 'models/atten_transUnet_test/config.yml' \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    print('-' * 20)\n",
    "    print(\"Loaded training configuration:\")\n",
    "    for key in config:\n",
    "        print(f'{key}: {config[key]}')\n",
    "    print('-' * 20)\n",
    "\n",
    "    # Define loss function (criterion) - must match training\n",
    "    if config['loss'] == 'BCEDiceLoss':\n",
    "        criterion = BCEDiceLoss().cuda()\n",
    "    else:\n",
    "        # Assuming other losses are defined in losses.py\n",
    "        criterion = losses.__dict__[config['loss']]().cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    model = HybridAttentionUNet(n_classes=config['num_classes'])  \n",
    "    model = model.cuda()\n",
    "\n",
    "    model_path = 'models/atten_transUnet_test/model.pth'  \n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Successfully loaded model weights from {model_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Model weights not found at {model_path}. Please ensure the training was successful.\")\n",
    "        return\n",
    "\n",
    "    # Data loading code for test set\n",
    "    test_img_dir = 'dataset_TN3K/test-image'\n",
    "    test_mask_dir = 'dataset_TN3K/test-mask'\n",
    "\n",
    "    test_img_ids = glob(os.path.join(test_img_dir, f'*{config[\"img_ext\"]}'))\n",
    "    test_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in test_img_ids]\n",
    "\n",
    "    if not test_img_ids:\n",
    "        print(f\"No test images found in {test_img_dir}. Please check the path and extension.\")\n",
    "        return\n",
    "\n",
    "    test_transform = Compose([\n",
    "        A.Resize(config['input_h'], config['input_w']), # Resize for model input\n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    test_dataset = Dataset(\n",
    "        img_ids=test_img_ids,\n",
    "        img_dir=test_img_dir,\n",
    "        mask_dir=test_mask_dir,\n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1, \n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    output_prediction_dir = 'models/atten_transUnet_test/predictions' \n",
    "    print(f\"\\nStarting evaluation and prediction saving to: {output_prediction_dir}\")\n",
    "    val_log = validate_and_predict(config, test_loader, model, criterion, output_prediction_dir)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Test Loss: {val_log['loss']:.4f}\")\n",
    "    print(f\"Test IoU: {val_log['iou']:.4f}\")\n",
    "    print(f\"Test Dice: {val_log['dice']:.4f}\")\n",
    "    print(f\"Predicted masks saved to {output_prediction_dir}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d412c3b-e409-4823-aff2-c826bae26736",
   "metadata": {},
   "source": [
    "--- Evaluation Results ---  \n",
    "Test Loss: 0.6016  \n",
    "Test IoU: 0.3397  \n",
    "Test Dice: 0.4666  \n",
    "Predicted masks saved to models/atten_transUnet_test/predictions\n",
    "\n",
    "### 1.7 find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5c00a51-bde8-4037-9f5a-48978c7e5ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model by Dice coefficient of atten_transUnet_384_500epoch:\n",
      "epoch       4.000000\n",
      "lr          0.000001\n",
      "loss        0.651742\n",
      "iou         0.379094\n",
      "val_loss    0.627975\n",
      "val_iou     0.417066\n",
      "val_dice    0.584978\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "Best model by IoU:\n",
      "epoch       4.000000\n",
      "lr          0.000001\n",
      "loss        0.651742\n",
      "iou         0.379094\n",
      "val_loss    0.627975\n",
      "val_iou     0.417066\n",
      "val_dice    0.584978\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log_path = 'models/atten_transUnet_test/log.csv'\n",
    "df = pd.read_csv(log_path)\n",
    "\n",
    "best_model_by_dice = df.loc[df['val_dice'].idxmax()]\n",
    "best_model_by_iou = df.loc[df['val_iou'].idxmax()]\n",
    "\n",
    "print(\"Best model by Dice coefficient of atten_transUnet_384_500epoch:\")\n",
    "print(best_model_by_dice)\n",
    "\n",
    "print(\"\\nBest model by IoU:\")\n",
    "print(best_model_by_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47164d7d-3fac-4d02-b605-c3e5cf1e966b",
   "metadata": {},
   "source": [
    "### 1.8 Plot curves\n",
    "#### Loss and IoU curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e78d2a6-1670-4262-9a5a-5c01ca8e2458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_path = 'models/atten_transUnet_test/log.csv'\n",
    "df = pd.read_csv(log_path)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))  \n",
    "\n",
    "axes[0].plot(df['epoch'], df['loss'], label='Train Loss')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training vs Validation Loss of attention_unet of atten_transUnet_test')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(df['epoch'], df['iou'], label='Train IoU')\n",
    "axes[1].plot(df['epoch'], df['val_iou'], label='Validation IoU')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('IoU')\n",
    "axes[1].set_title('Training vs Validation IoU')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883dd02-1b1c-449c-8f02-07d17e97fa3b",
   "metadata": {},
   "source": [
    "#### Learning rate curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e0dc330-ec16-42c1-bd95-e01212971fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log = pd.read_csv('models/atten_transUnet_test/log.csv')\n",
    "epochs = log['epoch']\n",
    "lrs = log['lr']\n",
    "val_ious = log['val_iou']\n",
    "\n",
    "best_epochs = []\n",
    "best_so_far = -float('inf')\n",
    "for i in range(len(val_ious)):\n",
    "    if val_ious[i] > best_so_far:\n",
    "        best_so_far = val_ious[i]\n",
    "        best_epochs.append(i) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, lrs, marker='o', label='Learning Rate')\n",
    "\n",
    "max_epoch = epochs.max()\n",
    "xticks = list(range(0, max_epoch + 1, 40))\n",
    "plt.xticks(xticks)\n",
    "\n",
    "plt.text(\n",
    "    epochs.iloc[-1],\n",
    "    lrs.iloc[-1] * 0.9,  \n",
    "    f'{epochs.iloc[-1]}',\n",
    "    color='black',\n",
    "    fontsize=10,\n",
    "    ha='right',\n",
    "    va='top'\n",
    ")\n",
    "\n",
    "plt.scatter(epochs.iloc[best_epochs], lrs.iloc[best_epochs], \n",
    "            s=80, color='red', label='Saved Best Model')\n",
    "\n",
    "# 图像美化\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('atten_transUnet_test: Learning Rate Curve')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ce8fb-0925-42ce-93bd-dabd6a12f849",
   "metadata": {},
   "source": [
    "## ------------------------------------------\n",
    "## 2/2 hybrid_Attention_transUnet_MobileViT\n",
    "### 2.1 Edit ResNetV2\n",
    "Part of the code in the following chunk was adapted from `from Beckschen_TransUNet.networks.vit_seg_modeling_resnet_skip import ResNetV2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e1d1ffc-1784-49ba-bdab-8745676a4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from os.path import join as pjoin\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "\n",
    "class StdConv2d(nn.Conv2d):\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
    "        w = (w - m) / torch.sqrt(v + 1e-5)\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n",
    "                        self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=bias, groups=groups)\n",
    "\n",
    "\n",
    "def conv1x1(cin, cout, stride=1, bias=False):\n",
    "    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n",
    "                     padding=0, bias=bias)\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation (v2) bottleneck block.\"\"\"\n",
    "\n",
    "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
    "        super().__init__()\n",
    "        cout = cout or cin\n",
    "        cmid = cmid or cout//4\n",
    "\n",
    "        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv1 = conv1x1(cin, cmid, bias=False)\n",
    "        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
    "        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1\n",
    "        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n",
    "        self.conv3 = conv1x1(cmid, cout, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if (stride != 1 or cin != cout):\n",
    "            # Projection also with pre-activation according to paper.\n",
    "            self.downsample = conv1x1(cin, cout, stride, bias=False)\n",
    "            self.gn_proj = nn.GroupNorm(cout, cout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Residual branch\n",
    "        residual = x\n",
    "        if hasattr(self, 'downsample'):\n",
    "            residual = self.downsample(x)\n",
    "            residual = self.gn_proj(residual)\n",
    "\n",
    "        # Unit's branch\n",
    "        y = self.relu(self.gn1(self.conv1(x)))\n",
    "        y = self.relu(self.gn2(self.conv2(y)))\n",
    "        y = self.gn3(self.conv3(y))\n",
    "\n",
    "        y = self.relu(residual + y)\n",
    "        return y\n",
    "\n",
    "    def load_from(self, weights, n_block, n_unit):\n",
    "        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n",
    "        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n",
    "        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n",
    "\n",
    "        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n",
    "        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n",
    "\n",
    "        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n",
    "        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n",
    "\n",
    "        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n",
    "        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n",
    "\n",
    "        self.conv1.weight.copy_(conv1_weight)\n",
    "        self.conv2.weight.copy_(conv2_weight)\n",
    "        self.conv3.weight.copy_(conv3_weight)\n",
    "\n",
    "        self.gn1.weight.copy_(gn1_weight.view(-1))\n",
    "        self.gn1.bias.copy_(gn1_bias.view(-1))\n",
    "\n",
    "        self.gn2.weight.copy_(gn2_weight.view(-1))\n",
    "        self.gn2.bias.copy_(gn2_bias.view(-1))\n",
    "\n",
    "        self.gn3.weight.copy_(gn3_weight.view(-1))\n",
    "        self.gn3.bias.copy_(gn3_bias.view(-1))\n",
    "\n",
    "        if hasattr(self, 'downsample'):\n",
    "            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n",
    "            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n",
    "            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n",
    "\n",
    "            self.downsample.weight.copy_(proj_conv_weight)\n",
    "            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n",
    "            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc5189f4-3e69-447d-a58d-62aa3ba02077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetV2(nn.Module):                              \n",
    "    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n",
    "\n",
    "    def __init__(self, block_units, width_factor):\n",
    "        super().__init__()\n",
    "        width = int(64 * width_factor)\n",
    "        self.width = width\n",
    "\n",
    "        self.root = nn.Sequential(OrderedDict([\n",
    "            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n",
    "            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n",
    "            ('relu', nn.ReLU(inplace=True)),\n",
    "            # ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
    "        ]))\n",
    "\n",
    "        self.body = nn.Sequential(OrderedDict([\n",
    "            ('block1', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n",
    "                ))),\n",
    "            ('block2', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n",
    "                ))),\n",
    "            ('block3', nn.Sequential(OrderedDict(\n",
    "                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n",
    "                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n",
    "                ))),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        b, c, in_size, _ = x.size()\n",
    "        \n",
    "        x = self.root(x)\n",
    "        features.append(x)  # feature 0 (64x112x112)\n",
    "    \n",
    "        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)(x)  # -> 56x56\n",
    "    \n",
    "        for i in range(len(self.body)-1):  # block1, block2\n",
    "            x = self.body[i](x)\n",
    "            right_size = int(in_size / 4 / (i+1))\n",
    "            if x.size()[2] != right_size:\n",
    "                pad = right_size - x.size()[2]\n",
    "                assert pad < 3 and pad > 0, f\"x {x.size()} should {right_size}\"\n",
    "                feat = torch.zeros((b, x.size()[1], right_size, right_size), device=x.device)\n",
    "                feat[:, :, 0:x.size()[2], 0:x.size()[3]] = x[:]\n",
    "            else:\n",
    "                feat = x\n",
    "            features.append(feat)\n",
    "    \n",
    "        # Also add the final feature of block3 to features (deepest layer)\n",
    "        x = self.body[-1](x)\n",
    "        features.append(x)  # Add this line\n",
    "    \n",
    "        return x, features[::-1]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a57b7-6ec7-4855-8884-eac074de2533",
   "metadata": {},
   "source": [
    "### 2.2 Define unetConv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4353e95b-726b-4bef-82c9-e65ca46ac452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ozan_oktay_Attention_UNet.models.networks_other import init_weights \n",
    "\n",
    "class unetConv2(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_batchnorm, n=2, ks=3, stride=1, padding=1):\n",
    "        super(unetConv2, self).__init__()\n",
    "        self.n = n\n",
    "        self.ks = ks\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        s = stride\n",
    "        p = padding\n",
    "        if is_batchnorm:\n",
    "            for i in range(1, n+1):\n",
    "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "                                     nn.BatchNorm2d(out_size),\n",
    "                                     nn.ReLU(inplace=True),)\n",
    "                setattr(self, 'conv%d'%i, conv)\n",
    "                in_size = out_size\n",
    "\n",
    "        else:\n",
    "            for i in range(1, n+1):\n",
    "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, ks, s, p),\n",
    "                                     nn.ReLU(inplace=True),)\n",
    "                setattr(self, 'conv%d'%i, conv)\n",
    "                in_size = out_size\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for i in range(1, self.n+1):\n",
    "            conv = getattr(self, 'conv%d'%i)\n",
    "            x = conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daaf19-4b45-419a-a62b-7a6fe2bd6a99",
   "metadata": {},
   "source": [
    "### 2.3 Modify unetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ffa72ab-0a7e-4361-9eea-59ee1fa8876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class unetUp(nn.Module):\n",
    "    def __init__(self, up_in_channels, cat_in_channels, out_channels, is_deconv):\n",
    "        super(unetUp, self).__init__()\n",
    "\n",
    "        if is_deconv:\n",
    "            self.up = nn.ConvTranspose2d(up_in_channels, up_in_channels, kernel_size=4, stride=2, padding=1)\n",
    "        else:\n",
    "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "        self.conv = unetConv2(cat_in_channels, out_channels, False)\n",
    "\n",
    "        # initialise\n",
    "        for m in self.children():\n",
    "            if isinstance(m, unetConv2): continue\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        outputs2 = self.up(inputs2)\n",
    "        offset = outputs2.size()[2] - inputs1.size()[2]\n",
    "        padding = 2 * [offset // 2, offset // 2]\n",
    "        outputs1 = F.pad(inputs1, padding)\n",
    "        return self.conv(torch.cat([outputs1, outputs2], dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0c8a4-4151-4a1e-99cc-6d5854e76b90",
   "metadata": {},
   "source": [
    "### 2.4 class HybridAttenTransMob 🔥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e944f95-09c8-4633-8be1-d654e819537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from ozan_oktay_Attention_UNet.models.layers.nonlocal_layer import NONLocalBlock2D\n",
    "from src.module import MobileVitBlock  \n",
    "\n",
    "\n",
    "class HybridAttenTransMob(nn.Module):\n",
    "    def __init__(self, n_classes=1, is_deconv=True, feature_scale=4, nonlocal_mode='embedded_gaussian', nonlocal_sf=4):\n",
    "        super(HybridAttenTransMob, self).__init__()\n",
    "\n",
    "        self.is_deconv = is_deconv\n",
    "        self.feature_scale = feature_scale\n",
    "\n",
    "        # Use ResNetV2 as the encoder (replacing the original conv1~center)\n",
    "        self.resnet_encoder = ResNetV2(block_units=[3, 4, 9], width_factor=1)\n",
    "\n",
    "        # Get the output channel dimensions from the encoder (as specified in TransUNet)\n",
    "        self.enc_channels = [64, 256, 512, 1024]  # order of features returned by ResNetV2 body\n",
    "\n",
    "        # Build non-local attention\n",
    "        self.nonlocal1 = NONLocalBlock2D(in_channels=self.enc_channels[0], inter_channels=self.enc_channels[0] // 4,\n",
    "                                         sub_sample_factor=nonlocal_sf, mode=nonlocal_mode)\n",
    "        self.nonlocal2 = NONLocalBlock2D(in_channels=self.enc_channels[1], inter_channels=self.enc_channels[1] // 4,\n",
    "                                         sub_sample_factor=nonlocal_sf, mode=nonlocal_mode)\n",
    "\n",
    "        # Decoder part (from the Attention U-Net structure)\n",
    "        self.up_concat4 = unetUp(up_in_channels=1024, cat_in_channels=2048, out_channels=512, is_deconv=is_deconv) # 第四版\n",
    "        self.up_concat3 = unetUp(512, 1024, 256, is_deconv)\n",
    "        self.up_concat2 = unetUp(256, 512, 128, is_deconv)\n",
    "        self.up_concat1 = unetUp(128, 192, 64, is_deconv)\n",
    "\n",
    "        # MobileViT block added to conv1 and conv2  \n",
    "        self.mv_block1 = MobileVitBlock(\n",
    "            in_channels=self.enc_channels[0],\n",
    "            out_channels=self.enc_channels[0],      \n",
    "            d_model=64,                             \n",
    "            layers=1,\n",
    "            mlp_dim=128\n",
    "        )\n",
    "\n",
    "        self.mv_block2 = MobileVitBlock(           \n",
    "            in_channels=self.enc_channels[1],\n",
    "            out_channels=self.enc_channels[1],\n",
    "            d_model=96,\n",
    "            layers=1,\n",
    "            mlp_dim=192\n",
    "        )\n",
    "        \n",
    "        # Reduce output channels before connecting to the classification layer\n",
    "        self.final = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "        \n",
    "        # Initialize the weights of the final conv layer\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, features = self.resnet_encoder(inputs)\n",
    "    \n",
    "        # Now features[0] is the deepest layer (512x28x28), and features[3] is the shallowest layer (64x112x112)\n",
    "        conv4 = features[0]\n",
    "        conv3 = features[1]\n",
    "        conv2 = features[2]\n",
    "        conv1 = features[3]\n",
    "    \n",
    "        # MobileViT block               \n",
    "        conv1 = self.mv_block1(conv1)\n",
    "        conv2 = self.mv_block2(conv2)\n",
    "        \n",
    "        # Non-local attention\n",
    "        conv1 = self.nonlocal1(conv1)\n",
    "        conv2 = self.nonlocal2(conv2)\n",
    "\n",
    "        center = x  # alternatively, use conv4\n",
    "        \n",
    "        up4 = self.up_concat4(conv4, center)\n",
    "        up3 = self.up_concat3(conv3, up4)\n",
    "        up2 = self.up_concat2(conv2, up3)\n",
    "        up1 = self.up_concat1(conv1, up2)\n",
    "    \n",
    "        final = self.final(up1)\n",
    "        \n",
    "        return final\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_argmax_softmax(pred):\n",
    "        return F.softmax(pred, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d21ee5-daed-4ded-b598-d0704dd2ec93",
   "metadata": {},
   "source": [
    "### 2.5 train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdaf64e5-65e7-4ae2-991e-11b8cab68af8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "name: atten_trans_Mob_test\n",
      "epochs: 5\n",
      "batch_size: 16\n",
      "arch: Axial\n",
      "deep_supervision: False\n",
      "input_channels: 3\n",
      "num_classes: 1\n",
      "input_w: 224\n",
      "input_h: 224\n",
      "loss: BCEDiceLoss\n",
      "dataset: ICOS\n",
      "img_ext: .jpg\n",
      "mask_ext: .jpg\n",
      "optimizer: Adam\n",
      "lr: 0.0005\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0001\n",
      "nesterov: False\n",
      "scheduler: CosineAnnealingLR\n",
      "min_lr: 1e-06\n",
      "factor: 0.5\n",
      "patience: 3\n",
      "milestones: 1,2\n",
      "gamma: 0.6666666666666666\n",
      "early_stopping: -1\n",
      "num_workers: 16\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/layers/nonlocal_layer.py:51: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  nn.init.constant(self.W[1].weight, 0)\n",
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/layers/nonlocal_layer.py:52: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  nn.init.constant(self.W[1].bias, 0)\n",
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/networks_other.py:42: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/networks_other.py:46: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  init.normal(m.weight.data, 1.0, 0.02)\n",
      "/data/home/ha24521/DL/thyroid_ultrasound_segmentation/ozan_oktay_Attention_UNet/models/networks_other.py:47: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  init.constant(m.bias.data, 0.0)\n",
      "/data/home/ha24521/.conda/envs/Practical_5/lib/python3.11/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/data/home/ha24521/.conda/envs/Practical_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable parameters 51896017\n",
      "Epoch [0/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]/data/home/ha24521/.conda/envs/Practical_5/lib/python3.11/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "100%|██████████| 143/143 [00:39<00:00,  3.58it/s, loss=1.31, iou=0.18] \n",
      "100%|██████████| 36/36 [00:03<00:00,  9.03it/s, loss=0.795, iou=0.279, dice=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.3135 - iou 0.1799 - val_loss 0.7946 - val_iou 0.2787\n",
      "Current LR: 0.0004523497400965494\n",
      "=> saved best model\n",
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:34<00:00,  4.11it/s, loss=0.795, iou=0.22] \n",
      "100%|██████████| 36/36 [00:03<00:00, 10.17it/s, loss=0.762, iou=0.281, dice=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7951 - iou 0.2198 - val_loss 0.7624 - val_iou 0.2807\n",
      "Current LR: 0.0003275997400965494\n",
      "=> saved best model\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:34<00:00,  4.11it/s, loss=0.774, iou=0.248]\n",
      "100%|██████████| 36/36 [00:03<00:00, 10.01it/s, loss=0.759, iou=0.274, dice=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7744 - iou 0.2475 - val_loss 0.7588 - val_iou 0.2738\n",
      "Current LR: 0.0001734002599034506\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:34<00:00,  4.09it/s, loss=0.751, iou=0.269]\n",
      "100%|██████████| 36/36 [00:03<00:00, 10.01it/s, loss=0.738, iou=0.297, dice=0.457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7510 - iou 0.2686 - val_loss 0.7380 - val_iou 0.2972\n",
      "Current LR: 4.8650259903450626e-05\n",
      "=> saved best model\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:35<00:00,  4.08it/s, loss=0.725, iou=0.285]\n",
      "100%|██████████| 36/36 [00:03<00:00, 10.08it/s, loss=0.709, iou=0.31, dice=0.472] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7252 - iou 0.2847 - val_loss 0.7090 - val_iou 0.3103\n",
      "Current LR: 1e-06\n",
      "=> saved best model\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yaml\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import Compose, OneOf  # version of albumentaation 1.3.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from albumentations import RandomRotate90,Resize\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import archs\n",
    "import losses\n",
    "from dataset import Dataset\n",
    "from metrics import iou_score\n",
    "from utils import AverageMeter, str2bool\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--name', default=None,\n",
    "                        help='model name: (default: arch)')\n",
    "    parser.add_argument('--epochs', default=50, type=int, metavar='N',   \n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-b', '--batch_size', default=4, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 16)')\n",
    "    \n",
    "    # model\n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='Axial')   \n",
    "    parser.add_argument('--deep_supervision', default=False, type=str2bool)\n",
    "    parser.add_argument('--input_channels', default=3, type=int,           \n",
    "                        help='input channels')\n",
    "    parser.add_argument('--num_classes', default=1, type=int,              \n",
    "                        help='number of classes')\n",
    "    parser.add_argument('--input_w', default=128, type=int,                \n",
    "                        help='image width')\n",
    "    parser.add_argument('--input_h', default=128, type=int,\n",
    "                        help='image height')\n",
    "    \n",
    "    # loss\n",
    "    parser.add_argument('--loss', default='BCEDiceLoss',\n",
    "                        choices=LOSS_NAMES,\n",
    "                        help='loss: ' +\n",
    "                        ' | '.join(LOSS_NAMES) +\n",
    "                        ' (default: BCEDiceLoss)')\n",
    "    \n",
    "    # dataset\n",
    "    parser.add_argument('--dataset', default='ICOS',\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--img_ext', default='.jpg',\n",
    "                        help='image file extension')\n",
    "    parser.add_argument('--mask_ext', default='.jpg',\n",
    "                        help='mask file extension')\n",
    "\n",
    "    # optimizer\n",
    "    parser.add_argument('--optimizer', default='Adam',\n",
    "                        choices=['Adam', 'SGD'],\n",
    "                        help='loss: ' +\n",
    "                        ' | '.join(['Adam', 'SGD']) +\n",
    "                        ' (default: Adam)')\n",
    "    parser.add_argument('--lr', '--learning_rate', default=1e-4, type=float,  \n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float,\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float,           \n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--nesterov', default=False, type=str2bool,\n",
    "                        help='nesterov')\n",
    "\n",
    "    # scheduler\n",
    "    parser.add_argument('--scheduler', default='CosineAnnealingLR',\n",
    "                        choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])\n",
    "    parser.add_argument('--min_lr', default=1e-6, type=float,           \n",
    "                        help='minimum learning rate')\n",
    "    parser.add_argument('--factor', default=0.5, type=float)            \n",
    "    parser.add_argument('--patience', default=3, type=int)              \n",
    "    parser.add_argument('--milestones', default='1,2', type=str)\n",
    "    parser.add_argument('--gamma', default=2/3, type=float)\n",
    "    parser.add_argument('--early_stopping', default=-1, type=int,\n",
    "                        metavar='N', help='early stopping (default: -1)')\n",
    "    parser.add_argument('--cfg', type=str, metavar=\"FILE\", help='path to config file', )\n",
    "    # parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file', )\n",
    "\n",
    "    parser.add_argument('--num_workers', default=4, type=int)\n",
    "\n",
    "    config = parser.parse_args()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "def train(config, train_loader, model, criterion, optimizer):\n",
    "    avg_meters = {'loss': AverageMeter(),\n",
    "                  'iou': AverageMeter()}\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=len(train_loader))\n",
    "    for input, target in train_loader:     \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        target = target.unsqueeze(1)\n",
    "\n",
    "        # compute output\n",
    "        if config['deep_supervision']:\n",
    "            outputs = model(input)\n",
    "            loss = 0\n",
    "            for output in outputs:\n",
    "                loss += criterion(output, target)\n",
    "            loss /= len(outputs)\n",
    "            iou,dice = iou_score(outputs[-1], target)\n",
    "        else:\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            iou,dice = iou_score(output, target)\n",
    "\n",
    "        # compute gradient and do optimizing step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "        avg_meters['iou'].update(iou, input.size(0))\n",
    "\n",
    "        postfix = OrderedDict([\n",
    "            ('loss', avg_meters['loss'].avg),\n",
    "            ('iou', avg_meters['iou'].avg),\n",
    "        ])\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "\n",
    "def validate(config, val_loader, model, criterion):\n",
    "    avg_meters = {'loss': AverageMeter(),\n",
    "                  'iou': AverageMeter(),\n",
    "                  'dice': AverageMeter()}\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(val_loader))\n",
    "        for input, target in val_loader:     \n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            target = target.unsqueeze(1)\n",
    "\n",
    "            # compute output\n",
    "            if config['deep_supervision']:\n",
    "                outputs = model(input)\n",
    "                loss = 0\n",
    "                for output in outputs:\n",
    "                    loss += criterion(output, target)\n",
    "                loss /= len(outputs)\n",
    "                iou,dice = iou_score(outputs[-1], target)\n",
    "            else:\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "                iou,dice = iou_score(output, target)\n",
    "\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['iou'].update(iou, input.size(0))\n",
    "            avg_meters['dice'].update(dice, input.size(0))\n",
    "            postfix = OrderedDict([\n",
    "                ('loss', avg_meters['loss'].avg),\n",
    "                ('iou', avg_meters['iou'].avg),\n",
    "                ('dice', avg_meters['dice'].avg)\n",
    "            ])\n",
    "            pbar.set_postfix(postfix)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg),\n",
    "                        ('dice', avg_meters['dice'].avg)])\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "        weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "        wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "        wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "        pred = torch.sigmoid(pred)\n",
    "        smooth = 1\n",
    "        size = pred.size(0)\n",
    "        pred_flat = pred.view(size, -1)\n",
    "        mask_flat = mask.view(size, -1)\n",
    "        intersection = pred_flat * mask_flat\n",
    "        dice_score = (2 * intersection.sum(1) + smooth) / (pred_flat.sum(1) + mask_flat.sum(1) + smooth)\n",
    "        dice_loss = 1 - dice_score.sum() / size\n",
    "\n",
    "        return (wbce + 0.6*dice_loss).mean()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def main():\n",
    "    \n",
    "    config = {\n",
    "    'name': 'atten_trans_Mob_test',\n",
    "    'epochs': 5, # I recommend 200 epochs for fully demonstrating the potential of transUnet. Here I use 5 epochs to test whether the code runs successfully.\n",
    "    'batch_size': 16,              \n",
    "    'arch': 'Axial',\n",
    "    'deep_supervision': False,\n",
    "    'input_channels': 3,\n",
    "    'num_classes': 1,\n",
    "    'input_w': 224,\n",
    "    'input_h': 224,\n",
    "    'loss': 'BCEDiceLoss',\n",
    "    'dataset': 'ICOS',\n",
    "    'img_ext': '.jpg',\n",
    "    'mask_ext': '.jpg',\n",
    "    'optimizer': 'Adam',\n",
    "    \n",
    "    'lr': 5e-4,                       \n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-4,\n",
    "    'nesterov': False,\n",
    "    'scheduler': 'CosineAnnealingLR',\n",
    "    'min_lr': 1e-6,                  \n",
    "    'factor': 0.5,                   \n",
    "    'patience': 3,                   \n",
    "    'milestones': '1,2',\n",
    "    'gamma': 2/3,\n",
    "    'early_stopping': -1,          \n",
    "    'num_workers': 16              \n",
    "}\n",
    "    set_seed(42)\n",
    "    \n",
    "    if config['name'] is None:\n",
    "        if config['deep_supervision']:\n",
    "            config['name'] = '%s_%s_wDS' % (config['dataset'], config['arch'])\n",
    "        else:\n",
    "            config['name'] = '%s_%s_woDS' % (config['dataset'], config['arch'])\n",
    "    \n",
    "    os.makedirs('models/%s' % config['name'], exist_ok=True)\n",
    "\n",
    "    print('-' * 20)\n",
    "    for key in config:\n",
    "        print('%s: %s' % (key, config[key]))\n",
    "    print('-' * 20)\n",
    "\n",
    "    with open('models/%s/config.yml' % config['name'], 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    # define loss function (criterion)\n",
    "    if config['loss'] == 'BCEDiceLoss':\n",
    "        criterion = BCEDiceLoss().cuda() #nn.BCEWithLogitsLoss().cuda()\n",
    "    else:\n",
    "        criterion = losses.__dict__[config['loss']]().cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    model = HybridAttenTransMob(n_classes=config['num_classes'])   \n",
    "    model = model.cuda()\n",
    "\n",
    "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print (\"\\nTrainable parameters\", pytorch_total_params)\n",
    "    \n",
    "    # print(params)\n",
    "    if config['optimizer'] == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    elif config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n",
    "                              nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if config['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=config['epochs'], eta_min=config['min_lr'])\n",
    "    elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config['factor'], patience=config['patience'], min_lr=config['min_lr'])\n",
    "    elif config['scheduler'] == 'MultiStepLR':\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config['milestones'].split(',')], gamma=config['gamma'])\n",
    "    elif config['scheduler'] == 'ConstantLR':\n",
    "        scheduler = None\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Data loading code\n",
    "    train_img_ids = glob('dataset_TN3K/training_image/*')\n",
    "    train_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in train_img_ids]\n",
    "\n",
    "    val_img_ids = glob('dataset_TN3K/validation_image/*')\n",
    "    val_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in val_img_ids]\n",
    "    \n",
    "\n",
    "    train_transform = Compose([\n",
    "        RandomRotate90(),\n",
    "        A.HorizontalFlip(),        \n",
    "        A.VerticalFlip(),          \n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.5, rotate_limit=45, p=0.75),\n",
    "        A.Transpose(),\n",
    "        Resize(config['input_h'], config['input_w']), \n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    val_transform = Compose([\n",
    "        Resize(config['input_h'], config['input_w']),\n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = Dataset(\n",
    "        img_ids=train_img_ids,\n",
    "        img_dir='dataset_TN3K/training_image',\n",
    "        mask_dir='dataset_TN3K/training_mask',\n",
    "        \n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=train_transform)\n",
    "    \n",
    "    val_dataset = Dataset(\n",
    "        img_ids=val_img_ids,\n",
    "        img_dir=os.path.join('dataset_TN3K/validation_image'),\n",
    "        mask_dir=os.path.join('dataset_TN3K/validation_mask'),\n",
    "        \n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=val_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        drop_last=False)\n",
    "    log = OrderedDict([\n",
    "        ('epoch', []),\n",
    "        ('lr', []),\n",
    "        ('loss', []),\n",
    "        ('iou', []),\n",
    "        ('val_loss', []),\n",
    "        ('val_iou', []),\n",
    "        ('val_dice', []),\n",
    "    ])\n",
    "\n",
    "    best_iou = 0\n",
    "    trigger = 0\n",
    "    for epoch in range(config['epochs']):\n",
    "        print('Epoch [%d/%d]' % (epoch, config['epochs']))\n",
    "\n",
    "        # train for one epoch\n",
    "        train_log = train(config, train_loader, model, criterion, optimizer)\n",
    "        \n",
    "        # evaluate on validation set\n",
    "        val_log = validate(config, val_loader, model, criterion)\n",
    "\n",
    "        if config['scheduler'] == 'CosineAnnealingLR':\n",
    "            scheduler.step()\n",
    "        elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "            scheduler.step(val_log['loss'])\n",
    "\n",
    "        print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n",
    "              % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n",
    "\n",
    "        log['epoch'].append(epoch)\n",
    "        log['lr'].append(optimizer.param_groups[0]['lr'])       \n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']}\") \n",
    "\n",
    "        log['loss'].append(train_log['loss'])\n",
    "        log['iou'].append(train_log['iou'])\n",
    "        log['val_loss'].append(val_log['loss'])\n",
    "        log['val_iou'].append(val_log['iou'])\n",
    "        log['val_dice'].append(val_log['dice'])\n",
    "\n",
    "        pd.DataFrame(log).to_csv('models/%s/log.csv' % \n",
    "                                 config['name'], index=False)\n",
    "        trigger += 1\n",
    "\n",
    "        if val_log['iou'] > best_iou:\n",
    "            torch.save(model.state_dict(), 'models/%s/model.pth' %\n",
    "                       config['name'])\n",
    "            best_iou = val_log['iou']\n",
    "            print(\"=> saved best model\")\n",
    "            trigger = 0\n",
    "\n",
    "        # early stopping\n",
    "        if config['early_stopping'] >= 0 and trigger >= config['early_stopping']:\n",
    "            print(\"=> early stopping\")\n",
    "            break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1a9ab-afdc-4e7e-b205-249d761d21ec",
   "metadata": {},
   "source": [
    "### 2.6 calculate IoU and Dice\n",
    "atten_trans_Mob_test  \n",
    "dataset_TN3K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf121502-fb60-4055-b606-192576f96eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Loaded training configuration:\n",
      "arch: Axial\n",
      "batch_size: 16\n",
      "dataset: ICOS\n",
      "deep_supervision: False\n",
      "early_stopping: -1\n",
      "epochs: 5\n",
      "factor: 0.5\n",
      "gamma: 0.6666666666666666\n",
      "img_ext: .jpg\n",
      "input_channels: 3\n",
      "input_h: 224\n",
      "input_w: 224\n",
      "loss: BCEDiceLoss\n",
      "lr: 0.0005\n",
      "mask_ext: .jpg\n",
      "milestones: 1,2\n",
      "min_lr: 1e-06\n",
      "momentum: 0.9\n",
      "name: atten_trans_Mob_test\n",
      "nesterov: False\n",
      "num_classes: 1\n",
      "num_workers: 16\n",
      "optimizer: Adam\n",
      "patience: 3\n",
      "scheduler: CosineAnnealingLR\n",
      "weight_decay: 0.0001\n",
      "--------------------\n",
      "Successfully loaded model weights from models/atten_trans_Mob_test/model.pth\n",
      "\n",
      "Starting evaluation and prediction saving to: models/atten_trans_Mob_test/predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   0%|          | 0/614 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0183\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   1%|          | 6/614 [00:01<01:41,  5.97it/s, loss=0.837, iou=0.154, dice=0.243]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0586\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0350\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0073\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0476\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0240\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0366\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   2%|▏         | 15/614 [00:01<00:41, 14.30it/s, loss=0.79, iou=0.218, dice=0.328] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0130\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0227\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0533\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0020\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0117\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0423\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0007\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0184\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0490\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0587\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   4%|▍         | 26/614 [00:01<00:22, 26.32it/s, loss=0.73, iou=0.26, dice=0.38]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0351\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0074\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0477\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0241\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0367\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0131\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0228\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0534\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0021\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0118\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0424\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   6%|▌         | 37/614 [00:02<00:15, 36.09it/s, loss=0.713, iou=0.259, dice=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0008\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0185\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0491\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0588\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0352\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0075\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0478\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0242\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0368\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0132\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0229\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:   8%|▊         | 49/614 [00:02<00:13, 43.45it/s, loss=0.75, iou=0.234, dice=0.348] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0535\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0119\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0009\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0186\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0492\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0589\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0353\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0076\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0479\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0243\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0410\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  10%|▉         | 60/614 [00:02<00:11, 46.55it/s, loss=0.75, iou=0.227, dice=0.338] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0369\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0133\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0536\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0300\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0090\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0187\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0493\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0354\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0077\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0244\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0411\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  11%|█         | 69/614 [00:02<00:12, 44.73it/s, loss=0.744, iou=0.226, dice=0.338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0134\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0537\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0301\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0091\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0188\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0494\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0355\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0078\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0245\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0412\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  13%|█▎        | 81/614 [00:03<00:10, 48.99it/s, loss=0.737, iou=0.245, dice=0.362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0135\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0538\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0302\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0299\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0092\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0189\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0495\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0356\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0079\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0246\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0010\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  15%|█▍        | 91/614 [00:03<00:10, 51.00it/s, loss=0.728, iou=0.251, dice=0.37] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0413\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0136\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0539\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0303\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0480\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0370\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0093\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0496\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0260\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0357\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0247\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  17%|█▋        | 102/614 [00:03<00:09, 51.75it/s, loss=0.725, iou=0.261, dice=0.383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0011\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0414\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0137\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0304\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0481\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0371\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0094\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0497\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0261\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0358\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0248\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  18%|█▊        | 113/614 [00:03<00:09, 51.07it/s, loss=0.718, iou=0.264, dice=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0012\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0415\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0138\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0305\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0482\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0372\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0095\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0498\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0262\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0359\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0123\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  20%|██        | 125/614 [00:03<00:09, 52.80it/s, loss=0.717, iou=0.263, dice=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0526\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0249\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0013\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0416\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0139\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0306\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0080\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0483\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0373\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0540\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0096\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0499\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  22%|██▏       | 137/614 [00:04<00:09, 52.30it/s, loss=0.718, iou=0.257, dice=0.38] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0263\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0124\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0430\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0527\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0014\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0320\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0417\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0307\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0081\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0484\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0374\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  24%|██▍       | 147/614 [00:04<00:09, 51.77it/s, loss=0.716, iou=0.258, dice=0.382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0541\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0097\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0264\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0125\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0431\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0528\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0015\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0321\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0418\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0308\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0082\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  26%|██▌       | 157/614 [00:04<00:09, 49.74it/s, loss=0.712, iou=0.264, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0485\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0375\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0542\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0265\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0126\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0432\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0529\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0016\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0322\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  27%|██▋       | 166/614 [00:04<00:09, 48.45it/s, loss=0.713, iou=0.263, dice=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0419\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0309\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0083\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0486\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0250\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0376\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0140\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0543\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0266\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0030\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  29%|██▊       | 176/614 [00:04<00:09, 47.39it/s, loss=0.712, iou=0.262, dice=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0127\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0433\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0600\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0017\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0323\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0084\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0390\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0487\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0251\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0377\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  30%|███       | 187/614 [00:05<00:08, 50.13it/s, loss=0.713, iou=0.259, dice=0.383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0141\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0544\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0267\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0031\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0128\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0434\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0601\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0018\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0085\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0391\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0488\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  32%|███▏      | 199/614 [00:05<00:08, 49.94it/s, loss=0.712, iou=0.258, dice=0.38] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0252\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0378\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0142\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0545\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0268\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0032\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0129\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0435\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0602\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0019\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0086\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  34%|███▍      | 209/614 [00:05<00:07, 51.02it/s, loss=0.712, iou=0.256, dice=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0392\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0489\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0253\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0379\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0143\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0546\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0310\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0269\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0033\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0436\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0200\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  36%|███▌      | 220/614 [00:05<00:07, 49.89it/s, loss=0.71, iou=0.255, dice=0.377] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0603\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0087\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0393\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0254\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0560\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0144\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0547\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0311\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0034\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0437\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0201\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  38%|███▊      | 231/614 [00:05<00:07, 49.59it/s, loss=0.707, iou=0.254, dice=0.375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0604\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0088\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0394\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0255\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0561\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0145\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0548\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0312\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0035\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0438\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  39%|███▉      | 241/614 [00:06<00:07, 50.70it/s, loss=0.704, iou=0.256, dice=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0202\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0605\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0199\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0089\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0395\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0256\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0562\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0146\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0549\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0313\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0036\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  41%|████      | 252/614 [00:06<00:06, 52.03it/s, loss=0.704, iou=0.255, dice=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0439\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0203\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0606\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0380\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0270\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0396\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0160\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0257\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0563\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0147\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0314\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  43%|████▎     | 264/614 [00:06<00:06, 52.55it/s, loss=0.703, iou=0.256, dice=0.378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0037\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0204\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0607\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0381\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0271\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0397\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0161\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0258\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0564\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0022\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0425\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  45%|████▍     | 274/614 [00:06<00:06, 51.30it/s, loss=0.7, iou=0.259, dice=0.382]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0148\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0315\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0038\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0205\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0608\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0382\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0272\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0398\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0162\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0259\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0565\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  47%|████▋     | 286/614 [00:07<00:06, 53.02it/s, loss=0.7, iou=0.257, dice=0.379]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0023\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0426\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0149\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0316\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0039\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0206\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0609\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0383\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0550\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0273\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0440\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  48%|████▊     | 295/614 [00:07<00:06, 50.50it/s, loss=0.699, iou=0.257, dice=0.38] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0399\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0163\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0566\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0024\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0330\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0427\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0220\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0317\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0207\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0384\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  50%|████▉     | 306/614 [00:07<00:06, 49.94it/s, loss=0.696, iou=0.26, dice=0.384] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0551\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0274\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0441\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0164\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0567\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0025\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0331\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0428\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0221\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0318\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  51%|█████▏    | 316/614 [00:07<00:06, 49.43it/s, loss=0.696, iou=0.261, dice=0.384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0208\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0385\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0552\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0275\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0442\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0165\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0568\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0026\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0332\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0429\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0222\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  53%|█████▎    | 327/614 [00:07<00:05, 51.39it/s, loss=0.695, iou=0.264, dice=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0319\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0209\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0386\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0150\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0553\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0276\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0040\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0443\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0610\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0166\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0569\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  55%|█████▍    | 337/614 [00:08<00:05, 49.53it/s, loss=0.694, iou=0.264, dice=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0027\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0333\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0500\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0223\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0290\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0387\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0151\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0554\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0277\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0041\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  57%|█████▋    | 348/614 [00:08<00:05, 50.14it/s, loss=0.694, iou=0.264, dice=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0444\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0611\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0167\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0028\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0334\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0501\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0291\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0388\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0152\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0555\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0278\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  59%|█████▊    | 360/614 [00:08<00:05, 50.59it/s, loss=0.694, iou=0.265, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0042\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0445\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0612\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0168\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0029\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0335\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0502\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0292\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0389\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0153\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0556\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  60%|██████    | 369/614 [00:08<00:05, 48.72it/s, loss=0.694, iou=0.266, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0279\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0043\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0446\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0210\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0613\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0169\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0336\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0100\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0503\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0570\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  62%|██████▏   | 380/614 [00:08<00:04, 50.73it/s, loss=0.695, iou=0.263, dice=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0293\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0154\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0460\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0557\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0044\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0447\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0211\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0337\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0101\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0504\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0098\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  64%|██████▎   | 390/614 [00:09<00:04, 49.25it/s, loss=0.693, iou=0.265, dice=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0571\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0294\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0155\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0461\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0558\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0045\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0448\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0212\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0338\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0102\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  65%|██████▌   | 402/614 [00:09<00:04, 51.15it/s, loss=0.693, iou=0.266, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0505\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0099\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0572\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0295\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0156\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0462\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0559\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0046\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0449\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0213\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0339\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  67%|██████▋   | 411/614 [00:09<00:04, 50.63it/s, loss=0.691, iou=0.268, dice=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0103\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0506\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0280\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0170\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0573\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0296\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0060\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0157\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0463\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0324\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  69%|██████▊   | 422/614 [00:09<00:03, 50.66it/s, loss=0.69, iou=0.268, dice=0.392] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0047\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0214\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0520\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0104\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0507\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0281\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0171\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0574\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0297\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0061\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0158\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  71%|███████   | 433/614 [00:09<00:03, 50.78it/s, loss=0.689, iou=0.269, dice=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0464\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0325\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0048\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0215\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0521\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0105\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0508\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0282\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0172\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0575\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0298\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  72%|███████▏  | 444/614 [00:10<00:03, 52.39it/s, loss=0.689, iou=0.269, dice=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0062\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0159\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0465\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0326\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0049\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0216\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0522\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0106\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0509\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0283\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0450\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  74%|███████▍  | 454/614 [00:10<00:03, 50.82it/s, loss=0.688, iou=0.269, dice=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0173\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0576\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0340\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0063\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0466\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0230\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0327\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0120\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0217\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0523\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  76%|███████▌  | 466/614 [00:10<00:02, 51.52it/s, loss=0.689, iou=0.267, dice=0.39] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0107\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0284\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0590\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0451\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0174\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0577\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0341\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0064\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0467\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0231\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0328\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  78%|███████▊  | 476/614 [00:10<00:02, 51.14it/s, loss=0.688, iou=0.268, dice=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0121\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0218\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0524\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0108\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0285\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0591\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0452\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0175\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0578\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0342\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0065\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  79%|███████▉  | 487/614 [00:11<00:02, 49.64it/s, loss=0.688, iou=0.268, dice=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0468\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0232\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0329\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0122\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0219\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0525\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0109\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0286\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0592\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0050\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  81%|████████  | 498/614 [00:11<00:02, 52.00it/s, loss=0.688, iou=0.268, dice=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0453\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0176\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0579\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0343\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0510\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0066\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0469\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0233\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0400\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0190\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0287\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0593\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  83%|████████▎ | 510/614 [00:11<00:02, 50.77it/s, loss=0.692, iou=0.268, dice=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0051\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0454\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0177\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0344\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0511\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0067\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0234\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0401\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0191\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0288\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  85%|████████▍ | 519/614 [00:11<00:01, 50.64it/s, loss=0.691, iou=0.267, dice=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0594\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0052\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0455\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0178\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0345\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0512\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0068\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0235\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0402\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0192\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0289\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  86%|████████▋ | 530/614 [00:11<00:01, 50.83it/s, loss=0.693, iou=0.266, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0595\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0053\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0456\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0179\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0346\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0110\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0513\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0069\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0236\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0000\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0403\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  88%|████████▊ | 540/614 [00:12<00:01, 50.41it/s, loss=0.692, iou=0.265, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0580\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0470\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0193\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0596\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0054\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0360\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0457\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0347\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0111\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0514\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  90%|████████▉ | 552/614 [00:12<00:01, 51.07it/s, loss=0.691, iou=0.266, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0237\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0001\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0404\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0581\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0471\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0194\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0597\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0055\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0361\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0458\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0348\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  92%|█████████▏| 564/614 [00:12<00:00, 52.84it/s, loss=0.693, iou=0.265, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0112\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0515\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0238\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0002\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0405\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0582\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0472\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0195\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0598\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0056\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0362\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0459\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  94%|█████████▎| 575/614 [00:12<00:00, 52.72it/s, loss=0.695, iou=0.264, dice=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0349\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0113\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0516\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0239\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0003\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0406\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0180\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0583\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0070\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0473\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0196\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  95%|█████████▌| 585/614 [00:12<00:00, 51.77it/s, loss=0.695, iou=0.264, dice=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0599\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0057\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0363\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0224\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0530\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0114\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0420\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0517\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0004\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0407\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0181\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  97%|█████████▋| 597/614 [00:13<00:00, 53.57it/s, loss=0.694, iou=0.266, dice=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0584\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0071\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0474\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0197\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0058\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0364\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0225\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0531\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0115\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0421\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0518\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0005\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting:  99%|█████████▉| 609/614 [00:13<00:00, 54.21it/s, loss=0.695, iou=0.265, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0408\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0182\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0585\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0072\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0475\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0198\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0059\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0365\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0226\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0532\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0116\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating and Predicting: 100%|██████████| 614/614 [00:13<00:00, 45.42it/s, loss=0.695, iou=0.265, dice=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image ID: 0422\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0519\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0006\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "Processing image ID: 0409\n",
      "Input ultrasound image size (H, W): torch.Size([224, 224])\n",
      "Ground truth mask size (H, W): torch.Size([224, 224])\n",
      "\n",
      "--- Evaluation Results ---\n",
      "Test Loss: 0.6950\n",
      "Test IoU: 0.2651\n",
      "Test Dice: 0.3894\n",
      "Predicted masks saved to models/atten_trans_Mob_test/predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.core.composition import Compose\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import archs \n",
    "import losses\n",
    "from metrics import iou_score\n",
    "from dataset_alt import Dataset \n",
    "\n",
    "\n",
    "def validate_and_predict(config, test_loader, model, criterion, output_dir):\n",
    "    avg_meters = {'loss': AverageMeter(), # \n",
    "                  'iou': AverageMeter(),\n",
    "                  'dice': AverageMeter()}\n",
    "\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(test_loader), desc=\"Validating and Predicting\")\n",
    "        for i, (input, target, metadata) in enumerate(test_loader):\n",
    "            img_id = metadata['img_id'][0] \n",
    "\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            target_original_shape = target.shape[1:]  # Get H, W of original mask\n",
    "            target = target.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "            # Print original image and mask sizes\n",
    "            print(f\"\\nProcessing image ID: {img_id}\") \n",
    "            print(f\"Input ultrasound image size (H, W): {input.shape[2:]}\")\n",
    "            print(f\"Ground truth mask size (H, W): {target_original_shape}\")\n",
    "\n",
    "            # Compute output\n",
    "            if config['deep_supervision']:\n",
    "                outputs = model(input)\n",
    "                loss = 0\n",
    "                for output in outputs:\n",
    "                    loss += criterion(output, target)\n",
    "                loss /= len(outputs)\n",
    "                output = outputs[-1] # Use the last output for final prediction\n",
    "            else:\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            output = F.interpolate(output, size=target_original_shape, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            iou, dice = iou_score(output, target) \n",
    "\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['iou'].update(iou, input.size(0))\n",
    "            avg_meters['dice'].update(dice, input.size(0))\n",
    "\n",
    "            postfix = OrderedDict([\n",
    "                ('loss', avg_meters['loss'].avg),\n",
    "                ('iou', avg_meters['iou'].avg),\n",
    "                ('dice', avg_meters['dice'].avg)\n",
    "            ])\n",
    "            pbar.set_postfix(postfix)\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Save prediction mask\n",
    "            pred_mask = torch.sigmoid(output).cpu().numpy()[0, 0]  # Take first image in batch, first channel\n",
    "            pred_mask[pred_mask>=0.5]=1   \n",
    "            pred_mask[pred_mask<0.5]=0\n",
    "            \n",
    "            pred_mask = (pred_mask * 255).astype(np.uint8)\n",
    "            predicted_image = Image.fromarray(pred_mask)\n",
    "            predicted_image.save(os.path.join(output_dir, f\"{img_id}_pred.jpg\"))\n",
    "\n",
    "        pbar.close()\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg),\n",
    "                        ('dice', avg_meters['dice'].avg)])\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the configuration used for training\n",
    "    config_path = 'models/atten_trans_Mob_test/config.yml' \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    print('-' * 20)\n",
    "    print(\"Loaded training configuration:\")\n",
    "    for key in config:\n",
    "        print(f'{key}: {config[key]}')\n",
    "    print('-' * 20)\n",
    "\n",
    "    # Define loss function (criterion) - must match training\n",
    "    if config['loss'] == 'BCEDiceLoss':\n",
    "        criterion = BCEDiceLoss().cuda()\n",
    "    else:\n",
    "        # Assuming other losses are defined in losses.py\n",
    "        criterion = losses.__dict__[config['loss']]().cuda()\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    model = HybridAttenTransMob(n_classes=config['num_classes'])  \n",
    "    model = model.cuda()\n",
    "\n",
    "    model_path = 'models/atten_trans_Mob_test/model.pth'   \n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Successfully loaded model weights from {model_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Model weights not found at {model_path}. Please ensure the training was successful.\")\n",
    "        return\n",
    "\n",
    "    # Data loading code for test set\n",
    "    test_img_dir = 'dataset_TN3K/test-image'\n",
    "    test_mask_dir = 'dataset_TN3K/test-mask'\n",
    "\n",
    "    test_img_ids = glob(os.path.join(test_img_dir, f'*{config[\"img_ext\"]}'))\n",
    "    test_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in test_img_ids]\n",
    "\n",
    "    if not test_img_ids:\n",
    "        print(f\"No test images found in {test_img_dir}. Please check the path and extension.\")\n",
    "        return\n",
    "\n",
    "    test_transform = Compose([\n",
    "        A.Resize(config['input_h'], config['input_w']), # Resize for model input\n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    test_dataset = Dataset(\n",
    "        img_ids=test_img_ids,\n",
    "        img_dir=test_img_dir,\n",
    "        mask_dir=test_mask_dir,\n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1, # Use batch size 1 for validation to easily handle individual image saving and logging\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    output_prediction_dir = 'models/atten_trans_Mob_test/predictions' \n",
    "    print(f\"\\nStarting evaluation and prediction saving to: {output_prediction_dir}\")\n",
    "    val_log = validate_and_predict(config, test_loader, model, criterion, output_prediction_dir)\n",
    "\n",
    "    print(f\"\\n--- Evaluation Results ---\")\n",
    "    print(f\"Test Loss: {val_log['loss']:.4f}\")\n",
    "    print(f\"Test IoU: {val_log['iou']:.4f}\")\n",
    "    print(f\"Test Dice: {val_log['dice']:.4f}\")\n",
    "    print(f\"Predicted masks saved to {output_prediction_dir}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f8af4-5011-47e7-8818-67cf62c5656b",
   "metadata": {},
   "source": [
    "--- Evaluation Results ---  \n",
    "Test Loss: 0.6950  \n",
    "Test IoU: 0.2651  \n",
    "Test Dice: 0.3894  \n",
    "Predicted masks saved to models/atten_trans_Mob_test/predictions\n",
    "\n",
    "### 2.7 find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1aca296b-e0de-489c-83fe-0922bd2c965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model by Dice coefficient of atten_trans_Mob_test:\n",
      "epoch       4.000000\n",
      "lr          0.000001\n",
      "loss        0.725244\n",
      "iou         0.284719\n",
      "val_loss    0.708986\n",
      "val_iou     0.310253\n",
      "val_dice    0.471899\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "Best model by IoU:\n",
      "epoch       4.000000\n",
      "lr          0.000001\n",
      "loss        0.725244\n",
      "iou         0.284719\n",
      "val_loss    0.708986\n",
      "val_iou     0.310253\n",
      "val_dice    0.471899\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log_path = 'models/atten_trans_Mob_test/log.csv'\n",
    "df = pd.read_csv(log_path)\n",
    "\n",
    "best_model_by_dice = df.loc[df['val_dice'].idxmax()]\n",
    "best_model_by_iou = df.loc[df['val_iou'].idxmax()]\n",
    "\n",
    "print(\"Best model by Dice coefficient of atten_trans_Mob_test:\")\n",
    "print(best_model_by_dice)\n",
    "\n",
    "print(\"\\nBest model by IoU:\")\n",
    "print(best_model_by_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64903acd-8270-4027-99c4-39b121d8702f",
   "metadata": {},
   "source": [
    "### 2.8 Plot curves\n",
    "#### Loss and IoU curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9da467b-2b97-47af-8eac-1ce62cc27dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_path = 'models/atten_trans_Mob_test/log.csv'\n",
    "df = pd.read_csv(log_path)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))  \n",
    "\n",
    "axes[0].plot(df['epoch'], df['loss'], label='Train Loss')\n",
    "axes[0].plot(df['epoch'], df['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training vs Validation Loss of attention_unet of atten_trans_Mob_test')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(df['epoch'], df['iou'], label='Train IoU')\n",
    "axes[1].plot(df['epoch'], df['val_iou'], label='Validation IoU')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('IoU')\n",
    "axes[1].set_title('Training vs Validation IoU')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0bcf49-5e9f-47c9-b76e-11d20ce7f4ff",
   "metadata": {},
   "source": [
    "#### Learning rate curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e562f4c4-0e77-4a33-8092-016ae52298b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log = pd.read_csv('models/atten_trans_Mob_test/log.csv')\n",
    "epochs = log['epoch']\n",
    "lrs = log['lr']\n",
    "val_ious = log['val_iou']\n",
    "\n",
    "best_epochs = []\n",
    "best_so_far = -float('inf')\n",
    "for i in range(len(val_ious)):\n",
    "    if val_ious[i] > best_so_far:\n",
    "        best_so_far = val_ious[i]\n",
    "        best_epochs.append(i) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, lrs, marker='o', label='Learning Rate')\n",
    "\n",
    "max_epoch = epochs.max()\n",
    "xticks = list(range(0, max_epoch + 1, 40))\n",
    "plt.xticks(xticks)\n",
    "\n",
    "plt.text(\n",
    "    epochs.iloc[-1],\n",
    "    lrs.iloc[-1] * 0.9,  \n",
    "    f'{epochs.iloc[-1]}',\n",
    "    color='black',\n",
    "    fontsize=10,\n",
    "    ha='right',\n",
    "    va='top'\n",
    ")\n",
    "\n",
    "plt.scatter(epochs.iloc[best_epochs], lrs.iloc[best_epochs], \n",
    "            s=80, color='red', label='Saved Best Model')\n",
    "\n",
    "# 图像美化\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('atten_trans_Mob_test: Learning Rate Curve')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03786c51-8178-4553-b212-0d1c881c4a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Practical_5)",
   "language": "python",
   "name": "practical_5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
